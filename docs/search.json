[{"path":"index.html","id":"presentación","chapter":"Capítulo 1 Presentación","heading":"Capítulo 1 Presentación","text":"En el mundo dinámico y competitivo de los negocios, la capacidad de tomar decisiones informadas basadas en datos es crucial. La asignatura “Herramientas Cuantitativo Informáticas para la Toma de Decisiones en la Empresa” está diseñada para equipar los estudiantes del MBA con las habilidades y conocimientos necesarios para analizar, interpretar y aplicar datos estadísticos de manera efectiva en diversas áreas empresariales.El análisis de la información cuantitativa es una herramienta poderosa que permite los gerentes y profesionales tomar decisiones basadas en evidencia, identificando tendencias, patrones y relaciones significativas en los datos. Esta asignatura se centra en proporcionar una comprensión sólida de los conceptos estadísticos fundamentales, así como en la aplicación práctica de estas herramientas utilizando software informático especializado.lo largo esta asignatura, los estudiantes explorarán una amplia gama de temas estadísticos, comenzando con los principios básicos de la estadística descriptiva y la inferencia estadística. Aprenderán calcular y interpretar medidas de tendencia central y dispersión, realizar pruebas de hipótesis y construir intervalos de confianza. Además, se abordarán técnicas más avanzadas como el análisis de regresión, la correlación y la estadística multivariante, proporcionando los estudiantes una comprensión profunda de cómo estas herramientas pueden ser aplicadas en contextos empresariales.Una parte esencial del curso es la integración de herramientas informáticas cuantitativas, como R, Python y Excel. Estas plataformas solo facilitan el análisis de grandes volúmenes de datos, sino que también permiten la visualización de resultados de manera clara y concisa. Los estudiantes tendrán la oportunidad de trabajar con datos reales y desarrollar sus habilidades prácticas través de ejercicios y proyectos que simulan situaciones del mundo real.El lenguaje R, en particular, se destaca como una herramienta potente y versátil para impartir esta asignatura. R es ampliamente reconocido por su capacidad para manejar grandes conjuntos de datos y realizar análisis estadísticos complejos con una eficiencia notable. Además, su vasto ecosistema de paquetes y bibliotecas especializadas permite los estudiantes aplicar una variedad de técnicas estadísticas avanzadas con facilidad. R también ofrece robustas capacidades de visualización de datos, permitiendo la creación de gráficos y reportes altamente personalizados y de gran impacto visual. La combinación de estas características hace de R una herramienta ideal para enseñar y aplicar estadísticas en un entorno académico y profesional, asegurando que los estudiantes adquieran habilidades prácticas y relevantes para sus futuras carreras.La asignatura también enfatiza la interpretación y comunicación de los resultados estadísticos. Es fundamental que los profesionales solo sepan cómo realizar análisis, sino también cómo presentar sus hallazgos de manera que sean comprensibles y útiles para la toma de decisiones estratégicas.","code":""},{"path":"introducción-a-r.html","id":"introducción-a-r","chapter":"Capítulo 2 Introducción a R","heading":"Capítulo 2 Introducción a R","text":"","code":""},{"path":"introducción-a-r.html","id":"objetivos","chapter":"Capítulo 2 Introducción a R","heading":"2.1 Objetivos","text":"¿Qué pretendemos en 4 horas?Conocer un lenguaje de programación (R) extremadamente flexible, gratuito con altas prestaciones.¿Qué pretendemos?Manejar R con solturaDesarrollar programas para manejar grandes volúmenes de datos y analizar procesos usando herramientas de aprendizaje automático….","code":""},{"path":"introducción-a-r.html","id":"el-lenguaje-r","chapter":"Capítulo 2 Introducción a R","heading":"2.2 El lenguaje R","text":"R es un lenguaje de programación y un entorno de software libre para el análisis estadístico y la visualización de datos. Desarrollado originalmente por Ross Ihaka y Robert Gentleman en la Universidad de Auckland en los años 90, R se ha convertido en una herramienta esencial en la estadística, la ciencia de datos y la investigación académica. Su principal fortaleza reside en su capacidad para manejar, analizar y graficar grandes volúmenes de datos de manera eficiente.R es especialmente valorado por su amplia gama de paquetes y librerías que extienden su funcionalidad básica, permitiendo los usuarios realizar tareas complejas de análisis de datos, modelado estadístico, minería de datos y aprendizaje automático. Además, la comunidad de usuarios de R es muy activa, contribuyendo constantemente con nuevos paquetes y actualizaciones, lo que mantiene al lenguaje en la vanguardia de las innovaciones en análisis de datos.El entorno de desarrollo integrado más popular para R es RStudio, que proporciona una interfaz amigable y herramientas adicionales que facilitan la escritura y ejecución de código R. La versatilidad de R y su capacidad para integrarse con otros lenguajes de programación y sistemas de bases de datos lo convierten en una opción preferida para analistas, científicos de datos y estadísticos de todo el mundo.","code":""},{"path":"introducción-a-r.html","id":"por-qué-aprender-r","chapter":"Capítulo 2 Introducción a R","heading":"2.3 ¿Por qué aprender R?","text":"Aprender usar R puede ser beneficioso por varias razones, especialmente si estás interesado en la estadística, la ciencia de datos, la investigación científica o cualquier campo relacionado con el análisis de datos. continuación, se presentan algunas razones para aprender usar R:Ciencia de Datos: R se ha convertido en una herramienta fundamental en el campo de la ciencia de datos. Es utilizado para la limpieza de datos, la exploración de datos, el aprendizaje automático y la generación de informes.Ciencia de Datos: R se ha convertido en una herramienta fundamental en el campo de la ciencia de datos. Es utilizado para la limpieza de datos, la exploración de datos, el aprendizaje automático y la generación de informes.Big Data R es una herramienta poderosa cuando se trata de trabajar con grandes volúmenes de datos, también conocidos como “big data”. través de sus numerosos paquetes y extensiones, R ofrece capacidades avanzadas para la manipulación, procesamiento y análisis de conjuntos de datos masivos. Su capacidad para cargar, gestionar y realizar cálculos en datos de gran tamaño es fundamental en campos como la ciencia de datos y el análisis estadístico. Además, R se integra bien con herramientas de big data como Hadoop y Spark, lo que permite los profesionales de datos abordar proyectos que involucran la recopilación y análisis de datos escala empresarial. En resumen, R es una opción sólida para aquellos que deseen trabajar con big data y obtener información valiosa partir de conjuntos de datos extensos y complejos.Big Data R es una herramienta poderosa cuando se trata de trabajar con grandes volúmenes de datos, también conocidos como “big data”. través de sus numerosos paquetes y extensiones, R ofrece capacidades avanzadas para la manipulación, procesamiento y análisis de conjuntos de datos masivos. Su capacidad para cargar, gestionar y realizar cálculos en datos de gran tamaño es fundamental en campos como la ciencia de datos y el análisis estadístico. Además, R se integra bien con herramientas de big data como Hadoop y Spark, lo que permite los profesionales de datos abordar proyectos que involucran la recopilación y análisis de datos escala empresarial. En resumen, R es una opción sólida para aquellos que deseen trabajar con big data y obtener información valiosa partir de conjuntos de datos extensos y complejos.Poderosa Herramienta Estadística R es un lenguaje de programación y un entorno de desarrollo diseñado específicamente para estadísticas y análisis de datos. Ofrece una amplia gama de funciones estadísticas y técnicas de modelado que son esenciales para la investigación y el análisis de datos.Poderosa Herramienta Estadística R es un lenguaje de programación y un entorno de desarrollo diseñado específicamente para estadísticas y análisis de datos. Ofrece una amplia gama de funciones estadísticas y técnicas de modelado que son esenciales para la investigación y el análisis de datos.Existe una Comunidad Activa R cuenta con una comunidad de usuarios y desarrolladores activa y diversa. Esto significa que hay una gran cantidad de recursos, paquetes y documentación disponibles en línea. Puedes encontrar soluciones para una variedad de problemas y obtener ayuda de la comunidad cuando lo necesites.Existe una Comunidad Activa R cuenta con una comunidad de usuarios y desarrolladores activa y diversa. Esto significa que hay una gran cantidad de recursos, paquetes y documentación disponibles en línea. Puedes encontrar soluciones para una variedad de problemas y obtener ayuda de la comunidad cuando lo necesites.Paquetes Especializados R tiene una gran cantidad de paquetes diseñados para tareas específicas. Ya sea que necesites realizar análisis de series temporales, gráficos avanzados, aprendizaje automático o análisis bioestadísticos, es probable que encuentres un paquete en R que se adapte tus necesidades.Paquetes Especializados R tiene una gran cantidad de paquetes diseñados para tareas específicas. Ya sea que necesites realizar análisis de series temporales, gráficos avanzados, aprendizaje automático o análisis bioestadísticos, es probable que encuentres un paquete en R que se adapte tus necesidades.Es Multiplataforma ¿Usas Windows? Muy bien. ¿Tienes Mac? hay problema. ¿Eres puro Linux? pasa nada. R está disponible y funcionando en todos estos sistemas operativos.Es Multiplataforma ¿Usas Windows? Muy bien. ¿Tienes Mac? hay problema. ¿Eres puro Linux? pasa nada. R está disponible y funcionando en todos estos sistemas operativos.Visualización de Datos R ofrece capacidades avanzadas de visualización de datos. Puedes crear gráficos y visualizaciones de alta calidad para comunicar tus resultados de manera efectiva. Librerías como ggplot2 son ampliamente utilizadas para crear visualizaciones personalizadas y elegantes.Visualización de Datos R ofrece capacidades avanzadas de visualización de datos. Puedes crear gráficos y visualizaciones de alta calidad para comunicar tus resultados de manera efectiva. Librerías como ggplot2 son ampliamente utilizadas para crear visualizaciones personalizadas y elegantes.Flexibilidad y Personalización R es altamente personalizable y extensible. Puedes escribir tus propias funciones y paquetes para adaptar R tus necesidades específicas. Esto es especialmente útil si estás realizando investigaciones originales o trabajando en proyectos específicos.Flexibilidad y Personalización R es altamente personalizable y extensible. Puedes escribir tus propias funciones y paquetes para adaptar R tus necesidades específicas. Esto es especialmente útil si estás realizando investigaciones originales o trabajando en proyectos específicos.Uso en la Industria y la Academia R es ampliamente utilizado tanto en la industria como en la academia. Aprender R puede abrirte puertas en una variedad de campos, incluyendo la ciencia de datos, la investigación académica, la consultoría y más.Uso en la Industria y la Academia R es ampliamente utilizado tanto en la industria como en la academia. Aprender R puede abrirte puertas en una variedad de campos, incluyendo la ciencia de datos, la investigación académica, la consultoría y más.Gratuito y de Código Abierto R es un software de código abierto y es gratuito para su uso. Esto lo hace accesible para una amplia gama de usuarios y organizaciones sin incurrir en costos de licencia.Gratuito y de Código Abierto R es un software de código abierto y es gratuito para su uso. Esto lo hace accesible para una amplia gama de usuarios y organizaciones sin incurrir en costos de licencia.Herramientas de Integración Existen numerosas herramientas y entornos que se integran fácilmente con R, como RStudio, que proporciona un entorno de desarrollo amigable y funcionalidad adicional para facilitar la programación en R.Herramientas de Integración Existen numerosas herramientas y entornos que se integran fácilmente con R, como RStudio, que proporciona un entorno de desarrollo amigable y funcionalidad adicional para facilitar la programación en R.Replicabilidad y Documentación: R fomenta la replicabilidad de investigaciones y análisis al permitir que los usuarios documenten sus pasos y resultados de manera efectiva en forma de scripts y documentos R Markdown.Replicabilidad y Documentación: R fomenta la replicabilidad de investigaciones y análisis al permitir que los usuarios documenten sus pasos y resultados de manera efectiva en forma de scripts y documentos R Markdown.","code":""},{"path":"introducción-a-r.html","id":"rstudio","chapter":"Capítulo 2 Introducción a R","heading":"2.4 RStudio","text":"RStudio es un entorno de desarrollo integrado (IDE, GUI en ingles) diseñado específicamente para el lenguaje de programación R. Lanzado en 2011, RStudio proporciona una interfaz amigable y herramientas robustas que facilitan la escritura, depuración y ejecución de código R. Entre sus características destacan el editor de scripts con resaltado de sintaxis, la consola interactiva, herramientas para la gestión de proyectos y la visualización integrada de gráficos y datos.Además, RStudio soporta la integración con sistemas de control de versiones como Git y facilita la creación de documentos reproducibles través de R Markdown. Estas capacidades hacen de RStudio una elección predilecta para estadísticos y científicos de datos.","code":""},{"path":"introducción-a-r.html","id":"qué-cosas-hago-con-r","chapter":"Capítulo 2 Introducción a R","heading":"2.5 Qué cosas hago con R","text":"En la empresa, antes era imprescindible saber inglés. Ahora, se da por descontado y cada vez es más importante conocer un lenguaje de programación que te permita manejar información numérica (asociada los grandes volúmenes de información que se generan)R se ha convertido en mi lenguaje de programación favorito (hay otros)Puedo escribir artículos científicos combinando texto y datosPuedo hacer reproducible la investigaciónPuedo generar potentes, y llamativos gráficosHacer gráficos dinámicos e interactivosIntegrar lenguajes html, latex, texto plano,..Generar mapas interactivosGenerar páginas webs (como esta!)Manejar millones de observaciones fácilmente!Un lenguaje altamente flexible…","code":""},{"path":"introducción-a-r.html","id":"creando-el-entorno-de-trabajo-en-rstudio.","chapter":"Capítulo 2 Introducción a R","heading":"2.6 Creando el entorno de trabajo en RStudio.","text":"R es un lenguaje de programación y entorno de software ampliamente utilizado en estadísticas y análisis de datos. Lo que lo hace aún más poderoso es su integración con RStudio, un entorno de desarrollo integrado (IDE, abreviatura de Integrated Development Environment) diseñado específicamente para trabajar con R. RStudio proporciona una interfaz amigable y altamente funcional que facilita la escritura, prueba y depuración de código R.Puedes descargar R aquí .Puedes descarta RStudio aquíhttps://www.uv.es/vcoll/primeros-pasos.htmlTrabajamos con la interfaz RStudio antes que con la de R porque es “más amigable”.Un tipo básico de archivo de texto es un script R. Los scripts R tienen la extensión “.R” y reconocen todo el texto como si fuera código R.Cuando trabaja con código R, puede agregar comentarios usando este nombre: “#”. Cualquier línea que comience con esta marca se ignora al ejecutar el código.Una vez estamos en RStudio, podemos escribir y ejecutar las órdenes de varias formas:directamente en la consolaa través de un script (.R)con ficheros Rmarkdown (.Rmd)Como podemos ver, RStudio está (normalmente) dividido en 4 paneles.","code":""},{"path":"introducción-a-r.html","id":"consola","chapter":"Capítulo 2 Introducción a R","heading":"2.6.1 Consola","text":"Por defecto, la consola se encuentra en el panel inferior-izquierdo. ¿Vemos la pestaña que pone Console? Inmediatamente debajo aparece un texto informativo y, finalmente, el símbolo “>”. Aquí es donde R espera que le demos instrucciones. Para ejecutarlas y obtener el resultado pulsamos enter.Vamos hacer este ejemplo:En el ejemplo anterior se han ido introduciendo y ejecutando las instrucciones una una. También es posible ejecutar desde la consola más de una instrucciones. Para ello, las instrucciones deben separarse con un “;”.","code":"\n2+2\n#> [1] 4\n5*(3-1)^2\n#> [1] 20\nsqrt(4)\n#> [1] 2\n2 + 2 ; 5*(3-1)^2 ; sqrt(4)\n#> [1] 4\n#> [1] 20\n#> [1] 2"},{"path":"introducción-a-r.html","id":"scripts","chapter":"Capítulo 2 Introducción a R","heading":"2.6.2 Scripts","text":"Trabajar en la consola es muy limitado ya que las instrucciones se han de introducir una una. Lo habitual es trabajar con scripts o ficheros de instrucciones. Estos ficheros tienen extensión .R.Se puede crear una script con cualquier editor de texto, pero nosotros lo haremos desde RStudio. Para ello, seleccionamos la siguiente ruta de menús:File > New File > R scriptEl panel del script se sitúa en la parte superior-izquierda de RStudio. Ahora podemos escribir las instrucciones línea por línea. Las instrucciones las podemos ejecutar una una o las podemos seleccionar y ejecutar en bloque. Para ejecutar las instrucciones tenemos varias alternativas:Hacemos clic en el botón: Run (botón situado en la parte derecha de las opciones del panel de script)Hacemos clic en el botón: Run (botón situado en la parte derecha de las opciones del panel de script)Pulsamos Ctrl+rPulsamos Ctrl+r","code":""},{"path":"introducción-a-r.html","id":"entorno","chapter":"Capítulo 2 Introducción a R","heading":"2.6.3 Entorno","text":"El panel, llamémoslo de entorno esta compuesto de varias pestañas:EnvironmentHistoryOtras cosas …En el “Environment” se irán registrando los objetos que vayamos creando en la sesión de trabajo. También tenemos la opción de cargar y guardar una sesión de trabajo, importar datos y limpiar los objetos de la sesión. Estas opciones están accesibles través de la cinta de opciones de la pestaña.","code":""},{"path":"introducción-a-r.html","id":"miscelánea-archivos-gráficos-paquetes-ayuda-visor","chapter":"Capítulo 2 Introducción a R","heading":"2.6.4 Miscelánea: Archivos, Gráficos, Paquetes, Ayuda, Visor","text":"Con el nombre de Misceléna nos referimos al otro panel (que se encuentra en la parte inferior-derecha) del escritorio de RStudio.En este panel cabe destacar las siguientes pestañas, cada una con diferentes opciones:Files: es una especie de explotador de ficheros.Plots: donde se visualizan los gráficos que creamos. Entre las opciones disponibles se encuentran:\nZoom: para agrandar el gráfico y verlo en otra ventana.\nExport: para exportar/guardar el gráfico. Se puede guardar el gráfico como imagen, pdf o copiarlo al portapapeles.\nZoom: para agrandar el gráfico y verlo en otra ventana.Export: para exportar/guardar el gráfico. Se puede guardar el gráfico como imagen, pdf o copiarlo al portapapeles.Packages: proporciona un listado de los paquetes instalados en R y los que han sido cargado en la sesión. través de las opciones de esta pestaña podemos instalar nuevos paquetes o actualizar los existentes.Help: Para obtener ayuda sobre una determinada función.Otros:","code":""},{"path":"introducción-a-r.html","id":"configuración-del-directorio-de-trabajo.","chapter":"Capítulo 2 Introducción a R","heading":"2.6.5 Configuración del directorio de trabajo.","text":"Antes de comenzar trabajar debemos fijar el directorio donde queremos guardar nuestros ficheros. Básicamente, dos alternativas.","code":""},{"path":"introducción-a-r.html","id":"opción-1.-fijar-directorio.","chapter":"Capítulo 2 Introducción a R","heading":"2.6.5.1 Opción 1. Fijar directorio.","text":"Opción 1. Indicamos R la ruta donde queremos trabajar y la fijamos con la función setwd().setwd(“C:/ruta del directorio de trabajo”)Para comprobar el directorio de trabajo utilizamos la función getwd():getwd()Para obtener un listado de los ficheros que contiene la ruta establecida se usa la función dir().dir()","code":""},{"path":"introducción-a-r.html","id":"opción-2.-proyecto-de-r.","chapter":"Capítulo 2 Introducción a R","heading":"2.6.5.2 Opción 2. Proyecto de R.","text":"Al crear un proyecto todos los ficheros quedan vinculados directamente al proyecto. Para crear un proyecto selecciónFile > New project…Se abrirá un menú que guia en la generación del proyectoPara crear un proyecto en un nuevo directorio, hacemos clic en el botón New Directory. Seguidamente, seleccionamos el tipo de proyecto, en nuestro caso Empty Project. Ahora, asignamos un nombre al directorio (carpeta) que se va crear y que al mismo tiempo será el nombre del proyecto de R. Para terminar, hacemos clic en el botón Create Project. Al seguir este proceso se habrá creado una carpeta en Documentos y un fichero nombre_carpeta.Rproj.Para crear un proyecto en un nuevo directorio, hacemos clic en el botón New Directory. Seguidamente, seleccionamos el tipo de proyecto, en nuestro caso Empty Project. Ahora, asignamos un nombre al directorio (carpeta) que se va crear y que al mismo tiempo será el nombre del proyecto de R. Para terminar, hacemos clic en el botón Create Project. Al seguir este proceso se habrá creado una carpeta en Documentos y un fichero nombre_carpeta.Rproj.Para crear un proyecto en una carpeta que ya existe, hacemos clic en el botón Existing Directory y después seleccionamos la carpeta ayudándonos del Browse.. si fuera necesario. Una vez elegida la carpeta, clicamos en Create Project.Para crear un proyecto en una carpeta que ya existe, hacemos clic en el botón Existing Directory y después seleccionamos la carpeta ayudándonos del Browse.. si fuera necesario. Una vez elegida la carpeta, clicamos en Create Project.Para abrir un proyecto hacemos doble clic sobre el archivo con extensión .Rproj o lo abrimos desde el menú de RStudio: File > Open Project…Para abrir un proyecto hacemos doble clic sobre el archivo con extensión .Rproj o lo abrimos desde el menú de RStudio: File > Open Project…Ventaja de los proyectos: cualquier fichero que creemos (script de R, documento de Rmarkdown, etc.) y guardemos se guardará en la carpeta del proyecto.","code":""},{"path":"introducción-a-r.html","id":"instalar-y-cargar-paquetes.","chapter":"Capítulo 2 Introducción a R","heading":"2.6.6 Instalar y cargar paquetes.","text":"R está compuesto por un sistema base, pero para extender su funcionalidad es necesario instalar paquetes adicionales.Podemos instalar paquetes de varias formas:través del menú: Tools > Install packages…En el escritorio de RStudio: Packages/Install. Vemos los paquetes que tenemos actualmente instalados y aquellos que se encuentran cargados.Utilizando la función install.packages(). El nombre del paquete que queremos instalar debe ir entre comillas.En ocasiones, para nuestra sesión de trabajo necesitamos instalar varios paquetes.Es habitual iniciar la sesión de trabajo en R con un “pequeño programa” en el que se indica que para la sesión se requiere una serie de paquetes y que si están instalados los instale. Aquí tenemos la versión más sencilla para hacer estoUna vez instalado el paquete, hay que cargarlo para poderlo utilizar. Esto se hace con la función library().","code":"\n# dplyr es un paquete que se utiliza para manipular/gestionar datos\n# install.packages(\"dplyr\") \n# install.packages(c(\"dplyr\",\"ggplot2\"))\nif(!require(dplyr)) {install.packages(\"dplyr\")}\nlibrary(dplyr) # observad que el nombre del paquete no se pone entre comillas para cargarlo."},{"path":"introducción-a-r.html","id":"ayuda-en-r.","chapter":"Capítulo 2 Introducción a R","heading":"2.6.7 Ayuda en R.","text":"En muchas ocasiones necesitamos ayuda sobre cómo funciona una determinada función, cuáles son sus argumentos, etc. Hay varias formas de pedir la ayuda de R. Vamos pedir la ayuda de la función mean().Si ejecutamos directamente la función library() se abrirá una ventana listando los paquetes que tenemos instalados en R. En el escritorio de RStudio, en la pestaña Packages también tenemos en listado de paquetes instalados (organizados en dos bloques: User Library y System Library)library()\nPara obtener ayuda sobre un determinado paquete…library(help=“foreign”)En ocasiones junto con los paquetes se facilita una documentación sobre su uso, esto se le llama vignettes.vignette() # Para ver una lista de las vignettes las que podemos acceder por paquete y una vignette concreta de un paquete:Pero sin duda, una de las mejores fuentes de ayuda en R nos la proporciona internet. Bien haciendo directamente en google la búsqueda sobre el tema que estamos interesados, bien acudiendo algunas de las muchas webs que ofrecen ayuda. Algunas de las más populares y recomendables webs son:https://es.stackoverflow.com/https://stackoverflow.co/https://chat.openai.com/","code":"\nhelp(mean)\n?mean"},{"path":"introducción-a-r.html","id":"primeros-pasos","chapter":"Capítulo 2 Introducción a R","heading":"2.7 Primeros pasos","text":"","code":""},{"path":"introducción-a-r.html","id":"script-básico-para-apreder-r","chapter":"Capítulo 2 Introducción a R","heading":"2.7.1 Script básico para apreder R","text":"","code":"\n# Comentarios en R se realizan con el símbolo '#' al principio de la línea\n\n# Asignación de variables\nx <- 5            # Asigna el valor 5 a la variable x\ny <- 3            # Asigna el valor 3 a la variable y\n\n# Realizar cálculos\nsuma <- x + y     # Suma x e y y guarda el resultado en la variable suma\nresta <- x - y    # Resta y de x y guarda el resultado en la variable resta\nproducto <- x * y # Multiplica x por y y guarda el resultado en la variable producto\ndivision <- x / y # Divide x por y y guarda el resultado en la variable division\n\n# Imprimir resultados\ncat(\"La suma es:\", suma, \"\\n\")\n#> La suma es: 8\ncat(\"La resta es:\", resta, \"\\n\")\n#> La resta es: 2\ncat(\"El producto es:\", producto, \"\\n\")\n#> El producto es: 15\ncat(\"La división es:\", division, \"\\n\")\n#> La división es: 1.666667\n\n# Crear un vector de números del 1 al 10\nmi_vector <- 1:10\n\n# Imprimir el vector\ncat(\"Mi vector:\", mi_vector, \"\\n\")\n#> Mi vector: 1 2 3 4 5 6 7 8 9 10\n\n# Crear un gráfico de dispersión simple\nplot(mi_vector, mi_vector, main=\"Gráfico de Dispersión\", xlab=\"Eje X\", ylab=\"Eje Y\", col=\"blue\", pch=19)\n\n# Guardar el gráfico en un archivo PNG\npng(filename=\"grafico.png\")\nplot(mi_vector, mi_vector, main=\"Gráfico de Dispersión\", xlab=\"Eje X\", ylab=\"Eje Y\", col=\"blue\", pch=19)\ndev.off() # Cerrar el archivo PNG\n#> quartz_off_screen \n#>                 2"},{"path":"introducción-a-r.html","id":"tipos-de-datos-en-r","chapter":"Capítulo 2 Introducción a R","heading":"2.7.2 Tipos de Datos en R","text":"En R, existen varios tipos de datos que se utilizan para almacenar y manipular información. continuación, se presentan algunos de los tipos de datos más comunes en R:Numérico (numeric): Representa números reales o decimales. Por ejemplo, x <- 3.14.Numérico (numeric): Representa números reales o decimales. Por ejemplo, x <- 3.14.Entero (integer): Representa números enteros. Por ejemplo, y <- 5L (la “L” indica que es un número entero).Entero (integer): Representa números enteros. Por ejemplo, y <- 5L (la “L” indica que es un número entero).Caracteres (character): Almacena texto o cadenas de caracteres. Se utiliza comillas simples o dobles. Por ejemplo, nombre <- \"Juan\".Caracteres (character): Almacena texto o cadenas de caracteres. Se utiliza comillas simples o dobles. Por ejemplo, nombre <- \"Juan\".Lógico (logical): Representa valores lógicos Verdadero (TRUE) o Falso (FALSE). Se utiliza para evaluaciones condicionales. Por ejemplo, es_mayor <- TRUE.Lógico (logical): Representa valores lógicos Verdadero (TRUE) o Falso (FALSE). Se utiliza para evaluaciones condicionales. Por ejemplo, es_mayor <- TRUE.Factor: Se utiliza para representar datos categóricos o variables cualitativas. Los factores tienen niveles que indican las categorías. Por ejemplo, genero <- factor(c(\"Masculino\", \"Femenino\")).Factor: Se utiliza para representar datos categóricos o variables cualitativas. Los factores tienen niveles que indican las categorías. Por ejemplo, genero <- factor(c(\"Masculino\", \"Femenino\")).Fecha y hora: R tiene tipos de datos específicos para fechas y horas, como Date y POSIXct, que permiten realizar operaciones y cálculos con fechas y horas.Fecha y hora: R tiene tipos de datos específicos para fechas y horas, como Date y POSIXct, que permiten realizar operaciones y cálculos con fechas y horas.Lista (list): Permite almacenar una colección heterogénea de objetos (como vectores, matrices u otras listas) en una sola estructura de datos. Por ejemplo, mi_lista <- list(1, \"texto\", c(2, 3, 4)).Lista (list): Permite almacenar una colección heterogénea de objetos (como vectores, matrices u otras listas) en una sola estructura de datos. Por ejemplo, mi_lista <- list(1, \"texto\", c(2, 3, 4)).Matriz (matrix): Es una estructura bidimensional que almacena datos del mismo tipo en filas y columnas. Por ejemplo, matriz <- matrix(1:6, nrow = 2, ncol = 3).Matriz (matrix): Es una estructura bidimensional que almacena datos del mismo tipo en filas y columnas. Por ejemplo, matriz <- matrix(1:6, nrow = 2, ncol = 3).Arreglo (array): Similar una matriz, pero puede tener más de dos dimensiones. Por ejemplo, mi_arreglo <- array(1:12, dim = c(2, 3, 2)).Arreglo (array): Similar una matriz, pero puede tener más de dos dimensiones. Por ejemplo, mi_arreglo <- array(1:12, dim = c(2, 3, 2)).DataFrame: Es similar una matriz, pero puede contener diferentes tipos de datos en cada columna. Los data frames son muy utilizados para almacenar conjuntos de datos. Por ejemplo, mi_df <- data.frame(nombre = c(\"Juan\", \"María\"), edad = c(25, 30)).DataFrame: Es similar una matriz, pero puede contener diferentes tipos de datos en cada columna. Los data frames son muy utilizados para almacenar conjuntos de datos. Por ejemplo, mi_df <- data.frame(nombre = c(\"Juan\", \"María\"), edad = c(25, 30)).NULL: Representa la ausencia de valor o datos faltantes. Por ejemplo, si una variable tiene un valor asignado, se considera NULL.NULL: Representa la ausencia de valor o datos faltantes. Por ejemplo, si una variable tiene un valor asignado, se considera NULL.Infinito y NaN: R también tiene representaciones especiales para el infinito (Inf o -Inf) y para valores indefinidos (NaN, que significa “es un número”).Infinito y NaN: R también tiene representaciones especiales para el infinito (Inf o -Inf) y para valores indefinidos (NaN, que significa “es un número”).Estos son algunos de los tipos de datos más comunes en R. Es importante comprender cómo trabajar con cada tipo de dato, ya que es fundamental para el análisis y la manipulación de datos en R.","code":""},{"path":"introducción-a-r.html","id":"la-importancia-de-los-objetos-en-r","chapter":"Capítulo 2 Introducción a R","heading":"2.7.3 La Importancia de los Objetos en R","text":"Uno de los conceptos fundamentales en R es el uso de objetos para almacenar y manipular datos. Los objetos son contenedores que almacenan información, como números, texto, vectores, matrices, data frames y más. Comprender la importancia de los objetos es esencial para trabajar de manera efectiva en R. continuación, se destacan algunas razones por las cuales los objetos son fundamentales:Organización de Datos: Los objetos permiten organizar los datos de manera estructurada. Por ejemplo, puedes almacenar datos en un vector, una matriz o un data frame, lo que facilita la gestión y la manipulación de la información.Organización de Datos: Los objetos permiten organizar los datos de manera estructurada. Por ejemplo, puedes almacenar datos en un vector, una matriz o un data frame, lo que facilita la gestión y la manipulación de la información.Reutilización: Los objetos se pueden reutilizar en múltiples operaciones y análisis. Puedes crear un objeto con datos y luego realizar diversas operaciones estadísticas o gráficas sin necesidad de volver cargar los datos cada vez.Reutilización: Los objetos se pueden reutilizar en múltiples operaciones y análisis. Puedes crear un objeto con datos y luego realizar diversas operaciones estadísticas o gráficas sin necesidad de volver cargar los datos cada vez.Claridad y Documentación: El uso de objetos con nombres descriptivos mejora la claridad del código. Puedes asignar nombres significativos los objetos, lo que facilita la comprensión del código y su documentación.Claridad y Documentación: El uso de objetos con nombres descriptivos mejora la claridad del código. Puedes asignar nombres significativos los objetos, lo que facilita la comprensión del código y su documentación.Programación Modular: Los objetos permiten dividir un problema en partes más pequeñas y manejables. Puedes crear funciones que operen sobre objetos específicos, lo que promueve la programación modular y la reutilización de código.Programación Modular: Los objetos permiten dividir un problema en partes más pequeñas y manejables. Puedes crear funciones que operen sobre objetos específicos, lo que promueve la programación modular y la reutilización de código.Interacción con Paquetes: Muchos paquetes y funciones en R trabajan con objetos específicos. Al comprender cómo funcionan estos objetos, puedes aprovechar al máximo la funcionalidad de los paquetes y realizar análisis avanzados.Interacción con Paquetes: Muchos paquetes y funciones en R trabajan con objetos específicos. Al comprender cómo funcionan estos objetos, puedes aprovechar al máximo la funcionalidad de los paquetes y realizar análisis avanzados.Visualización y Gráficos: Los objetos pueden contener datos que se utilizan para crear visualizaciones y gráficos. Los paquetes de gráficos en R, como ggplot2, se basan en objetos para generar gráficos personalizados.Visualización y Gráficos: Los objetos pueden contener datos que se utilizan para crear visualizaciones y gráficos. Los paquetes de gráficos en R, como ggplot2, se basan en objetos para generar gráficos personalizados.Análisis Estadístico: Los objetos son esenciales para realizar análisis estadísticos en R. Puedes aplicar pruebas, modelos y métodos estadísticos los datos almacenados en objetos.Análisis Estadístico: Los objetos son esenciales para realizar análisis estadísticos en R. Puedes aplicar pruebas, modelos y métodos estadísticos los datos almacenados en objetos.En resumen, los objetos son la base de la programación en R y desempeñan un papel crucial en el análisis de datos y la creación de visualizaciones. Comprender cómo trabajar con objetos y cómo seleccionar el tipo adecuado de objeto para tus datos es esencial para aprovechar al máximo las capacidades de R.","code":""},{"path":"introducción-a-r.html","id":"dataframes-en-r","chapter":"Capítulo 2 Introducción a R","heading":"2.7.4 DataFrames en R","text":"Un DataFrame en R es una estructura de datos bidimensional que se utiliza para almacenar y organizar datos de manera tabular. Cada columna de un DataFrame puede contener un tipo de dato diferente, como numérico, de caracteres, lógico, etc. Los DataFrames son muy utilizados para trabajar con conjuntos de datos y realizar análisis de datos en R.","code":""},{"path":"introducción-a-r.html","id":"ejemplo-de-dataframe","chapter":"Capítulo 2 Introducción a R","heading":"2.7.5 Ejemplo de DataFrame","text":"Supongamos que queremos crear un DataFrame para almacenar información sobre estudiantes, incluyendo sus nombres, edades y calificaciones en dos materias: Matemáticas y Ciencias. Podemos crear un DataFrame de la siguiente manera:Cómo identificar los elementos de un dataframe","code":"\n# Crear un DataFrame de ejemplo\nestudiantes <- data.frame(\n  Nombre = c(\"Juan\", \"María\", \"Carlos\", \"Ana\"),\n  Edad = c(25, 30, 22, 28),\n  Matematicas = c(90, 85, 78, 92),\n  Ciencias = c(88, 92, 76, 89)\n)\n\n# Mostrar el DataFrame\nestudiantes\n#>   Nombre Edad Matematicas Ciencias\n#> 1   Juan   25          90       88\n#> 2  María   30          85       92\n#> 3 Carlos   22          78       76\n#> 4    Ana   28          92       89\nestudiantes$Edad\n#> [1] 25 30 22 28\nestudiantes[,2]\n#> [1] 25 30 22 28\nestudiantes[1,]\n#>   Nombre Edad Matematicas Ciencias\n#> 1   Juan   25          90       88"},{"path":"introducción-a-r.html","id":"un-for-en-r","chapter":"Capítulo 2 Introducción a R","heading":"2.8 Un for en R","text":"En R, un bucle se utiliza para repetir una serie de instrucciones un número específico de veces o para iterar sobre una secuencia de elementos, como vectores o listas. Aquí tienes un ejemplo simple de cómo se usa un bucle para imprimir los números del 1 al 5.En este ejemplo, utilizaremos un bucle para generar una serie de datos y almacenarlos en un DataFrame. Supongamos que queremos calcular y almacenar los primeros diez números pares en un DataFrame. Utilizaremos un bucle para generar estos números y luego los almacenaremos en un DataFrame.","code":"\n# Ejemplo de bucle for para imprimir números del 1 al 5\nfor (i in 1:5) {\n  print(i)\n}\n#> [1] 1\n#> [1] 2\n#> [1] 3\n#> [1] 4\n#> [1] 5\n# Crear un DataFrame vacío\nmi_df <- data.frame(NumerosPares = numeric(0))\n# Usar un bucle for para generar números pares y almacenarlos\nfor (i in 1:10) {\n  numero_par <- 2 * i\n  mi_df <- rbind(mi_df,numero_par)\n}\n# Mostrar el DataFrame resultante\nmi_df\n#>    X2\n#> 1   2\n#> 2   4\n#> 3   6\n#> 4   8\n#> 5  10\n#> 6  12\n#> 7  14\n#> 8  16\n#> 9  18\n#> 10 20"},{"path":"introducción-a-r.html","id":"leer-un-excel","chapter":"Capítulo 2 Introducción a R","heading":"2.9 Leer un Excel","text":"","code":""},{"path":"introducción-a-r.html","id":"datos-empleados","chapter":"Capítulo 2 Introducción a R","heading":"2.9.1 Datos Empleados","text":"RStudio permite cargar datos través de menús (File > Import Dataset). Por menús se pueden cargar datos CSV, EXCEL, SPSS, SAS y STATA.","code":"\nlibrary(readxl)\nmyxls <- read_xlsx(\"Datos/Datos_de_empleados.xlsx\")\nhead(myxls)\n#> # A tibble: 6 × 10\n#>      id sexo  fechnac  educ catlab salario salini tiempemp\n#>   <dbl> <chr> <chr>   <dbl>  <dbl>   <dbl>  <dbl>    <dbl>\n#> 1     1 h     19027      15      3   57000  27000       98\n#> 2     2 h     21328      16      1   40200  18750       98\n#> 3     3 m     10800      12      1   21450  12000       98\n#> 4     4 m     17272       8      1   21900  13200       98\n#> 5     5 h     20129      15      1   45000  21000       98\n#> 6     6 h     21419      15      1   32100  13500       98\n#> # ℹ 2 more variables: expprev <dbl>, minoría <dbl>\nmyxls$fechnac <- as.Date(as.numeric(myxls$fechnac))\n#> Warning in as.Date(as.numeric(myxls$fechnac)): NAs\n#> introduced by coercion"},{"path":"introducción-a-r.html","id":"leer-un-csv","chapter":"Capítulo 2 Introducción a R","heading":"2.9.2 Leer un csv","text":"Al igual que los archivos de excel, R puede leer multitud de formatos. En general para leer cada formato es necesario disponer del paquete que hace esa función.Los archivos con extensión csv son muy comunes para la transferencia de bases de datos. Por ejemplo read.csv() es una función que lee este formatoCSV significa “comma separated data”. En realidad CSV es un caso particular de “tabular o text data”","code":""},{"path":"introducción-a-r.html","id":"actividad-1","chapter":"Capítulo 2 Introducción a R","heading":"2.10 Actividad 1:","text":"El objetivo de esta actividad es familiarizarse con los conceptos básicos de R y practicar algunas operaciones simples.Instalación de Paquetes:\nInstala el paquete stringr usando el siguiente comando:\nCarga el paquete stringr\nExplora la viñeta y investiga sobre las cosas que hace\nUsa los ejemplos que aparecen en la ayuda de la funció ‘str_sub()’\nInstala el paquete stringr usando el siguiente comando:Carga el paquete stringrExplora la viñeta y investiga sobre las cosas que haceUsa los ejemplos que aparecen en la ayuda de la funció ‘str_sub()’Creación de un DataFrame:\nCrea un DataFrame llamado datos con tres columnas: Nombre, Edad y Puntuación.\nLlena el DataFrame con al menos 5 filas de datos ficticios.\nCrea un DataFrame llamado datos con tres columnas: Nombre, Edad y Puntuación.Llena el DataFrame con al menos 5 filas de datos ficticios.Operaciones Básicas:\nCalcula la media de las edades en el DataFrame.\nEncuentra la persona con la puntuación más alta.\nFiltra las filas para mostrar solo las personas mayores de 25 años.\nOrdena el DataFrame por nombre en orden alfabético.\nCalcula la media de las edades en el DataFrame.Encuentra la persona con la puntuación más alta.Filtra las filas para mostrar solo las personas mayores de 25 años.Ordena el DataFrame por nombre en orden alfabético.Visualización de Datos (Opcional):\nSi te sientes cómodo, intenta visualizar alguna característica de tu DataFrame, como un gráfico de barras para las puntuaciones.\nSi te sientes cómodo, intenta visualizar alguna característica de tu DataFrame, como un gráfico de barras para las puntuaciones.","code":""},{"path":"introducción-a-r.html","id":"actividad-2","chapter":"Capítulo 2 Introducción a R","heading":"2.11 Actividad 2:","text":"El objetivo de esta actividad es practicar el uso del bucle en R para generar tablas de multiplicar.Elige un número para el cual quieres generar la tabla de multiplicar. Puede ser cualquier número entero positivo.Elige un número para el cual quieres generar la tabla de multiplicar. Puede ser cualquier número entero positivo.Utiliza un bucle para generar la tabla de multiplicar del número seleccionado.Utiliza un bucle para generar la tabla de multiplicar del número seleccionado.La tabla debe incluir multiplicaciones del número del 1 al 10.La tabla debe incluir multiplicaciones del número del 1 al 10.Muestra los resultados en forma de una tabla que tenga dos columnas: una para el multiplicador y otra para el resultado de la multiplicación.Muestra los resultados en forma de una tabla que tenga dos columnas: una para el multiplicador y otra para el resultado de la multiplicación.","code":""},{"path":"introducción-a-r.html","id":"actividad-3","chapter":"Capítulo 2 Introducción a R","heading":"2.12 Actividad 3:","text":"El objetivo es descargar un fichero de Internet y procesarloDescargar el excel de esta webhttps://econet.carm.es/web/crem/inicio/-/crem/sicrem/PU_padron/series/sec4_sec2.htmlManejar el excel para que en la primera fila aparezca el nombre de la variable y en cada fila siguiente la población de cada municipio. Eliminar la fila de Total.Manejar el excel para que en la primera fila aparezca el nombre de la variable y en cada fila siguiente la población de cada municipio. Eliminar la fila de Total.Importar el fichero RImportar el fichero RCalcular la suma de población en la CARM cada año usando la función sum()Calcular la suma de población en la CARM cada año usando la función sum()","code":""},{"path":"estadística-con-r.html","id":"estadística-con-r","chapter":"Capítulo 3 Estadística con R","heading":"Capítulo 3 Estadística con R","text":"R es un lenguaje especializado en implementar técnicas estadísticas.R es un lenguaje especializado en implementar técnicas estadísticas.Utilizaremos R para realizar un análisis estadístico básico.Utilizaremos R para realizar un análisis estadístico básico.","code":""},{"path":"estadística-con-r.html","id":"resumen-estadístico","chapter":"Capítulo 3 Estadística con R","heading":"3.1 Resumen estadístico","text":"El archivo “datos de empleados.sav” de SPSS es un conjunto de datos ampliamente utilizado en análisis estadísticos y gestión de recursos humanos. Este archivo contiene información detallada sobre los empleados de una organización, abarcando diversas variables como edad, género, departamento, puesto de trabajo, salario, años de servicio, nivel educativo y rendimiento laboral. Gracias esta riqueza de datos, los analistas pueden explorar patrones y tendencias dentro de la fuerza laboral, lo que facilita la toma de decisiones informadas en áreas cruciales como la contratación, capacitación, evaluación del desempeño y retención de empleados.Por ejemplo, los datos pueden utilizarse para identificar diferencias salariales entre distintos géneros o departamentos, analizar la relación entre la antigüedad y el rendimiento laboral, o evaluar el impacto de la formación en el desarrollo profesional. Además, la estructura del archivo “datos de empleados.sav” permite la aplicación de diversas técnicas estadísticas y métodos de análisis, como regresión, análisis de varianza y pruebas de hipótesis, proporcionando un enfoque robusto para abordar cuestiones complejas en la gestión de recursos humanos.Las funciones más elementales tienen una sintaxis muy intuitivaPara calcular algunos indicadores básicos se utilizan comandos sencillos","code":"\nlibrary(readxl)\nbbdd <- read_xlsx(\"Datos/Datos_de_empleados.xlsx\")\n# bbdd$fechnac <- as.Date(as.numeric(bbdd$fechnac),format = \"%Y%m%d\")\nknitr::kable(head(bbdd[,c(2,6:7)],6))> # Min, max,...\n> min(bbdd$salario)\n#> [1] 15750\n> max(bbdd$salario)\n#> [1] 135000\n> which.max(bbdd$salario)\n#> [1] 29\n> bbdd$salario[which.max(bbdd$salario)]\n#> [1] 135000\n> sum(bbdd$salario)\n#> [1] 16314875"},{"path":"estadística-con-r.html","id":"medidas-de-posición-central","chapter":"Capítulo 3 Estadística con R","heading":"3.1.1 Medidas de posición central","text":"Las medidas de tendencia central son estadísticas fundamentales que describen el punto medio o típico de un conjunto de datos. Entre las más utilizadas se encuentran la media aritmética, la mediana y la moda. La media aritmética, o promedio, se calcula sumando todos los valores y dividiéndolos por el número total de observaciones, proporcionando una medida del centro basada en todos los datos. La mediana, en cambio, es el valor que divide el conjunto de datos en dos mitades iguales, siendo especialmente útil cuando hay valores atípicos o distribuciones sesgadas, ya que se ve afectada por extremos. La moda es el valor que aparece con mayor frecuencia en el conjunto de datos y puede ser útil para identificar el valor más común en una distribución. Cada una de estas medidas ofrece una perspectiva diferente sobre la centralidad de los datos y se elige según las características del conjunto de datos y el contexto del análisisAlgunos detalles sobre valores perdidosEn R, los valores faltantes están representados por el símbolo NA (disponible).\nLos valores imposibles (por ejemplo, la división por cero) están representados por el símbolo NaN (es un número)Igualmente se obtien en medidas de posición centralLas medidas de posición central, como los cuartiles, deciles y percentiles, son herramientas estadísticas que dividen un conjunto de datos en partes iguales para analizar su distribución. Los cuartiles dividen los datos en cuatro partes: el primer cuartil (Q1) indica el 25% inferior, el segundo cuartil (Q2) es la mediana, y el tercer cuartil (Q3) representa el 75% inferior. Los deciles y percentiles proporcionan divisiones más finas, en diez y cien partes, respectivamente.15750.0 24000.0 28875.0 36937.5 135000.015750.0 22050.0 50027.5","code":"> # Media y Mediana de Salario\n> mean(bbdd$salario)\n#> [1] 34419.57\n> median(bbdd$salario)\n#> [1] 28875\n> \n> # Es fácil calcular la media geométrica\n> x <- bbdd$salario\n> n <- length(x)\n> prod(x)^(1/n)\n#> [1] Inf\n> geom <- exp(mean(log(x)))\n> print(geom)\n#> [1] 31470.09\n> \n> # También la media armónica\n> armo <- 1/mean(1/x)\n> print(armo)\n#> [1] 29366.07> x <- c(1,2,NA,3)\n> mean(x) # devuelve NA\n#> [1] NA\n> sum(x, na.rm=TRUE) # devuelve 2\n#> [1] 6\n> mean(x, na.rm=TRUE) # devuelve 2\n#> [1] 2\nquantile(bbdd$salario)  0%      25%      50%      75%     100% \nquantile(bbdd$salario,c(0,.15,.85)) 0%     15%     85% "},{"path":"estadística-con-r.html","id":"medidas-de-dispersión-absolutas","chapter":"Capítulo 3 Estadística con R","heading":"3.1.2 Medidas de dispersión absolutas","text":"Las medidas de dispersión absolutas cuantifican la variabilidad o dispersión de un conjunto de datos respecto su centro. Entre las más comunes se encuentran el rango, la desviación media y la desviación estándar. El rango es la diferencia entre el valor máximo y el mínimo, proporcionando una medida simple de la extensión de los datos. La desviación media calcula el promedio de las diferencias absolutas entre cada dato y la media, ofreciendo una visión general de la dispersión. La desviación estándar mide la dispersión de los datos respecto la media.","code":"> # Medidas de dispersión absolutas\n> range(bbdd$salario)\n#> [1]  15750 135000\n> IQR(bbdd$salario) # Rango intercuartílico\n#> [1] 12937.5\n> sd(bbdd$salario) # Desviación estándar\n#> [1] 17075.66\n> var(bbdd$salario) # Varianza\n#> [1] 291578214"},{"path":"estadística-con-r.html","id":"medidas-de-asimetría-y-curtosis","chapter":"Capítulo 3 Estadística con R","heading":"3.1.3 Medidas de asimetría y curtosis","text":"Las medidas de asimetría y curtosis evalúan la forma y la distribución de un conjunto de datos. La asimetría indica si los datos están sesgados hacia un lado: una asimetría positiva sugiere una cola larga la derecha, mientras que una negativa indica una cola larga la izquierda. La curtosis mide la “agudeza” de la distribución: una curtosis alta (leptocúrtica) indica colas más pesadas y un pico más pronunciado, mientras que una baja (platicúrtica) señala colas más ligeras y un pico más plano. Estas medidas son cruciales para comprender la distribución de los datos más allá de la media y la dispersión[1] 2.117877[1] 8.30863","code":"\nif(!require(moments)) {install.packages(\"moments\")}\n#> Loading required package: moments\nlibrary(moments)\n\nskewness(bbdd$salario) #nos da el valor de la asimetria de los datos de la variable x\nkurtosis(bbdd$salario) #nos da el achatamiento de la distribucion de los datos de la variable x."},{"path":"estadística-con-r.html","id":"la-función-summary","chapter":"Capítulo 3 Estadística con R","heading":"3.1.4 La función ‘summary()’","text":"La función ‘summary()’ en R proporciona un resumen estadístico de un objeto, como un conjunto de datos o un modelo. Para data frames, incluye medidas como la media, mediana, mínimos, máximos y cuartiles, ofreciendo una visión rápida y completa de las características clave del conjunto de datos","code":"\n# Resumen de la variable\nsummary(bbdd$salario)\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   15750   24000   28875   34420   36938  135000\n# También para una data frame\nBBDD <- as.data.frame(bbdd[,c(3,6:7)])\nsummary(BBDD)\n#>    fechnac             salario           salini     \n#>  Length:474         Min.   : 15750   Min.   : 9000  \n#>  Class :character   1st Qu.: 24000   1st Qu.:12488  \n#>  Mode  :character   Median : 28875   Median :15000  \n#>                     Mean   : 34420   Mean   :17016  \n#>                     3rd Qu.: 36938   3rd Qu.:17490  \n#>                     Max.   :135000   Max.   :79980\n# Un summary por categorías\n# Añadimos variable al data frame\nbbdd$MiVariable <- floor(bbdd$tiempemp/30)\nby(bbdd$salario, bbdd$MiVariable, summary)\n#> bbdd$MiVariable: 2\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   15750   23438   27900   33087   35250  103500 \n#> --------------------------------------------- \n#> bbdd$MiVariable: 3\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#>   16200   25200   30825   38021   40875  135000"},{"path":"estadística-con-r.html","id":"actividad","chapter":"Capítulo 3 Estadística con R","heading":"3.1.5 Actividad","text":"Importar datos del archivo “datos de empleados”:Importar datos del archivo “datos de empleados”:Calcular medidas de tendencia central (media y mediana) y de dispersión (rango, desviación estándar) para una variable numérica, por ejemplo, el salario.Calcular medidas de tendencia central (media y mediana) y de dispersión (rango, desviación estándar) para una variable numérica, por ejemplo, el salario.Calcular medidas de asimetría y curtosis para la misma variable.Calcular medidas de asimetría y curtosis para la misma variable.","code":""},{"path":"estadística-con-r.html","id":"gráficos-con-r","chapter":"Capítulo 3 Estadística con R","heading":"3.2 Gráficos con R","text":"Las gráficas son la mejor forma de simplificar lo complejo.Un buen gráfico suele ser más accesible que una tabla. Sin embargo es muy importante tener claro qué gráfico queremos hacer.Las facilidades gráficas de R constituyen una de las componentes más importantes de este lenguaje.R incluye muchas y muy variadas funciones para hacer gráficas estadísticas estándar: desde gráficos muy simples figuras de gran calidad para incluir en artículos y libros.Permite además construir otras nuevas la medida del usuario (aunque veces hacer cosas simples es fácil).Permite exportar gráficas en distintos formatos: PDF, JPEG, GIF, etc.Para ver una demo de gráficos con colores: demo(graphics).Aquí únicamente veremos algunas de todas las posibilidades.Otra alternativa son los paquetes: ggplot y ggplot2.La función plot()La función plot() en R se utiliza para crear gráficos básicos. Es fundamental para visualizar datos, permitiendo la generación de gráficos de dispersión, líneas, barras, y más. Con diversos argumentos y opciones de personalización, plot() facilita el análisis visual y la interpretación de conjuntos de datos complejos.** Algunas opciones de la función plot()**main: Cambia el título del gráficosub: Cambia el subtítulo del gráficotype: Tipo de gráfico (puntos, líneas, etc.)xlab, ylab: Cambia las etiquetas de los ejesxlim, ylim: Cambia el rango de valores de los ejeslty: Cambia el tipo de línea; lwd: Cambia el grosor de líneacol: Color con el que dibujaGráfico de sectoresLa función boxplot realiza este clásico gráficoLa función boxplot() en R se usa para crear gráficos de caja, que visualizan la distribución de un conjunto de datos través de sus cuartiles. Muestra la mediana, los cuartiles y los posibles valores atípicos, permitiendo identificar la dispersión, simetría y anomalías en los datosEscribe colors() para una lista de colores","code":"> x <- (0:100)/10\n> y <- sin(x)\n> plot(x, y, main=\"Función Seno\")\nplot(x, y, main=\"Seno\", type=\"l\")\nplot(x, y, main=\"Función Seno\", lty=2, col=\"red\", type=\"l\")\nplot(x, cos(x), main=\"Función Coseno\", lty=3, col=\"blue\", type=\"l\",xlim=c(0, 2), ylab=\"cos(x)\")\npie(c(3,5,8))\npie(c(3,5,8), labels=c(\"Uno\",\"Dos\",\"Tres\"),col=c(\"blue\",\"red\",\"green\"),main=\"Mi gráfico\")\n# Con una sola variable\nboxplot(bbdd$salario,col=c('powderblue'))\n# Con las tres variables a la vez\nboxplot(bbdd$salario,bbdd$salini,col=c('powderblue','#FF6D0099'))\n# Los dos juntos\npar(mfrow=c(1,2))\nboxplot(bbdd$salario,bbdd$salini,col=c('powderblue','#FF6D0099'))\nboxplot(log(bbdd$salario),log(bbdd$salini),col=c('powderblue','#FF6D0099'))"},{"path":"estadística-con-r.html","id":"ggplot2","chapter":"Capítulo 3 Estadística con R","heading":"3.2.1 ggplot2","text":"El paquete ggplot2 en R es una herramienta poderosa para la creación de gráficos y visualizaciones de datos. Basado en la gramática de gráficos de Hadley Wickham, ggplot2 permite construir visualizaciones complejas de manera flexible y coherente. Utiliza un sistema de capas, donde cada capa representa una parte del gráfico, como los datos, los estéticos, y los elementos geométricos. Los usuarios pueden personalizar gráficos con facilidad, añadiendo títulos, etiquetas, y temas. Además, ggplot2 facilita la visualización de relaciones entre variables, distribuciones, y patrones, convirtiéndose en una opción popular para el análisis exploratorio de datos y la comunicación de resultadosCon ggplot se pueden hacer boxplots muy bonitos[https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf]También violin-plots[https://www.data--viz.com/caveat/boxplot.html]HistogramaSe puede incluir la curva normal sobre el histograma","code":"\nlibrary(ggplot2)\nDistr <- as.factor(bbdd$sexo)\nqplot( x=Distr , y=salario , data=bbdd , geom=c(\"boxplot\",\"jitter\") , fill=Distr) + theme_bw()\n#> Warning: `qplot()` was deprecated in ggplot2 3.4.0.\n#> This warning is displayed once every 8 hours.\n#> Call `lifecycle::last_lifecycle_warnings()` to see where\n#> this warning was generated.\nDistr <- as.factor(bbdd$catlab)\nqplot( x=Distr , y=salario , data=bbdd , geom=c(\"boxplot\",\"jitter\") , fill=Distr) +\ntheme_bw()\nhist(log(bbdd$salario), breaks=20, col = \"red\")\ng <- log(bbdd$salario)\nm <- mean(g)\nstd<-sqrt(var(g))\nhist(g, density=10, breaks=20, prob=TRUE,col=\"blue\", \n     xlab=\"log(Renta)\", \n     main=\"Curva normal sobre histograma\")\ncurve(dnorm(x, mean=m, sd=std), \n      col=\"red\", lwd=4, add=TRUE)"},{"path":"estadística-con-r.html","id":"actividad-4","chapter":"Capítulo 3 Estadística con R","heading":"3.2.2 Actividad","text":"Crear un histogramaCrear un gráfico de caja (boxplot) para comparar los salarios por departamento","code":"\nggplot(empleados, aes(x = salario)) +\n  geom_histogram(binwidth = 1000, fill = \"blue\", color = \"black\") +\n  labs(title = \"Distribución del Salario de los Empleados\", x = \"Salario\", y = \"Frecuencia\") +  theme_minimal()\nggplot(empleados, aes(x = departamento, y = salario, fill = departamento)) +\n  geom_boxplot() +\n  labs(title = \"Distribución del Salario por Departamento\", x = \"Departamento\", y = \"Salario\") +\n  theme_minimal()"},{"path":"estadística-con-r.html","id":"test-hipótesis","chapter":"Capítulo 3 Estadística con R","heading":"3.3 Test Hipótesis","text":"","code":""},{"path":"estadística-con-r.html","id":"un-simple-t-test-de-igualdad-de-medias","chapter":"Capítulo 3 Estadística con R","heading":"3.3.1 Un simple t-test de igualdad de medias","text":"El contraste de medias basado en la t de Student es una técnica estadística para comparar las medias de dos grupos y determinar si hay una diferencia significativa entre ellas. Se utiliza cuando los datos siguen una distribución normal y el tamaño de la muestra es pequeño. La prueba t calcula el valor t, que se compara con un valor crítico de la distribución t para decidir si se rechaza la hipótesis nula de igualdad de medias. Existen dos tipos principales: la prueba t para muestras independientes, que compara dos grupos distintos, y la prueba t para muestras dependientes, que compara dos mediciones en el mismo grupodata: bbdd$salario\nt = -7.1151, df = 473, p-value = 4.168e-12\nalternative hypothesis: true mean equal 40000\n95 percent confidence interval:\n32878.40 35960.73\nsample estimates:\nmean x\n34419.57data: bbdd$salario\nt = -0.74005, df = 473, p-value = 0.7702\nalternative hypothesis: true mean greater 35000\n95 percent confidence interval:\n33126.96 Inf\nsample estimates:\nmean x\n34419.57Puede usar la opción var.equal = TRUE para especificar varianzas iguales y una estimación de varianza agrupada. Puede usar la opción alternative = “less” o alternative = “greater” para especificar una prueba de una cola","code":"\n# Contrastar si la Renta media de 2015 es 40000 euros\nt.test(bbdd$salario,mu=40000)One Sample t-test\nt.test(bbdd$salario,mu=35000,alternative=\"greater\")One Sample t-test"},{"path":"estadística-con-r.html","id":"un-simple-t-test-de-igualdad-de-medias-pareadas","chapter":"Capítulo 3 Estadística con R","heading":"3.3.2 Un simple t-test de igualdad de medias pareadas","text":"Un poco mas complicado: ANOVA de un factorEl ANOVA de un factor (Análisis de Varianza) es una técnica estadística utilizada para comparar las medias de tres o más grupos independientes y determinar si existen diferencias significativas entre ellas. Este análisis evalúa si la variabilidad entre las medias de los grupos es mayor que la variabilidad dentro de los grupos. El ANOVA de un factor calcula un estadístico F, que compara la variabilidad entre los grupos con la variabilidad dentro de los grupos. Si el valor F es suficientemente alto, se rechaza la hipótesis nula de igualdad de medias. Es útil para experimentos con un único factor categórico y múltiples niveles.","code":"\nt.test(bbdd$salario,bbdd$salini)\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  bbdd$salario and bbdd$salini\n#> t = 20.152, df = 665.3, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  15707.74 19099.22\n#> sample estimates:\n#> mean of x mean of y \n#>  34419.57  17016.09\nt.test(log(bbdd$salario),log(bbdd$salini))\n#> \n#>  Welch Two Sample t-test\n#> \n#> data:  log(bbdd$salario) and log(bbdd$salini)\n#> t = 28.163, df = 932.96, p-value < 2.2e-16\n#> alternative hypothesis: true difference in means is not equal to 0\n#> 95 percent confidence interval:\n#>  0.6394884 0.7352878\n#> sample estimates:\n#> mean of x mean of y \n#> 10.356793  9.669405\ngroup <- as.factor(bbdd$catlab)\nlevels(group) <- c(\"asa\",\"aaas\",\"asddd\")\nanova <- aov(bbdd$salario ~ group, data = bbdd)\nsummary(anova)\n#>              Df    Sum Sq   Mean Sq F value Pr(>F)    \n#> group         2 8.944e+10 4.472e+10   434.5 <2e-16 ***\n#> Residuals   471 4.848e+10 1.029e+08                   \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nTukeyHSD(anova)\n#>   Tukey multiple comparisons of means\n#>     95% family-wise confidence level\n#> \n#> Fit: aov(formula = bbdd$salario ~ group, data = bbdd)\n#> \n#> $group\n#>                 diff       lwr       upr     p adj\n#> aaas-asa    3100.349 -1657.805  7858.503 0.2768689\n#> asddd-asa  36139.258 33251.225 39027.291 0.0000000\n#> asddd-aaas 33038.909 27761.979 38315.839 0.0000000"},{"path":"estadística-con-r.html","id":"test-de-normalidad-paramétricos","chapter":"Capítulo 3 Estadística con R","heading":"3.3.3 Test de normalidad paramétricos","text":"Test Paramétricos: K-S y ShapiroLos test de normalidad Kolmogorov-Smirnov (KS) y Shapiro-Wilk son utilizados para evaluar si un conjunto de datos sigue una distribución normal. El test KS compara la distribución empírica de los datos con una distribución normal teórica, evaluando las diferencias entre las funciones de distribución acumulada observada y esperada. El test Shapiro-Wilk, en cambio, evalúa la normalidad ajustando una estadística basada en los valores ordenados de la muestra. Mientras que el test KS es más general y se aplica cualquier distribución, el Shapiro-Wilk es más específico para la normalidad y suele ser más potente para muestras pequeñas. Ambos test ayudan validar supuestos en análisis estadísticos.data: g\nD = 0.13563, p-value = 5.343e-08\nalternative hypothesis: two-sideddata: g\nW = 0.92568, p-value = 1.438e-14Tenga en cuenta que, la prueba de normalidad es sensible al tamaño de la muestra. Las muestras pequeñas con mayor frecuencia pasan las pruebas de normalidad. Por lo tanto, es importante combinar la inspección visual y la prueba de significación para tomar la decisión correcta.","code":"\ng <- log(bbdd$salario)\nks.test(g, pnorm, mean(g), sd(g))\n#> Warning in ks.test.default(g, pnorm, mean(g), sd(g)): ties\n#> should not be present for the Kolmogorov-Smirnov testAsymptotic one-sample Kolmogorov-Smirnov test\nshapiro.test(g)Shapiro-Wilk normality test"},{"path":"estadística-con-r.html","id":"distribuciones-bivariadas","chapter":"Capítulo 3 Estadística con R","heading":"3.4 Distribuciones bivariadas","text":"","code":""},{"path":"estadística-con-r.html","id":"tablas-de-contingencia","chapter":"Capítulo 3 Estadística con R","heading":"3.4.1 Tablas de contingencia","text":"Una tabla de correlación muestra las relaciones entre variables cuantitativas, indicando la fuerza y dirección de la asociación mediante coeficientes de correlación, como Pearson o Spearman. Permite identificar patrones de dependencia lineal o lineal entre pares de variables. Por otro lado, una tabla de contingencia, también conocida como tabla de frecuencia cruzada, se utiliza para analizar la relación entre dos variables categóricas. Muestra la frecuencia de ocurrencia conjunta de las categorías de las variables, facilitando el análisis de asociaciones y la prueba de independencia mediante chi-cuadrado. Ambas tablas son herramientas fundamentales para la exploración y análisis de datos en estadística.La función table() calcula tablas de frecuencias partir de factores.","code":"> msa <- mean(bbdd$salario) # Salario\n> msi <- mean(bbdd$salini) # SAl ini\n> Ricos <- bbdd$salario > msa # ricos/pobres\n> Ricosi <- bbdd$salini > msi # densos/no dendos\n> # Distribuciones de frecuencia unidimensional\n> table(Ricos)\n#> Ricos\n#> FALSE  TRUE \n#>   329   145\n> table(Ricosi)\n#> Ricosi\n#> FALSE  TRUE \n#>   345   129\n> # Distribuciones de frecuencia bidimensionales\n> TaRiRi <- table(Ricos,Ricosi)\n> print(TaRiRi)\n#>        Ricosi\n#> Ricos   FALSE TRUE\n#>   FALSE   310   19\n#>   TRUE     35  110\n> TaRiSx <- table(Ricos,bbdd$sexo)\n> print(TaRiSx)\n#>        \n#> Ricos     h   m\n#>   FALSE 138 191\n#>   TRUE  120  25\n> \n> # Como siempre, hay gráficos informativos\n> library(graphics)\n> mosaicplot(TaRiRi,main = \"Ricos vs Ricosi\")> mosaicplot(TaRiSx,main = \"Ricos vs Sexo\",color = TRUE)"},{"path":"estadística-con-r.html","id":"algo-de-test-de-independencia","chapter":"Capítulo 3 Estadística con R","heading":"3.4.2 Algo de test de independencia","text":"El test de independencia chi-cuadrado (\\(\\chi^2\\)) se utiliza para evaluar si dos variables categóricas están asociadas o son independientes entre sí. Se basa en la comparación entre las frecuencias observadas en una tabla de contingencia y las frecuencias esperadas si las variables fueran independientes. Calcula un estadístico \\(\\chi^2\\) que se compara con un valor crítico de la distribución chi-cuadrado, considerando el número de grados de libertad. Si el estadístico \\(\\chi^2\\) es mayor que el valor crítico, se rechaza la hipótesis nula de independencia, indicando una asociación significativa entre las variables. Este test es útil para analizar relaciones en datos categóricos.","code":"> # Contrastes Chi2 de independencia\n> chisq.test(TaRiRi)\n#> \n#>  Pearson's Chi-squared test with Yates' continuity\n#>  correction\n#> \n#> data:  TaRiRi\n#> X-squared = 246.05, df = 1, p-value < 2.2e-16\n> chisq.test(TaRiSx)\n#> \n#>  Pearson's Chi-squared test with Yates' continuity\n#>  correction\n#> \n#> data:  TaRiSx\n#> X-squared = 65.953, df = 1, p-value = 4.618e-16"},{"path":"estadística-con-r.html","id":"correlaciones","chapter":"Capítulo 3 Estadística con R","heading":"3.4.3 Correlaciones","text":"Una matriz de correlaciones es una tabla que muestra las correlaciones entre múltiples variables cuantitativas. Cada celda indica el coeficiente de correlación entre un par de variables, facilitando la identificación de relaciones lineales y patrones de asociación en grandes conjuntos de datos.","code":"> cor(bbdd$salario,bbdd$salini)\n#> [1] 0.8801175\n> cor(bbdd$salario,bbdd$expprev)\n#> [1] -0.09746693\n> # Un plot con correlaciones\n> library(ggcorrplot)\n> ggcorrplot(cor(bbdd[,6:9]),lab_size = 3,hc.order = TRUE,method = \"circle\",lab = TRUE)> # para saber si es significativamente distinta de cero\n> cor.test(bbdd$salario,bbdd$salini)\n#> \n#>  Pearson's product-moment correlation\n#> \n#> data:  bbdd$salario and bbdd$salini\n#> t = 40.276, df = 472, p-value < 2.2e-16\n#> alternative hypothesis: true correlation is not equal to 0\n#> 95 percent confidence interval:\n#>  0.8580696 0.8989267\n#> sample estimates:\n#>       cor \n#> 0.8801175\n> plot(bbdd$salario,bbdd$salini, main=\"Salario vs Salario Inicial\", col='red')"},{"path":"regresión-lineal-múltiple.html","id":"regresión-lineal-múltiple","chapter":"Capítulo 4 Regresión Lineal Múltiple","heading":"Capítulo 4 Regresión Lineal Múltiple","text":"Esta sesión trata sobre la regresión lineal, que es un enfoque sencillo dentro de los algoritmos de aprendizaje supervisado y que se utiliza principalmente para predecir respuestas cuantitativas. Aunque puede parecer menos emocionante que otros métodos estadísticos más modernos, la regresión lineal sigue siendo una herramienta ampliamente utilizada y útil. Queremos destacar que es una base importante para comprender métodos más complejos, ya que muchos de estos métodos se pueden ver como extensiones de la regresión lineal. Por lo tanto es importante comprender la regresión lineal antes de abordar métodos de aprendizaje más avanzados. El enfoque principal de esta sesión es revisar las ideas clave detrás del modelo de regresión lineal y el método de mínimos cuadrados utilizado para ajustar este modelo.La regresión múltiple tiene como objetivo analizar un modelo que pretende explicar el comportamiento de una variable (endógena, explicada o dependiente), que se denota como \\(Y\\), utilizando la información proporcionada por los valores tomados por un conjunto de variables explicativas (exógenas o independientes), que se denotan por \\(X_1, X_2,\\dots, X_p\\). El modelo lineal (modelo econométrico) viene dado de la forma:donde \\(\\beta_0,\\beta_1,\\dots,\\beta_p\\) son parámetros desconocidos (o coeficientes) y \\(e\\) es un termino aleatorio de error, que es independiente de las variables explicativas \\(X_1, X_2,\\dots, X_p\\) y tiene media cero.","code":""},{"path":"regresión-lineal-múltiple.html","id":"estimación-de-los-parámetros.-método-de-los-mínimos-cuadrados.","chapter":"Capítulo 4 Regresión Lineal Múltiple","heading":"4.1 Estimación de los parámetros. Método de los mínimos cuadrados.","text":"Supongamos que tenemos una muestra de tamaño \\(n\\) en la que hemos observado las variables\n\\[Y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\y_n \\end{pmatrix};\\quad X=(1, X_1, X_2,\\dots, X_p)=\\begin{pmatrix}\n1&x_{11}&x_{12}&\\dots&x_{1p}\\\\\n1&x_{21}&x_{22}&\\dots&x_{2p}\\\\\n&&&\\vdots&\\\\\n1&x_{n1}&x_{n2}&\\dots&x_{np}\n\\end{pmatrix}\\]Si denotamos por \\[\\beta=\\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_p\\end{pmatrix}\\]\nel modelo econométrico se puede expresar en forma matricial como\n\\[Y=X\\beta+E\\]\nTeoremaSi las columnas de \\(X\\) son linealmente independientes, entonces el estimador mínimo cuadrático de los coeficientes del modelo sería\n\\[\\widehat{\\beta}=(X^tX)^{-1}X^tY\\]\ndonde \\(X^t\\) denota la matriz traspuesta de X.Demostración: \nDenotemos por \\(\\widehat{Y}=X\\widehat{\\beta}\\) tenemois que los terminos de error (o residuos) se pueden escribir como \\(e_i=y_i-\\widehat{y}_i=y_i-\\widehat{\\beta_0}+\\widehat{\\beta_1}x_{i1}-\\widehat{\\beta_2}x_{i2}-\\dots-\\widehat{\\beta_p}x_{ip}\\). El método de estimación mínimo cuadrático consiste en obtener el vector de coeficientes \\(\\widehat{\\beta}\\) que minimiza la suma de los errores al cuadrado. Teniendo en cuenta que al ser escalares \\(Y^tX\\widehat{\\beta}=\\widehat{\\beta}^tX^tY\\), se verifica que la suma de los errores al cuadrado se puede escribir comoPara minimizar \\(RSS\\) tenemos que resolver la ecuación\n\\[\\frac{\\partial RSS}{\\partial \\widehat{\\beta}}=-2Y^tX+2\\widehat{\\beta}^tX^tX=0\\]\ny por tanto como las columnas de \\(X\\) son linealmente independientes existe la inversa de la matriz \\(X^tX\\) obteniendo \\[\\widehat{\\beta}=(X^tX)^{-1}X^tY\\] tal y como queriamos demostrar.\\(\\square\\)Obsérvese que \\(\\widehat{\\beta}_i\\) midel el cambbio en \\(Y\\) por cada cambio unitario en \\(X_i\\) para todo \\(=1, 2,\\dots, p\\).\nAdemás si comprobamos que los residuos son homocedásticos, independientes e identicamente distribuidos como una distribución \\(N(0,\\sigma^2)\\), tenemos que \\(Y\\) se distribuye como \\(N(X\\widehat{\\beta},\\sigma^2 )\\). Un estimador de la varianza del error sería:\n\\[\\sigma^2=\\frac{1}{n-(p+1)}\\sum_{=1}^ne_i^2=\\frac{1}{n-(p+1)}\\sum_{=1}^n(y_i-\\widehat{y}_i)^2\\]Como \\(\\widehat{\\beta}=(X^tX)^{-1}X^tY\\) se verifica que su media es \\[\\mathbb{E}(\\widehat{\\beta})=(X^tX)^{-1}X^tE(Y)=(X^tX)^{-1}X^tX\\beta=\\beta.\\] Si queremos hacer inferencia para contrastar una hipotesis nula del estilo \\(H_0: \\, \\beta=0\\) tenemos que \\[Var(\\widehat{\\beta})=(X^tX)^{-1}X^tVar(Y)X(X^tX)^{-1}=(X^tX)^{-1}X^t\\sigma^2X(X^tX)^{-1}=\\sigma^2(X^tX)^{-1}\\] y como \\(\\widehat{\\beta}\\) es una combinmacion lineal de elementos de \\(Y\\) bajo \\(H_0\\) se verifica que \\[\\widehat{\\beta}\\sim N(0,\\sigma^2(X^tX)^{-1}).\\] Esto nos permite hacer inferencia sobre la significatividad de los parámetros estimados \\(\\widehat{\\beta}\\), contrastando si significativamente distintos de cero, así como calcular intervalos de confianza para los mismos.Los coeficientes estimados \\(\\widehat{\\beta}\\) nos proporcionan información sobre cuanto aporta cada variable independiente \\(X_1, X_2,\\dots, X_p\\) la estimación de \\(Y\\).","code":""},{"path":"regresión-lineal-múltiple.html","id":"descomposición-de-la-varianza-y-bondad-del-ajuste.","chapter":"Capítulo 4 Regresión Lineal Múltiple","heading":"4.2 Descomposición de la Varianza y bondad del ajuste.","text":"Basado en la ley del valor esperado total que dice \\(\\mathbb{E}(Y)=\\mathbb{E}(\\mathbb{E}(Y|X))\\) podemos demostrar el siguiente resultado.Proposición:\nSi \\(X\\) e \\(Y\\) son dos variables aleatorias definidas en el mismo espacio de probabilidad y suponemos que \\(Y\\) tiene varianza finita, entonces\n\\[Var(Y)=\\mathbb{E}(Var(Y|X))+Var(\\mathbb{E}(Y|X))\\]Demostración:\nSabemos que \\(Var(Y)=\\mathbb{E}(Y^2)-\\mathbb{E}(Y)^2\\) y por tanto \\(\\mathbb{E}(Y^2)=Var(Y)+\\mathbb{E}(Y)^2\\). Luego aplicando la ley del valor esperado total la expresion anterior tenemos que\n\\[\\mathbb{E}(Y^2)=\\mathbb{E}(Var(Y|X)+\\mathbb{E}(Y|X)^2)=\\mathbb{E}(Var(Y|X))+\\mathbb{E}(\\mathbb{E}(Y|X))^2.\\] Restando \\(\\mathbb{E}(Y)^2\\) en ambos lados de la igualdad anterior y applicando de nuevo ley del valor esperado total \\(\\mathbb{E}(Y)^2=\\mathbb{E}(\\mathbb{E}(Y|X))^2\\) tenemos quetal y como queriamos demostrar. \\(\\square\\)Corolario\nDado el modelo de regresión lineal \\(Y=X\\beta+E\\) se verifica que \\(Var(Y)=Var(E)+Var(\\widehat{Y})\\)Demostración:\nObsérvese que \\(Var(Y|X)=Var(E)=\\sigma^2\\) y \\(\\mathbb{E}(Y|X)=\\widehat{Y}\\). Por lo tanto como consecuencia de la proposición anterior tenemos que \\(Var(Y)=\\sigma^2+Var(\\widehat{Y})\\), es decir la varianza total de \\(Y\\) se descompone como la suma de la varianza explicada por \\(\\widehat{Y}\\) y la varianza de los errores. \\(\\square\\)","code":""},{"path":"regresión-lineal-múltiple.html","id":"coeficientes-de-determinación-y-correlación.","chapter":"Capítulo 4 Regresión Lineal Múltiple","heading":"4.2.1 Coeficientes de determinación y correlación.","text":"Se define el coeficiente de determinación \\(R^2\\) como la proporcion de la varianza total que es recogida por la varianza de la variable ajustada, es decir \\[R^2=\\frac{Var(\\widehat{Y})}{Var(Y)}=1-\\frac{Var(E)}{Var(Y)}\\]\nDe esta última expresión es inmediato ver que \\(0\\leq R^2\\leq 1\\). Observese que si \\(R^2=0\\) significa que \\(Var(E)=Var(Y)\\) y por la proposición anterior \\(Var(\\widehat{Y})=0\\), por tanto \\(\\widehat{Y}=E(Y)\\) y el modelo recoge nada de la variabilidad total y como consecuencia el ajuste es malo. En el otro extremo, si \\(R^2=1\\) se sigue que \\(Var(\\widehat{Y})=Var(Y)\\) y \\(Var(E)=0\\) y por tanto el modelo recoge toda la variabilidad obteniendo \\(Y=\\widehat{Y}\\) y \\(E=0\\).\nDe esta manera concluimos que cuanto más cercano esté \\(R^2\\) 1 mejor será el ajuste del modelo.Proposición: Dado el modelo de regresión \\(Y=X\\beta+E\\) se verifica que \\[Cov(\\widehat{Y},E)=\\widehat{Y}^tE=0\\]Demostración: Como \\(E\\) tiene media cero, se sigue que \\(Cov(\\widehat{Y},E)=\\mathbb{E}(\\widehat{Y}^tE)=\\mathbb{E}(\\widehat{\\beta}^tX^t(Y-\\widehat{Y}))=\\\\=\\mathbb{E}(Y^tX(X^tX)^{-1}X^tY-Y^tX(X^tX)^{-1}X^tX(X^tX)^{-1}X^tY)=0 \\quad\\square\\)Definition Dadas dos variables estadísticas \\(U\\) y \\(V\\), se define el coefficiente de correlacion de Pearson de \\(U\\) y \\(V\\) como\n\\[Cor(U,V)=\\rho_{UV}=\\frac{Cov(U,V)}{\\sqrt{Var(U)Var(V)}}\\]Teorema:El coeficiente de determinacion \\(R^2\\) coincide con el coeficiente de correlacion dde Pearson de las variables \\(Y\\) e \\(\\widehat{Y}\\) al cuadrado:\n\\[R^2=Cor(\\widehat{Y},Y)^2\\]Demostración: Como \\(\\mathbb{E}(\\widehat{Y})=\\mathbb{E}(Y)\\) y \\(Cov(\\widehat{Y},E)=0\\) tenemos quey por tanto \\(Cor(\\widehat{Y},Y)=R^2\\) tal y como queríamos demostrar.\\(\\quad \\square\\)","code":""},{"path":"regresión-lineal-múltiple.html","id":"coeficientes-de-determinación-semi-parcial-y-parcial","chapter":"Capítulo 4 Regresión Lineal Múltiple","heading":"4.2.2 Coeficientes de determinación semi-parcial y parcial","text":"Para conocer cuanto contribuye la variable \\(X_k\\) de manera única al modelo de regresión, podemos pensar en cuanto se modifica el coeficiente de determinación al excluir esta variable en la regresión lineal. De esta manera si denotamos por \\(R_{-k}^2\\) el coefficiente de determinación del modelo de regresión lineal omitiendo la variable \\(X_k\\) la cantidad \\[R^2-R_{-k}^2\\] es una manera de cuantificar cuanta información única sobre \\(Y\\) en \\(X_k\\) está explicada por el resto de variables indeependientes. Esta cantidad es conocida como coeficiente de determinación semi-parcial. Se define el coefficiente de determinacion parcial  como \\[\\frac{R^2-R^2_{-k}}{1-R^2_{-k}}\\]Teorema: Sea \\(U\\) el residuo de la regresión lineal de una variable independiente \\(X_k\\) sobre el resto de las variables independientes \\(X_i\\) con \\(\\neq k\\). Denotemos por \\(\\widehat{Y}_{-k}\\) y \\(V\\) los valores ajustados de \\(Y\\) y los residuos cuando hacemos la resgesion de \\(Y\\) sobre todas las variables independientes excepto \\(X_k\\) respectivamente. Entonces el coeficiente de determinación semi-parcial y el coeficiente de determinación parcial se pueden calcular como:\\[R^2-R^2_{-k}=Cor(Y,U)^2 \\quad\\quad\\quad\\quad\\frac{R^2-R^2_{-k}}{1-R^2_{-k}}=Cor(U,V)^2\\]\nDemostración: La demostración la haremos utilizando algebra lineal. Para ello utilizaremos algunos conceptos básicos. Denotamos el producto escalar de dos vectores \\(S\\) y \\(W\\) por \\(\\langle S, W \\rangle=S^tW=\\sum_{=1}^n s_iw_i\\) y su norma por \\(||S||=\\sqrt{\\langle S, S\\rangle}\\). Sabemos que si \\(S\\) y \\(W\\) son ortogonales entonves \\(\\langle S,W\\rangle=0\\). Sea la matriz \\(P_{-k}\\) la proyección ortogonal en el espacio generado por todas las variables independientes excepto \\(X_k\\). Es conocido que la matriz \\(P_{-k}\\) es simétrica e idempotente, es decir \\(P_{-k}^2=P_{-k}\\). Entonces se tiene que \\(P_{-k}Y=\\widehat{Y}_{-k}\\) y \\(U=X_k-\\widehat{X}_k=(-P_{-k})X_k\\). Además \\(U\\) es ortogonal todos los \\(X_i\\) con \\(\\neq k\\) ya que \\(\\langle U,X_i\\rangle=U^tX_i=X_k^t(-P_{-k})X_i=X_k^t(X_i-X_i)=0\\). Por lo tanto, \\(\\widehat{Y}\\) que es la proyeccion ortogonal de \\(Y\\) en el espacio generado por todas las variables predictoras se puede descomponer como la suma de la proyección ortogonal sobre el espacio generado por todas la variables excepto \\(X_k\\) y uno ortogonal este, e.g. el generado por \\(U\\). Pero la proyeccion de \\(Y\\) en el espacio generado por \\(U\\) es el valor estimado de la recta de regrsion de \\(Y\\) sobre \\(U\\), es decir \\(\\frac{\\langle Y,U\\rangle}{||U||}U\\). Por tanto tenemos que \\[\\widehat{Y}=\\widehat{Y}_{-k}+\\frac{\\langle Y,U\\rangle}{||U||^2}U\\] y teniendo en cuenta que \\(Y_{-k}\\) y \\(U\\) son oryogonales calculando la norma al cuadrado en la expresion anterior \\[||Y||^2=||Y_{-k}||^2+\\frac{\\langle Y,U\\rangle^2}{||U||^2}.\\] Utilizando estas expresiones y que \\(\\mathbb{E}(Y)=\\mathbb{E}(\\widehat{Y})\\) tenemos queLuego el coeficiente de determinación semi-parcial queda \\(R^2-R^2_{-k}=Cor(Y,U)^2\\).Por otro lado como \\(1-R^2_{-k}=1-(1-\\frac{Var(V)}{Var(Y)})=\\frac{||V||^2}{||Y-\\mathbb{E}(Y)||^2}\\) y \\(\\widehat{Y}_{-k}\\) es ortogonal \\(U\\), tenemos que el coeficiente de determinación parcial es\\[\\frac{R^2-R^2_{-k}}{1-R^2_{-k}}=\\frac{\\frac{\\langle Y,U\\rangle^2}{||Y-\\mathbb{E}(Y)||^2||U||^2}}{\\frac{||V||^2}{||Y-\\mathbb{E}(Y)||^2}}=\\frac{\\langle Y,U\\rangle^2}{||U||^2||V||^2}=\\\\\n=\\frac{\\langle Y-\\widehat{Y}_{-k},U\\rangle^2}{||U||^2||V||^2}=\\frac{\\langle V,U\\rangle^2}{||U||^2||V||^2}=Cor(U,V)^2\\]\ntal y como queríamos demostrar.\\(\\,\\square\\)Como consecuencia el coeficiente de determinación semi-parcial tiene dos interpretaciones:1.- La mejora en \\(R^2\\) que resulta de introducir la variable \\(X_k\\) en el modelo de regresión que ya incluía al resto de variables independientes.2.- Es el coeficiente de determinación de la regresión lineal simple de \\(Y\\) sobre \\(U\\).Asimismo, el coeficiente de determinación parcial se puede interpretar como:1.- La fracción de la máxima mejorta posible en \\(R^2\\) al que contribuye la variable \\(X_k\\).2.- Es el coeficiente de determinación de la regresión simple de \\(V\\) sobre \\(U\\).","code":""},{"path":"regresión-lineal-múltiple.html","id":"ejemplo-de-regresión-lineal-múltiple-con-r","chapter":"Capítulo 4 Regresión Lineal Múltiple","heading":"4.3 Ejemplo de regresión lineal múltiple con R","text":"Carguemos la base de datos de empleadosExpliquemos con un modelo de regresión lineal el salario de los trabajadores como función del salario inicial, el nivel educativo del trabajador y los meses de experiencia previa. Para ello en R tenemos que indicar las variables relacionar de la siguiente manera:El modelo de regresión se estima utilizando el comando lm y un resumen del modelo aparece con el comando summary tal y como mostramos continuación:Obsérvese, que el salario inicial y el nivel educativo tienen un impacto positivo y significativo en el salario de la persona, mientras que los años de experiencia, pesar de ser significativo en el modelo, tiene un impacto negativo en el salario del empleado. EL coeficiente de determinación del modelo es de \\(R^2=0.8015\\) y por tanto podemos decir que el modelo recoge aproximadamente el \\(80\\%\\) de la varianza de la variable dependiente lo que significa que es un ajuste bueno.Podemos obtener los valores predichos por el modelo y representarlos graficamente.Podemos ahora calcular las correlaciones parciales de todas las variables intervinientes en el modelo. Por ejemplo calculemos el coeficiente de correlación parcial de la variable expprev en el modelo *modols. Para ello calculamos \\(R^2\\) y \\(R^2_{-expprev}\\) tal y como sigueOtra forma de calcularlo sería calculando \\(1-SR_{full}/SR_{expprev}\\) donde \\(SR_{full}\\) y \\(SR_{expprev}\\) son la suma de los cuadrados de los residuos del modelo con todas las variables \\(modols\\) y el modelo reducido sin la variable expprev, \\(modsibexpprev\\) respectuivamente.Por tanto tenemos que la mejora máxima en el coeficiente de determinación, \\(R^2\\), que produce la incorporación de la variable expprev es del \\(4.38\\%\\).Ejercicio:1.- Calcula e interpreta los coeficientes de correlación parcial del resto de variables del modelo modols.2.- Investiga el efecto que puede tener el género del empleado en el salario.","code":"> library(readxl)\n> datos <- read_xlsx(\"Datos/Datos_de_empleados.xlsx\")> formula<- datos$salario~datos$salini+datos$educ+datos$expprev> modols<-lm(formula)\n> summary(modols)\n#> \n#> Call:\n#> lm(formula = formula)\n#> \n#> Residuals:\n#>    Min     1Q Median     3Q    Max \n#> -28853  -4167  -1172   2724  48701 \n#> \n#> Coefficients:\n#>                 Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)   -3.662e+03  1.935e+03  -1.892   0.0591 .  \n#> datos$salini   1.749e+00  5.989e-02  29.198  < 2e-16 ***\n#> datos$educ     7.360e+02  1.687e+02   4.363 1.58e-05 ***\n#> datos$expprev -1.673e+01  3.605e+00  -4.641 4.51e-06 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7632 on 470 degrees of freedom\n#> Multiple R-squared:  0.8015, Adjusted R-squared:  0.8002 \n#> F-statistic: 632.6 on 3 and 470 DF,  p-value: < 2.2e-16> Yfit=modols$fitted.values\n> xx=seq(1,length(Yfit))\n> plot(xx,modols$fitted.values,pch=19,col='red',ylim=c(8500,136000),xlab=\"\",ylab=\"Salario\", xaxt=\"n\")\n> axis(1, at = seq(1,length(Yfit),20))\n> par(new=TRUE)\n> plot(datos$salario,pch=19,col='blue',xlab=\"\",ylab=\"Salario\",axes=FALSE)> par(new=FALSE)> restot=summary(modols)\n> R2=restot$r.squared\n> modsinexpprev=lm(datos$salario~datos$salini+datos$educ)# modelo sin la variable expprev\n> resexpprev=summary(modsinexpprev)\n> R2expprev=resexpprev$r.squared\n> PartialrR2expprev=(R2-R2expprev)/(1-R2expprev)\n> print(PartialrR2expprev)\n#> [1] 0.04381444> PartialrR2expprev2=1-sum(modols$residuals^2)/sum(modsinexpprev$residuals^2)\n> print(PartialrR2expprev2)\n#> [1] 0.04381444"},{"path":"análisis-de-componentes-principales.html","id":"análisis-de-componentes-principales","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"Capítulo 5 Análisis de Componentes Principales","text":"","code":""},{"path":"análisis-de-componentes-principales.html","id":"introducción","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.1 Introducción","text":"El análisis de componentes principales (ACP) es una técnica fundamental en estadística y análisis multivariado que se utiliza para simplificar y entender la estructura subyacente en conjuntos de datos complejos. Esta metodología tiene aplicaciones en diversas disciplinas, como la estadística, la ingeniería, la biología, la economía y la ciencia de datos.En esencia, el ACP busca transformar un conjunto de variables correlacionadas en un nuevo conjunto de variables correlacionadas, conocidas como componentes principales. Estos componentes se ordenan en función de la cantidad de varianza que explican en los datos originales, lo que permite destacar las direcciones principales de variabilidad en el conjunto de datos.La idea central detrás del ACP es reducir la dimensionalidad del conjunto de datos, manteniendo la mayor cantidad posible de información. Al proyectar los datos en un espacio de menor dimensión definido por los componentes principales, se facilita la visualización y la interpretación de patrones y tendencias en los datos, lo que puede ser crucial para la toma de decisiones informada.lo largo de este proceso, el ACP proporciona una herramienta valiosa para identificar patrones subyacentes, eliminar redundancias y resaltar las relaciones más importantes entre las variables, lo que contribuye significativamente la simplificación y comprensión de conjuntos de datos complejos.","code":""},{"path":"análisis-de-componentes-principales.html","id":"antes-de-empezar","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.2 Antes de empezar","text":"Antes de aplicar el ACP, es importante considerar algunas condiciones y realizar ciertos pasos:Tipo de Variables: El ACP se utiliza comúnmente con variables cuantitativas. Las variables deben ser de escala numérica, ya que el método implica operaciones algebraicas y estadísticas que requieren números. Si tienes variables categóricas, es posible que necesites realizar alguna transformación o utilizar técnicas diferentes.Escalas de Medición: Las variables deben tener escalas de medición comparables. Si las unidades de medida son muy diferentes entre las variables, es recomendable estandarizar o normalizar las variables antes de aplicar el ACP para evitar que una variable con una escala más grande domine la variabilidad.Correlación entre Variables: El ACP asume que existe cierta correlación entre las variables originales. Si las variables están completamente incorrelacionadas, el análisis aportará información significativa.Linealidad: El ACP asume linealidad entre las variables. Si la relación entre las variables es altamente lineal, el ACP puede capturar adecuadamente la estructura subyacente de los datos.","code":""},{"path":"análisis-de-componentes-principales.html","id":"el-acp","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.3 El ACP","text":"","code":""},{"path":"análisis-de-componentes-principales.html","id":"escalado-de-las-variables","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.3.1 Escalado de las variables","text":"El proceso de PCA identifica aquellas direcciones en las que la varianza es mayor. Como la varianza de una variable se mide en su misma escala elevada al cuadrado, si antes de calcular las componentes se estandarizan todas las variables para que tengan media 0 y desviación estándar 1, aquellas variables cuya escala sea mayor dominarán al resto. De ahí que sea recomendable estandarizar siempre los datos.","code":""},{"path":"análisis-de-componentes-principales.html","id":"influencia-de-outliers","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.3.2 Influencia de outliers","text":"Al trabajar con varianzas, el método PCA es altamente sensible outliers, por lo que es altamente recomendable estudiar si los hay. La detección de valores atípicos con respecto una determinada dimensión es algo relativamente sencillo de hacer mediante comprobaciones gráficas. Sin embargo, cuando se trata con múltiples dimensiones el proceso se complica. Por ejemplo, considérese un hombre que mide 2 metros y pesa 50 kg. Ninguno de los dos valores es atípico de forma individual, pero en conjunto se trataría de un caso muy excepcional. La distancia de Mahalanobis es una medida de distancia entre un punto y la media que se ajusta en función de la correlación entre dimensiones y que permite encontrar potenciales outliers en distribuciones multivariante.","code":""},{"path":"análisis-de-componentes-principales.html","id":"proporción-de-varianza-explicada","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.3.3 Proporción de varianza explicada","text":"Una de las preguntas más frecuentes que surge tras realizar un PCA es: ¿Cuánta información presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión? o lo que es lo mismo ¿Cuanta información es capaz de capturar cada una de las componentes principales obtenidas? Para contestar estas preguntas se recurre la proporción de varianza explicada por cada componente principal.","code":""},{"path":"análisis-de-componentes-principales.html","id":"número-óptimo-de-componentes-principales","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.3.4 Número óptimo de componentes principales","text":"Por lo general, dada una matriz de datos de dimensiones n x p, el número de componentes principales que se pueden calcular es como máximo de n-1 o p (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de interés utilizar el número mínimo de componentes que resultan suficientes para explicar los datos. existe una respuesta o método único que permita identificar cual es el número óptimo de componentes principales utilizar. Una forma de proceder muy extendida consiste en evaluar la proporción de varianza explicada acumulada y seleccionar el número de componentes mínimo partir del cual el incremento deja de ser sustancial.","code":""},{"path":"análisis-de-componentes-principales.html","id":"test-iniciales","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.4 Test iniciales","text":"","code":""},{"path":"análisis-de-componentes-principales.html","id":"pruebas-esfericidad-de-bartlett","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.4.1 Pruebas esfericidad de Bartlett","text":"Se utiliza para probar la Hipótesis Nula que afirma que las variables están correlacionadas en la población. Es decir, comprueba si la matriz de correlaciones es una matriz de identidad. Se puede dar como válidos aquellos resultados que nos presenten un valor elevado del test y cuya fiabilidad sea menor 0.05. En este caso se rechaza la Hipótesis Nula y se continúa con el Análisis.Prueba de esfericidad de Bartlett:Si Sig. (p-valor) < 0.05 aceptamos \\(H_0\\) (hipótesis nula) => se puede aplicar el análisis factorial.Si Sig. (p-valor) > 0.05 rechazamos \\(H_0\\) => se puede aplicar el análisis factorial.","code":""},{"path":"análisis-de-componentes-principales.html","id":"prueba-del-kmo","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.4.2 Prueba del KMO","text":"El test KMO (Kaiser, Meyer y Olkin) relaciona los coeficientes de correlación, \\(r_{jh}\\), observados entre las variables \\(X_j\\) y \\(X_h\\), y \\(a_{jh}\\) son los coeficientes de correlación parcial entre las variables \\(X_j\\) y \\(X_h\\). Cuanto más cerca de 1 tenga el valor obtenido del test KMO, implica que la relación entres las variables es alta. Si KMO ≥ 0.9, el test es muy bueno; notable para KMO ≥ 0.8; mediano para KMO ≥ 0.7; bajo para KMO ≥ 0.6; y muy bajo para KMO < 0.5.La prueba de evalúa la aplicabilidad del análisis factorial de las variables estudiadas. El modelo es significativo (aceptamos la hipótesis nula, H0) cuando se puede aplicar el análisis factorial","code":""},{"path":"análisis-de-componentes-principales.html","id":"una-aplicación-con-datos-de-empleados","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.5 Una aplicación con “Datos de Empleados”","text":"El objetivo es aplicar el ACP los datos “Datos de Empleados”. Debemos seleccionar solo las variables cuantitativas las que vamos añadir la edad del empleado.Lectura de datos y cálculo de matriz de correlacionesEjercicio: Incluir nueva variable: Edad del empleadoBuscar información para calcular la edad de cada empleado e introducirla como una nueva variableNormalizamos datosEl cálculo de los componentes principales depende de las unidades de medida empleadas en las variables. Es importante, antes de aplicar el PCA, estandarizar las variables para que tengan media 0 y desviación estándar 1.Test de adecuación del ACPCon este test podemos ver la relación de coeficientes de correlación entre variables si el valor es cercano 1, mayor será la relación entre variables, dado los valores, se puede decir que es apto para la realización del análisis de componentes.","code":"\nlibrary(openxlsx)\nbbdd <- read.xlsx(\"Datos/Datos_de_empleados.xlsx\")\ncor(bbdd$salario,bbdd$salini)\n#> [1] 0.8801175\ncor(bbdd$salario,bbdd$expprev)\n#> [1] -0.09746693\n# Un plot con correlaciones\nlibrary(ggcorrplot)\n#> Loading required package: ggplot2\nggcorrplot(cor(bbdd[,6:9]),lab_size = 3,hc.order = TRUE,method = \"circle\",lab = TRUE)\nde <- bbdd[,c(6:9,11)]\nde.n <- scale(de)\ncorr_matrix <- cor(de.n)\nggcorrplot(corr_matrix,lab = TRUE,digits=3)\nlibrary(corrplot)\n#> corrplot 0.92 loaded\ncorrplot(cor(de.n))\nlibrary(psych)\ncortest.bartlett(cor(de.n),n=474)\n#> $chisq\n#> [1] 1266.164\n#> \n#> $p.value\n#> [1] 7.662669e-266\n#> \n#> $df\n#> [1] 10\nKMO(de.n)\n#> Kaiser-Meyer-Olkin factor adequacy\n#> Call: KMO(r = de.n)\n#> Overall MSA =  0.48\n#> MSA for each item = \n#>  salario   salini tiempemp  expprev     edad \n#>     0.48     0.47     0.08     0.50     0.51"},{"path":"análisis-de-componentes-principales.html","id":"paquetes-de-r-para-el-acp","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.6 Paquetes de R para el ACP","text":"library(stats)prcomp() -> Forma rápida de implementar PCA sobre una matriz de datos.prcomp() -> Forma rápida de implementar PCA sobre una matriz de datos.princomp()princomp()library(FactoMineR)PCA() -> PCA con resultados más detallados. Los valores ausentes se reemplazan por la media de cada columna. Pueden incluirse variables categóricas suplementarias. Estandariza automáticamente los datos.library(factoextra)fviz_pca_ind() -> Representación de observaciones sobre componentes principales.fviz_pca_ind() -> Representación de observaciones sobre componentes principales.fviz_pca_var() -> Representación de variables sobre componentes principales.fviz_pca_var() -> Representación de variables sobre componentes principales.fviz_screeplot() -> Representación (gráfico barras) de eigenvalores.fviz_screeplot() -> Representación (gráfico barras) de eigenvalores.fviz_contrib() -> Representa la contribución de filas/columnas de los resultados de un pca.fviz_contrib() -> Representa la contribución de filas/columnas de los resultados de un pca.get_pca() -> Extrae la información sobre las observaciones y variables de un análisis PCA.get_pca() -> Extrae la información sobre las observaciones y variables de un análisis PCA.get_pca_var() -> Extrae la información sobre las variables.get_pca_var() -> Extrae la información sobre las variables.get_pca_ind() -> Extrae la información sobre las observaciones.get_pca_ind() -> Extrae la información sobre las observaciones.","code":""},{"path":"análisis-de-componentes-principales.html","id":"proporción-de-la-varianza-explicada","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.7 Proporción de la varianza explicada","text":"¿Cuánta información presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión?¿Cuánta información es capaz de capturar cada una de las componentes principales obtenidas?","code":"\n# https://rpubs.com/laurarojasmar/ACP\npca <- prcomp(de, scale = TRUE)\nprint(pca)\n#> Standard deviations (1, .., p=5):\n#> [1] 1.3993033 1.3197040 1.0021060 0.4405821 0.3193784\n#> \n#> Rotation (n x k) = (5 x 5):\n#>                  PC1         PC2          PC3         PC4\n#> salario  -0.60483585 -0.36493800  0.025204955  0.02748384\n#> salini   -0.54214209 -0.45751707 -0.101152801  0.01888093\n#> tiempemp -0.01592624 -0.07314815  0.992391695 -0.04844080\n#> expprev   0.39509934 -0.58382909 -0.065339258 -0.70311587\n#> edad      0.42883379 -0.55793488  0.004724985  0.70863934\n#>                  PC5\n#> salario   0.70682615\n#> salini   -0.69726014\n#> tiempemp -0.08489944\n#> expprev   0.06632468\n#> edad      0.05116844\nde.pca <- princomp(de.n)\nsummary(princomp(de.n))\n#> Importance of components:\n#>                          Comp.1    Comp.2    Comp.3\n#> Standard deviation     1.397826 1.3183112 1.0010484\n#> Proportion of Variance 0.391610 0.3483237 0.2008433\n#> Cumulative Proportion  0.391610 0.7399337 0.9407770\n#>                            Comp.4     Comp.5\n#> Standard deviation     0.44011714 0.31904129\n#> Proportion of Variance 0.03882252 0.02040051\n#> Cumulative Proportion  0.97959949 1.00000000"},{"path":"análisis-de-componentes-principales.html","id":"el-gráfico-de-sedimentación","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.8 El gráfico de sedimentación","text":"El gráfico de sedimentación se obtiene al representar en ordenadas las raíces características y en abscisas los números de las componentes principales correspondientes cada raíz característica en orden decreciente.Uniendo todos los puntos se obtiene una Figura que, en general, se parece al perfil de una montaña con una fuerte pendiente hasta llegar la base, formada por una meseta con una ligera inclinación.En este símil establecido de la montaña, en la meseta es donde se acumulan los guijarros caídos desde la cumbre, es decir, donde se sedimentan. Este es el motivo por lo que al gráfico se le conoce con el nombre de gráfico de sedimentación, su denominación en ingléses scree plot. De acuerdo con el criterio gráfico se retienen todas aquellas componentes previas la zona de sedimentación.","code":"\nscree(de.n)\nlibrary(psych)\nlibrary(FactoMineR)\nlibrary(factoextra)\nde.pca <- PCA(de.n, graph = FALSE)\nfviz_eig(de.pca, addlabels=T)"},{"path":"análisis-de-componentes-principales.html","id":"las-componentes","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.9 Las componentes","text":"","code":"\nbiplot(x = pca, scale = 0, cex = 0.6, col = c(\"blue4\", \"brown3\"))\nde.pca <- princomp(de.n)\na <- de.pca$scores\nbbdd$cp1 <- a[,1]\nbbdd$cp2 <- a[,2]\nplot(bbdd$cp1,bbdd$cp2,col=as.factor(bbdd$sexo))\nplot(bbdd$cp1,bbdd$cp2,col=as.factor(bbdd$catlab))"},{"path":"análisis-de-componentes-principales.html","id":"ejercicio-acp-como-indicador-sintético","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.10 Ejercicio: ACP como indicador sintético","text":"El ACP puede utilizarse para crear un indicador sintético al considerar los primeros componentes principales como resúmenes o representaciones condensadas del conjunto de datos original. Si bien cada componente principal es una combinación lineal de las variables originales, los primeros componentes suelen capturar las características más importantes o patrones de variabilidad presentes en los datos.","code":""},{"path":"análisis-de-componentes-principales.html","id":"los-datos","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.10.1 Los datos","text":"El siguiente conjunto de datos corresponde calificaciones de 20 estudiantes en 5 materias Ciencias Natuales (CNa), Matemáticas (Mat), Francés (Fra), Latín (Lat) y Literatura (Lit)Obtener un sumario de la informaciónObtener la matriz de correlacionesObtener test de adecuaciónObtener CPsInterpretarlas","code":"\n# https://bookdown.org/victor_morales/TecnicasML/an%C3%A1lisis-de-componentes-principales.html\nCNa <- c(7,5,5,6,7,4,5,5,6,6,6,5,6,8,6,4,6,6,6,7)\nMat <- c(7,5,6,8,6,4,5,6,5,5,7,5,6,7,7,3,4,6,5,7)\nFra <- c(5,6,5,5,6,6,5,5,7,6,5,4,6,8,5,4,7,7,4,6)\nLat <- c(5,6,7,6,7,7,5,5,6,6,6,5,6,8,6,4,8,7,4,7)\nLit <- c(6,5,5,6,6,6,6,5,6,6,5,4,5,8,6,4,7,7,4,6)\nNotas <- cbind(CNa,Mat,Fra,Lat,Lit)\nNotas\n#>       CNa Mat Fra Lat Lit\n#>  [1,]   7   7   5   5   6\n#>  [2,]   5   5   6   6   5\n#>  [3,]   5   6   5   7   5\n#>  [4,]   6   8   5   6   6\n#>  [5,]   7   6   6   7   6\n#>  [6,]   4   4   6   7   6\n#>  [7,]   5   5   5   5   6\n#>  [8,]   5   6   5   5   5\n#>  [9,]   6   5   7   6   6\n#> [10,]   6   5   6   6   6\n#> [11,]   6   7   5   6   5\n#> [12,]   5   5   4   5   4\n#> [13,]   6   6   6   6   5\n#> [14,]   8   7   8   8   8\n#> [15,]   6   7   5   6   6\n#> [16,]   4   3   4   4   4\n#> [17,]   6   4   7   8   7\n#> [18,]   6   6   7   7   7\n#> [19,]   6   5   4   4   4\n#> [20,]   7   7   6   7   6"},{"path":"análisis-de-componentes-principales.html","id":"aplicación-regresión-y-acp","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.11 Aplicación: Regresión y ACP","text":"En esta sección realizaremos un ejercicio en el que se combina la Regresión Lineal Múltiple (RLM) con el ACP. El objetivo es reducir los problemas de multicolinealidad en la RLM al incluir variables altamente correlacionadas.El ACP se utiliza principalmente para abordar problemas de multicolinealidad o reducción de dimensionalidad en conjuntos de datos, lo que puede tener implicaciones en modelos de regresión múltiple. Sin embargo, es importante tener en cuenta que al usar el ACP de esta manera, la interpretación de los coeficientes en términos de las variables originales puede volverse más complicada.","code":""},{"path":"análisis-de-componentes-principales.html","id":"lectura-y-sumario-de-los-datos","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.11.1 Lectura y sumario de los datos","text":"Utilizaremos la base de datos de venta de vehículos. El objetivo es predecir el precio en base sus características. Un sumario de los datos se muestra continuación.","code":"\nlibrary(openxlsx)\nCarSales <- read.xlsx(\"Datos/CarSAles.xlsx\")\nsummary(CarSales)\n#>   fabricante           modelo              ventas      \n#>  Length:157         Length:157         Min.   :  0.11  \n#>  Class :character   Class :character   1st Qu.: 14.11  \n#>  Mode  :character   Mode  :character   Median : 29.45  \n#>                                        Mean   : 53.00  \n#>                                        3rd Qu.: 67.96  \n#>                                        Max.   :540.56  \n#>                                                        \n#>     reventa           tipo            precio      \n#>  Min.   : 5.16   Min.   :0.0000   Min.   : 9.235  \n#>  1st Qu.:11.26   1st Qu.:0.0000   1st Qu.:18.017  \n#>  Median :14.18   Median :0.0000   Median :22.799  \n#>  Mean   :18.07   Mean   :0.2611   Mean   :27.391  \n#>  3rd Qu.:19.88   3rd Qu.:1.0000   3rd Qu.:31.948  \n#>  Max.   :67.55   Max.   :1.0000   Max.   :85.500  \n#>  NA's   :36                       NA's   :2       \n#>     motor_s         caballos     BaseNeumatico  \n#>  Min.   :1.000   Min.   : 55.0   Min.   : 92.6  \n#>  1st Qu.:2.300   1st Qu.:149.5   1st Qu.:103.0  \n#>  Median :3.000   Median :177.5   Median :107.0  \n#>  Mean   :3.061   Mean   :185.9   Mean   :107.5  \n#>  3rd Qu.:3.575   3rd Qu.:215.0   3rd Qu.:112.2  \n#>  Max.   :8.000   Max.   :450.0   Max.   :138.7  \n#>  NA's   :1       NA's   :1       NA's   :1      \n#>     anchura         longitud     peso_revestimiento\n#>  Min.   :62.60   Min.   :149.4   Min.   :1.895     \n#>  1st Qu.:68.40   1st Qu.:177.6   1st Qu.:2.971     \n#>  Median :70.55   Median :187.9   Median :3.342     \n#>  Mean   :71.15   Mean   :187.3   Mean   :3.378     \n#>  3rd Qu.:73.42   3rd Qu.:196.1   3rd Qu.:3.800     \n#>  Max.   :79.90   Max.   :224.5   Max.   :5.572     \n#>  NA's   :1       NA's   :1       NA's   :2         \n#>  tapón_combustible      kpl       \n#>  Min.   :10.30     Min.   :15.00  \n#>  1st Qu.:15.80     1st Qu.:21.00  \n#>  Median :17.20     Median :24.00  \n#>  Mean   :17.95     Mean   :23.84  \n#>  3rd Qu.:19.57     3rd Qu.:26.00  \n#>  Max.   :32.00     Max.   :45.00  \n#>  NA's   :1         NA's   :3"},{"path":"análisis-de-componentes-principales.html","id":"eliminación-de-datos-perdidos","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.11.2 Eliminación de datos perdidos","text":"Por simplicidad en el ejercicio se eliminan algunos datos en los que hay datos faltantes. En general estos valores podrian ser sustituidos por valores medios.","code":"\nCarSales <- CarSales[!is.na(CarSales$peso_revestimiento),]\nCarSales <- CarSales[!is.na(CarSales$precio),]\nCarSales <- CarSales[!is.na(CarSales$kpl),]"},{"path":"análisis-de-componentes-principales.html","id":"un-modelo-de-regresión-básico","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.11.3 Un modelo de regresión básico","text":"Se eliminan algunas variables significativas. Pueden ser importantes para ontener un modelo explicativo pero para un modelo predictivo.Finalmente se seleccionan algunas variables explicativas Xs","code":"\nformula <- precio ~ motor_s + caballos + BaseNeumatico + anchura + longitud + peso_revestimiento + tapón_combustible + kpl\nsummary(lm(formula , data=CarSales))\n#> \n#> Call:\n#> lm(formula = formula, data = CarSales)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -14.494  -3.106  -0.469   2.547  32.108 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        23.04275   17.32973   1.330  0.18575    \n#> motor_s            -3.84947    1.24859  -3.083  0.00246 ** \n#> caballos            0.26273    0.01825  14.397  < 2e-16 ***\n#> BaseNeumatico      -0.06376    0.15059  -0.423  0.67265    \n#> anchura            -0.42902    0.27540  -1.558  0.12149    \n#> longitud           -0.23427    0.08328  -2.813  0.00560 ** \n#> peso_revestimiento  9.01935    2.15367   4.188  4.9e-05 ***\n#> tapón_combustible   0.31214    0.30997   1.007  0.31563    \n#> kpl                 0.52783    0.25548   2.066  0.04063 *  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 6.738 on 143 degrees of freedom\n#> Multiple R-squared:  0.7932, Adjusted R-squared:  0.7816 \n#> F-statistic: 68.57 on 8 and 143 DF,  p-value: < 2.2e-16\n# Eliminamos variables no informativas\nformula <- precio ~ motor_s + caballos + anchura + longitud + peso_revestimiento + kpl\nsummary(lm(formula , data=CarSales))\n#> \n#> Call:\n#> lm(formula = formula, data = CarSales)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -13.810  -3.132  -0.354   2.399  32.529 \n#> \n#> Coefficients:\n#>                    Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)        23.95706   17.06820   1.404  0.16257    \n#> motor_s            -3.82675    1.24211  -3.081  0.00247 ** \n#> caballos            0.26213    0.01778  14.747  < 2e-16 ***\n#> anchura            -0.42581    0.27043  -1.575  0.11753    \n#> longitud           -0.25383    0.06039  -4.203 4.58e-05 ***\n#> peso_revestimiento  9.98519    1.85702   5.377 2.96e-07 ***\n#> kpl                 0.44621    0.23777   1.877  0.06258 .  \n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 6.715 on 145 degrees of freedom\n#> Multiple R-squared:  0.7917, Adjusted R-squared:  0.7831 \n#> F-statistic: 91.85 on 6 and 145 DF,  p-value: < 2.2e-16\nXs <- CarSales[,c(\"motor_s\",\"caballos\",\"anchura\",\"longitud\",\"peso_revestimiento\")]"},{"path":"análisis-de-componentes-principales.html","id":"evaluamos-matriz-de-correlaciones","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.11.4 Evaluamos matriz de correlaciones","text":"La multicolinealidad en un modelo de regresión múltiple implica la existencia de altas correlaciones entre las variables independientes, generando inestabilidad numérica y elevando la varianza de los estimadores de los coeficientes. Esto dificulta la interpretación precisa de los efectos individuales de las variables predictoras, así como la evaluación confiable de la importancia relativa de cada predictor en la predicción del resultado.","code":"\nlibrary(ggcorrplot)\nggcorrplot(cor(Xs),lab_size = 5,lab = TRUE)"},{"path":"análisis-de-componentes-principales.html","id":"test-de-adecuación-del-acp","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.11.5 Test de adecuación del ACP","text":"Test de Bartlett contrasta la hipótesis de que el determinante es distinto de la unidad (matriz identidad)Indicador KMO sugiere buena adecuación del ACP","code":"\nXs.n <- scale(Xs)\nlibrary(psych)\ncortest.bartlett(cor(Xs.n))\n#> $chisq\n#> [1] 356.0616\n#> \n#> $p.value\n#> [1] 2.059602e-70\n#> \n#> $df\n#> [1] 10\nKMO(Xs.n)\n#> Kaiser-Meyer-Olkin factor adequacy\n#> Call: KMO(r = Xs.n)\n#> Overall MSA =  0.81\n#> MSA for each item = \n#>            motor_s           caballos            anchura \n#>               0.75               0.74               0.85 \n#>           longitud peso_revestimiento \n#>               0.84               0.88"},{"path":"análisis-de-componentes-principales.html","id":"cálculo-de-las-componentes","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.11.6 Cálculo de las componentes","text":"la librería ‘stats’ incluye la función ‘princomp()’ que realiza el ACP.Una o como máximo 2 CP serían necesarias para resumir la información. Con dos CPs se está recogiendo el 86% de la información.","code":"\nXs.pca <- princomp(Xs.n)\nsummary(Xs.pca)\n#> Importance of components:\n#>                           Comp.1    Comp.2     Comp.3\n#> Standard deviation     1.8864719 0.8581591 0.54305278\n#> Proportion of Variance 0.7164689 0.1482628 0.05937187\n#> Cumulative Proportion  0.7164689 0.8647317 0.92410355\n#>                            Comp.4    Comp.5\n#> Standard deviation     0.50375651 0.3510200\n#> Proportion of Variance 0.05109024 0.0248062\n#> Cumulative Proportion  0.97519380 1.0000000"},{"path":"análisis-de-componentes-principales.html","id":"interpretación-de-las-cps","chapter":"Capítulo 5 Análisis de Componentes Principales","heading":"5.11.7 Interpretación de las CPs","text":"La CP1 es un indicador global. Todas las variables puntúan de forma positiva. La CP2 discrimina coches potentes (xon muchos CVs y cc) frente coches pequeños.","code":"\nXs.pca$loadings\n#> \n#> Loadings:\n#>                    Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\n#> motor_s             0.482  0.345                0.804\n#> caballos            0.421  0.618  0.386        -0.540\n#> anchura             0.457 -0.317 -0.175 -0.803 -0.120\n#> longitud            0.403 -0.628  0.563  0.356       \n#> peso_revestimiento  0.468        -0.708  0.478 -0.218\n#> \n#>                Comp.1 Comp.2 Comp.3 Comp.4 Comp.5\n#> SS loadings       1.0    1.0    1.0    1.0    1.0\n#> Proportion Var    0.2    0.2    0.2    0.2    0.2\n#> Cumulative Var    0.2    0.4    0.6    0.8    1.0\nbiplot(Xs.pca)\nsummary(lm(CarSales$precio ~ Xs.pca$scores[,1:2]))\n#> \n#> Call:\n#> lm(formula = CarSales$precio ~ Xs.pca$scores[, 1:2])\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -22.567  -4.051  -0.997   2.878  33.487 \n#> \n#> Coefficients:\n#>                            Estimate Std. Error t value\n#> (Intercept)                 27.3318     0.6601   41.40\n#> Xs.pca$scores[, 1:2]Comp.1   4.4839     0.3499   12.81\n#> Xs.pca$scores[, 1:2]Comp.2   9.7531     0.7692   12.68\n#>                            Pr(>|t|)    \n#> (Intercept)                  <2e-16 ***\n#> Xs.pca$scores[, 1:2]Comp.1   <2e-16 ***\n#> Xs.pca$scores[, 1:2]Comp.2   <2e-16 ***\n#> ---\n#> Signif. codes:  \n#> 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 8.138 on 149 degrees of freedom\n#> Multiple R-squared:  0.6856, Adjusted R-squared:  0.6814 \n#> F-statistic: 162.5 on 2 and 149 DF,  p-value: < 2.2e-16"},{"path":"análisis-cluster.html","id":"análisis-cluster","chapter":"Capítulo 6 Análisis Cluster","heading":"Capítulo 6 Análisis Cluster","text":"El análisis clúster es un conjunto amplio de técnicas para encontrar subgrupos de observaciones dentro de un conjunto de datos. Cuando agrupamos observaciones, queremos que las observaciones en el mismo grupo sean similares y que las observaciones en diferentes grupos sean diferentes. Dado que hay una variable de respuesta, este es un método supervisado, lo que implica que busca encontrar relaciones entre las \\(n\\) observaciones sin ser entrenado por una variable de respuesta. El agrupamiento nos permite identificar qué observaciones son similares y potencialmente categorizarlas como tales. El agrupamiento K-medias (K-means) es el método de agrupamiento más simple y comúnmente utilizado para dividir un conjunto de datos en un conjunto de k grupos o clusters.Para realizar un análisis de clúster en R, por lo general, los datos deben estar preparados de la siguiente manera:1.- Las filas representan observaciones (individuos) y las columnas representan variables.2.- Cualquier caso con valor omitido en los datos debe ser eliminado o estimado.3.- Los datos deben estar estandarizados para hacer que las variables sean comparables. Recuerde que la estandarización consiste en transformar las variables de manera que tengan una media cero y una desviación estándar de uno.Estas son las tres librerias de R que necesitaremos","code":"\nlibrary(tidyverse)  #  Manipulación de datos\nlibrary(cluster)    #  Algoritmos de clustering \nlibrary(factoextra) #  Algoritmos de clustering y visualización"},{"path":"análisis-cluster.html","id":"medidas-de-distancia","chapter":"Capítulo 6 Análisis Cluster","heading":"6.1 Medidas de distancia","text":"La clasificación de observaciones en grupos requiere algunos métodos para calcular la distancia o la (dis)similitud entre cada par de observaciones. El resultado de este cálculo se conoce como una matriz de disimilitud o distancia. Existen muchos métodos para calcular esta distancia. La elección de las medidas de distancia es un paso crítico en el agrupamiento lod datos y afectará la forma de los grupos o clusters. Los métodos clásicos para las medidas de distancia son las distancias Euclidea y Manhattan, aunque también existen otras distancias basadas en correlaciones:Distancia Euclidea\n\\[d_E(x,y)=\\sqrt{\\sum\\limits_{=1}^n(x_i-y_i)^2}\\]Distancia Manhattan\n\\[d_M(x,y)=\\sum\\limits_{=1}^n|x_i-y_i|\\]\nDistancia Correlación de Pearson\n\\[d_P(x,y)=1-\\frac{\\sum\\limits_{=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum\\limits_{=1}^n(x_i-\\bar{x})^2\\sum\\limits_{=1}^n(y_i-\\bar{y})^2}}\\]\nDistancia Correlación de Spearman\n\\[d_S(x,y)=1-\\frac{\\sum\\limits_{=1}^n(\\tilde{x}_i-\\bar{\\tilde{x}})(\\tilde{y}_i-\\bar{\\tilde{y}})}{\\sqrt{\\sum\\limits_{=1}^n(\\tilde{x}_i-\\bar{\\tilde{x}})^2\\sum\\limits_{=1}^n(\\tilde{y}_i-\\bar{\\tilde{y}})^2}}\\]\ndonde hemos denotado por \\(\\tilde{x}_i=rank(x_i)\\) el rango de la observación \\(x_i\\).Para el caso en el que los datos clusterizar sean secuencias de caracteres (tipo categóricos), tambien se puede calcular la Distancia de Hamming que se calcula contando el número de posiciones en las que los caracteres correspondientes son diferentes. Se utiliza principalmente en secuencias binarias. Por ejemplo, dadas las secuencias \\(\"1010101\"\\) y \\(\"1001001\"\\) la distancia de Hamming sería 3, ya que hay tres posiciones donde son diferentes.","code":""},{"path":"análisis-cluster.html","id":"análisis-cluster-con-el-algoritmo-k-medias.","chapter":"Capítulo 6 Análisis Cluster","heading":"6.2 Análisis cluster con el algoritmo K-medias.","text":"El análsis clúster K-medias es el algoritmo de aprendizaje automático supervisado más comúnmente utilizado para dividir un conjunto de datos en un conjunto de \\(k\\) grupos (es decir, \\(k\\) clústeres), donde \\(k\\) representa el número de grupos predefinido por el analista. La clasificacion de los individuos se hace de tal manera que dentro del mismo clúster son tan similares como sea posible, mientras que los individuos de diferentes clústeres son tan diferentes como sea posible. Cada clúster está representado por su centro (o centroide), que corresponde la media de los puntos asignados al clúster.La idea básica detrás de la del algoritmo k-medias consiste en definir grupos de manera que la variación total intra-cluster (conocida como variación total dentro del grupo) se minimice.Existen varios algoritmos de k-nedias disponibles. El algoritmo estándar es el algoritmo de Hartigan-Wong (Hartigan y Wong 1979), que define la variación total dentro del grupo como la suma de las distancias euclidianas al cuadrado entre los elementos y el centroide correspondiente:\n\\[W(C_k)=\\sum\\limits_{x_i\\C_k}(x_i-\\mu_k)^2\\]\ndonde \\(x_i\\) es el dato perteneciente al cluster \\(C_k\\) y \\(\\mu_k\\) es el valor medio de los puntos asignados al cluster \\(C_k\\).Cada punto \\(x_i\\) se asocia al clúster que verifica que la suma de los cuadrados de las distancias de cada observación al centro de su cluster asignado (valor medio \\(\\mu_k\\)) sea mínimo.Se define la variación total dentro de clúster como \\[VTDC=\\sum_{=1}^KW(C_k)=\\sum_{=1}^K\\sum\\limits_{x_i\\C_k}(x_i-\\mu_k)^2\\]\\(VTDC\\) mide la compacidad (es decir, la calidad) del agrupamiento y queremos que sea lo más pequeña posible.","code":""},{"path":"análisis-cluster.html","id":"k-medias","chapter":"Capítulo 6 Análisis Cluster","heading":"6.2.1 K-medias","text":"El primer paso al utilizar el algoritmo k-medias es indicar el número de grupos/clústeres \\(k\\) que se generarán en la solución final. El algoritmo comienza seleccionando aleatoriamente \\(k\\) individuos del conjunto de datos para que sirvan como los centros iniciales de los clústeres. Los individuos seleccionados también se conocen como medias o centroides del clúster. continuación, cada uno de los objetos restantes se le asigna su centroide más cercano, donde “más cercano” se define usando la distancia utilizada entre el objeto y la media del grupo. Este paso se llama “paso de asignación de clúster”. Después del paso de asignación, el algoritmo calcula el nuevo valor medio de cada clúster. El término “actualización del centroide del clúster” se utiliza para designar este paso. Ahora que los centros han sido recalculados, se vuelve verificar cada observación para ver si podría estar más cerca de un grupo diferente. Todos los objetos se vuelven asignar utilizando las medias actualizadas del grupo. Los pasos de asignación de grupo y actualización del centroide se repiten de forma iterativa hasta que las asignaciones de grupo dejan de cambiar (es decir, hasta que se alcanza la convergencia). Es decir, los grupos formados en la iteración actual son los mismos que los obtenidos en la iteración anterior.El algoritmo \\(K\\)-medias se puede resumir de la siguiente manera:Especificar el número de clúster \\(K\\) que se crearán (por el investigador).Especificar el número de clúster \\(K\\) que se crearán (por el investigador).Seleccionar aleatoriamente k individuos del conjunto de datos como los centros o medias iniciales del clúster.Seleccionar aleatoriamente k individuos del conjunto de datos como los centros o medias iniciales del clúster.Asignar cada observación su centroide más cercano, basado en una distancia entre el individuo y el centroide.Asignar cada observación su centroide más cercano, basado en una distancia entre el individuo y el centroide.Para cada uno de los \\(k\\) clústeres, actualizar el centroide del clúster calculando los nuevos valores medios de todos los puntos de datos en el clúster. El centroide de un \\(\\)-ésimo clúster es un vector de longitud \\(p\\) que contiene las medias de todas las variables para las observaciones en el \\(\\)-ésimo clúster y \\(p\\) es el número de variables.Para cada uno de los \\(k\\) clústeres, actualizar el centroide del clúster calculando los nuevos valores medios de todos los puntos de datos en el clúster. El centroide de un \\(\\)-ésimo clúster es un vector de longitud \\(p\\) que contiene las medias de todas las variables para las observaciones en el \\(\\)-ésimo clúster y \\(p\\) es el número de variables.Minimizar de forma iterativa la suma total de cuadrados dentro del grupo \\(VTDC\\). Es decir, iterar los pasos 3 y 4 hasta que las asignaciones de clúster dejen de cambiar o se alcance el número máximo de iteraciones. Por defecto, el software R utiliza 10 como valor predeterminado para el número máximo de iteraciones.Minimizar de forma iterativa la suma total de cuadrados dentro del grupo \\(VTDC\\). Es decir, iterar los pasos 3 y 4 hasta que las asignaciones de clúster dejen de cambiar o se alcance el número máximo de iteraciones. Por defecto, el software R utiliza 10 como valor predeterminado para el número máximo de iteraciones.","code":""},{"path":"análisis-cluster.html","id":"un-ejemplo-de-análisis-clúster-con-r","chapter":"Capítulo 6 Análisis Cluster","heading":"6.3 Un ejemplo de análisis clúster con R","text":"En esta sección seleccionamos las librerias utilizar y explicamos las funciones básicas para realizar un análsis cluster por k-medias","code":""},{"path":"análisis-cluster.html","id":"funciones-básicas-de-r-para-usar-k-medias","chapter":"Capítulo 6 Análisis Cluster","heading":"6.3.1 Funciones básicas de R para usar K-medias","text":"Para realizar un análisis cluster con R necesitamos cargar algunas librerias esenciales.El fichero de datos que vamos trabajar es CarSales.xlsx que contine datos de estimaciones de ventas, precios de catálogo y especificaciones físicas hipotéticas de varias marcas y modelos de vehículos.Seleccionamos aquellas variables del fichero de datos que son numéricas para realizar el análsis cluster en funcion de ellas guardandolas en el objeto de R del tipo data frame llamado datos. Seguidamente tipifico los datos (acción que consiste en quitar la media y dividir por la desciación tipica) para normalizar las variables que tengan media 0 y desviación típica 1 y elimino los NAN de la base de datos.Dentro de R, es sencillo calcular y visualizar la matriz de distancias utilizando las funciones get_dist y fviz_dist del paquete R factoextra.get_dist: para calcular una matriz de distancias entre las filas de una matriz de datos. La distancia predeterminada calculada es la Euclidiana; sin embargo, get_dist también admite otras distancias como las descritas anteriormente, entre otras.fviz_dist: para visualizar una matriz de distancias.En R, podemos emplear la función kmeans para llevar cabo el análisis de k-medias. En el ejemplo que vamos realizar, se organizarán los datos en dos grupos centers = 2. Además, la función kmeans cuenta con una opción llamada nstart que realiza intentos con múltiples configuraciones iniciales y registra la mejor de ellas. Por ejemplo, al incluir nstart = 25 se generarán 25 configuraciones iniciales. Este método se sugiere con frecuencia.Para mostrar la estructura interna del objeto k2 generado al realizar el algoritmo k-medias en el ejemplo anterior podemos utilizar la funcion strLa función kmeans produce como resultado una lista con información diversa, siendo los elementos más relevantes:1.- cluster: Un vector de números enteros (del 1 al k), que indica el clúster al que se asigna cada punto.2.- centers: Una matriz que contiene los centros de los clústeres.3.- totss: La suma total de los cuadrados.4.- withinss: Un vector que contiene las sumas de cuadrados dentro de cada clúster, con un componente por clúster.5.- tot.withinss: La suma total de cuadrados dentro de todos los clústeres, es decir, la suma de withinss.6.- betweenss: La suma de cuadrados entre clústeres, calculada como totss - tot.withinss.7.- size: El número de puntos en cada clúster.Al imprimir los resultados, observaremos que nuestras agrupaciones generaron clústeres con tamaños de 61 y 56, respectivamente.También es posible visualizar los resultados utilizando la función fviz_cluster. Esto proporciona una representación visual atractiva de los clústeres. En el caso de que haya más de dos dimensiones (variables), fviz_cluster llevará cabo un análisis de componentes principales (PCA) y representará los puntos de datos en función de los dos primeros componentes principales que explican la mayor parte de la variabilidad.Alternativamente, puedes utilizar gráficos de dispersión clásicos por pares para ilustrar los clústeres en comparación con las variables originales.","code":"> library(dplyr)\n> library(magrittr)\n> library(plyr)\n> library(ggplot2)\n> library(plotly)\n> library(tidyverse)  # data manipulation\n> library(cluster)    # clustering algorithms\n> # install.packages(\"factoextra\")\n> library(factoextra) # clustering algorithms & visualization> library(readxl)     # read xlsx files\n> data <- read_xlsx(\"Datos/CarSales.xlsx\")> datos=as.data.frame(data[,c(3,4,6:14)])# selecciono aquellos elementos utilizados para la clusterización\n> df=as.data.frame(scale(na.omit(datos)))# tipifico los datos, elimino los Nan y lo guardo en el data frame df> distance <- get_dist(df)\n> fviz_dist(distance, gradient = list(low = \"#00AFBB\", mid = \"white\", high = \"#FC4E07\"))> k2 <- kmeans(df, centers = 2, nstart = 25)> str(k2)\n#> List of 9\n#>  $ cluster     : Named int [1:117] 1 2 2 1 2 2 1 2 2 2 ...\n#>   ..- attr(*, \"names\")= chr [1:117] \"1\" \"2\" \"4\" \"5\" ...\n#>  $ centers     : num [1:2, 1:11] 0.0434 -0.0426 -0.4533 0.4457 -0.5719 ...\n#>   ..- attr(*, \"dimnames\")=List of 2\n#>   .. ..$ : chr [1:2] \"1\" \"2\"\n#>   .. ..$ : chr [1:11] \"ventas\" \"reventa\" \"precio\" \"motor_s\" ...\n#>  $ totss       : num 1276\n#>  $ withinss    : num [1:2] 261 558\n#>  $ tot.withinss: num 819\n#>  $ betweenss   : num 457\n#>  $ size        : int [1:2] 58 59\n#>  $ iter        : int 1\n#>  $ ifault      : int 0\n#>  - attr(*, \"class\")= chr \"kmeans\"> k2\n#> K-means clustering with 2 clusters of sizes 58, 59\n#> \n#> Cluster means:\n#>        ventas    reventa     precio    motor_s   caballos\n#> 1  0.04338422 -0.4533385 -0.5718609 -0.7030355 -0.6863263\n#> 2 -0.04264890  0.4456548  0.5621683  0.6911197  0.6746937\n#>   BaseNeumatico    anchura   longitud peso_revestimiento\n#> 1    -0.5307746 -0.6676727 -0.6046852         -0.7833115\n#> 2     0.5217784  0.6563562  0.5944363          0.7700351\n#>   tapón_combustible        kpl\n#> 1        -0.6622080  0.5717540\n#> 2         0.6509841 -0.5620632\n#> \n#> Clustering vector:\n#>   1   2   4   5   6   7   9  10  11  12  13  14  15  17  18 \n#>   1   2   2   1   2   2   1   2   2   2   2   2   2   2   2 \n#>  20  21  22  23  24  25  26  27  29  30  31  32  33  36  37 \n#>   1   1   1   2   2   2   1   1   1   1   2   1   2   1   1 \n#>  38  40  41  42  43  44  46  47  48  49  50  52  53  54  55 \n#>   1   2   2   2   2   2   2   1   1   1   1   2   2   2   2 \n#>  56  57  58  59  60  61  62  63  64  65  66  68  69  70  71 \n#>   1   2   1   1   1   2   2   1   1   1   2   1   1   2   2 \n#>  72  74  77  78  80  81  82  83  84  85  86  87  88  89  90 \n#>   2   2   2   2   1   1   1   2   1   2   1   1   1   1   2 \n#>  91  92  93  94  95  96 102 103 104 105 106 109 112 113 114 \n#>   2   2   1   2   2   2   1   1   2   2   2   1   2   2   2 \n#> 115 116 117 119 120 121 122 123 125 126 127 130 131 132 137 \n#>   1   1   2   1   1   2   2   2   1   2   2   1   1   1   1 \n#> 138 139 140 141 143 144 145 146 147 148 149 150 \n#>   1   2   1   1   1   1   2   1   1   1   1   1 \n#> \n#> Within cluster sum of squares by cluster:\n#> [1] 261.4993 557.6675\n#>  (between_SS / total_SS =  35.8 %)\n#> \n#> Available components:\n#> \n#> [1] \"cluster\"      \"centers\"      \"totss\"       \n#> [4] \"withinss\"     \"tot.withinss\" \"betweenss\"   \n#> [7] \"size\"         \"iter\"         \"ifault\"> fviz_cluster(k2, data = df)> data_sin_NAN=na.omit(data)\n> modelo=data_sin_NAN$modelo\n> \n> df %>%\n+   as_tibble() %>%\n+   mutate(cluster = k2$cluster,\n+          Modelo =modelo) %>%\n+   ggplot(aes(precio, caballos, color = factor(cluster), label = modelo)) +\n+   geom_text()> df2=as.data.frame(scale(na.omit(datos)))#vuelvo a cargar los datos\n> df2$clus=as.factor(k2$cluster)#defino una nueva columna de pertenencia a clúster\n> df2$clus=factor(df2$clus)#lo defino como un factor \n> data_long=gather(df2,caracteristica,valor,ventas:kpl,factor_key=TRUE)#apilo los datos\n> \n> ggplot(data_long,aes(as.factor(x=caracteristica),y=valor, group=clus,colour=clus))+\n+   stat_summary(fun=mean,geom=\"pointrange\",size=1)+\n+   stat_summary(geom=\"line\")+\n+   geom_point(aes(shape=clus))\n#> No summary function supplied, defaulting to `mean_se()`\n#> Warning: Removed 22 rows containing missing values\n#> (`geom_segment()`)."},{"path":"análisis-cluster.html","id":"selección-del-número-de-clústeres","chapter":"Capítulo 6 Análisis Cluster","heading":"6.3.2 Selección del número de clústeres","text":"Dado que el número de clústeres (\\(k\\)) debe establecerse antes de iniciar el algoritmo, menudo es ventajoso utilizar varios valores diferentes de \\(k\\) y examinar las diferencias en los resultados. Podemos ejecutar el mismo proceso para 3, 4 y 5 clústeres, y los resultados se muestran en la figura:Estos gráficos nos permiten ver las diferentes configuraciones para diferente número de clústeres y nos dan una intuición sobre el número óptimo de clústeres. Para facilitar al analista la determinación de este numero óptimo, continuación se explican los tres métodos más comunes para determinarlo, Elbow, Silhouette y el estadístico Gap","code":"> k3 <- kmeans(df, centers = 3, nstart = 25)\n> k4 <- kmeans(df, centers = 4, nstart = 25)\n> k5 <- kmeans(df, centers = 5, nstart = 25)\n> \n> # Gráficos para comparar\n> p1 <- fviz_cluster(k2, geom = \"point\", data = df) + ggtitle(\"k = 2\")\n> p2 <- fviz_cluster(k3, geom = \"point\",  data = df) + ggtitle(\"k = 3\")\n> p3 <- fviz_cluster(k4, geom = \"point\",  data = df) + ggtitle(\"k = 4\")\n> p4 <- fviz_cluster(k5, geom = \"point\",  data = df) + ggtitle(\"k = 5\")\n> \n> library(gridExtra)\n#> \n#> Attaching package: 'gridExtra'\n#> The following object is masked from 'package:dplyr':\n#> \n#>     combine\n> grid.arrange(p1, p2, p3, p4, nrow = 2)"},{"path":"análisis-cluster.html","id":"método-elbow","chapter":"Capítulo 6 Análisis Cluster","heading":"6.3.2.1 Método Elbow","text":"Recordad que la idea que utiliza el algoritmo k-medias para identificar los clústeres es minimizar la variación total dentro de clúster\\[VTDC=\\sum_{=1}^KW(C_k)=\\sum_{=1}^K\\sum\\limits_{x_i\\C_k}(x_i-\\mu_k)^2\\]La suma \\(VTDC\\), mide cuanto de compacto es el agrupamiento, y buscamos minimizar esta medida. Por tanto, podemos seguir el siguiente procedimiento para determinar el número óptimo de clústeres.1.- Calcula el algoritmo k-medias para diferentes valores de k. Por ejemplo, variando k desde 1 hasta 10 clústeres.2.- Calcular, para cada valor de k, la variación total dentro de los clústeres \\(VTDC\\).3.- Representar gráficamente la curva de valores de \\(VTDC\\) en función del número de clústeres k.4.-El punto en el gráfico donde se observa un quiebre (codo) se considera comúnmente como un indicador del número apropiado de clústeres.Podemos llevar cabo este procedimiento en R con el siguiente código. Los resultados sugieren que 3 es el número óptimo de clústeres, ya que se observa el codo.El método Elbow tambien se puede realizar con la función de R fviz_nbclust de la siguiente forma:","code":"> nk=10\n> pares=matrix(0,nk,2)# definimos la matriz donde guardar los pares (nº cluster, VTDC) para valores de k=1:nk\n> for (k in 1:nk){\n+   pares[k,]=c(k,kmeans(df, k, nstart = 10 )$tot.withinss)# calculamos (nº cluster, VTDC)\n+ }\n> plot(pares[,1], pares[,2],\n+        type=\"b\", pch = 19, frame = FALSE, \n+        xlab=\"Número de clústeres K\",\n+        ylab=\"Variación total dentro del clúster VTDC\")> set.seed(123)\n> fviz_nbclust(df, kmeans, method = \"wss\")"},{"path":"análisis-cluster.html","id":"método-silhouette","chapter":"Capítulo 6 Análisis Cluster","heading":"6.3.2.2 Método Silhouette","text":"El método Silhouette evalúa la calidad de un agrupamiento, es decir, determina como de bien cada individuo se encuentra dentro del clúster.\nPasamos ahora definir la silueta de un individuo \\(\\) que pertenece al clúster \\(C_I\\) como \\(s()=0\\) si \\(|C_I|=1\\) y\n\\[s()=\\frac{b()-()}{\\max\\{(),b()\\}}\\mbox{ si }\\quad |C_I|>1\\]\ndonde\n\\[()=\\frac{1}{|C_I|-1}\\sum\\limits_{j\\C_I, j\\neq }d(,j)\\]\nes la distancia media del individuo \\(\\) todos los individuos de su cluster y\n\\[b()=\\min\\limits_{k\\neq }\\frac{1}{|C_k|}\\sum\\limits_{j\\C_k}d(,j)\\]\nla distancia media más pequeña entre las distancias medias del individuo \\(\\) todos los individuos de un clúster \\(C_k\\). Es fácil deducir que \\(-1\\leq s()\\leq1\\). Un valor pequeño de \\(()\\) significa que el individuo \\(\\) es muy cercano los individuos de su clúster, mientras que un valos grande de \\(b()\\) significa que el individuo \\(\\) está muy distante de los indivíduos de su cluster vecino. Por todo ello, un valos de \\(s()\\) cercano 1 significa que el individuo \\(\\) está en el cluster apropiado, si es cercano -1 significa que sería más apropiado que el individuo \\(\\) perteneciera su clúster vecino y un valor \\(s()\\)=0 significa que podría pertenecer dos clústeres diferentes.Un valor de silueta promedio alto indica un buen agrupamiento. El método del coeficiente de silueta calcula la silueta promedio de las observaciones para diferentes valores de k. El número óptimo de clústeres, k, es aquel que maximiza la silueta promedio lo largo de un rango de posibles valores para k.\nEn este contexto se define el coeficiente Silhouette como\n\\[SC=\\max_{k}\\bar{s}(k)\\]\ndonde \\(\\bar{s}(k)\\) denota la media de \\(s()\\) para todos los \\(\\C_k\\).Para llevar cabo este enfoque, podemos utilizar la función silhouette del paquete cluster para calcular el ancho promedio de Silhouette. El siguiente código implementa este método para 2-\\(n_k\\) clústeres.","code":"> nk=10\n> Spares=matrix(0,nk-1,2)# definimos la matriz donde guardar los pares (nº cluster, VTDC) para valores de k=1:nk\n> cont=0\n> for (k in 2:nk){\n+   cont=cont+1\n+   resultado<-kmeans(df, k, nstart = 10)\n+   ss<-silhouette(resultado$cluster,dist(df))\n+   Spares[cont,]=c(k,mean(ss[,3]))# calculamos (nº cluster, VTDC)\n+ }\n> plot(Spares[,1], Spares[,2],\n+        type=\"b\", pch = 19, frame = FALSE, \n+        xlab=\"Número de clústeres K\",\n+        ylab=\"Silueta  promedio\")"},{"path":"análisis-cluster.html","id":"visualización-del-clúster-óptimo","chapter":"Capítulo 6 Análisis Cluster","heading":"6.3.3 Visualización del clúster óptimo","text":"Con los resultados obtenidos en las secciones anteriores indican que \\(k=3\\) es el número optimo de clústeres, realizaremos el algoritmo k-medias para k=3 y lo representaremos con las función fviz_cluster.","code":"> ClusterOpt <- kmeans(df, centers = 3, nstart = 25)\n> fviz_cluster(ClusterOpt, data = df)"},{"path":"análisis-cluster.html","id":"comentarios-finales-sobre-el-algoritmo-k-medias","chapter":"Capítulo 6 Análisis Cluster","heading":"6.4 Comentarios finales sobre el algoritmo k-medias","text":"El algoritmo k-medias es un algoritmo muy sencillo y rápido que puede manejar grandes conjuntos de datos eficientemente. Sin embargo, presenta algunas debilidades.Una posible limitación de k-medias es que requiere que especifiquemos previamente el número de clústeres. La agrupación jerárquica es una alternativa que exige comprometerse con una cantidad específica de clústeres. Además, la agrupación jerárquica tiene la ventaja de generar una representación visual atractiva de las observaciones en forma de un dendrograma.Otra desventaja del algoritmo k-medias es su sensibilidad valores atípicos, lo que puede resultar en diferentes resultados al cambiar el orden de los datos.","code":""},{"path":"análisis-cluster.html","id":"análisis-clúster-jerárquico","chapter":"Capítulo 6 Análisis Cluster","heading":"6.5 Análisis clúster jerárquico","text":"El análisis clúster jerárquico es un enfoque alternativo al algoritmo k-medias para identificar grupos en el conjunto de datos. requiere que especifiquemos previamente el número de clústeres generar, como lo exige el enfoque k-medias. Además, el análisis jerárquico tiene una ventaja adicional sobre k-medias que resulta en una atractiva representación gráfica basada en árbol de las observaciones, llamada dendrograma.Requeriremos el siguiente paquete adicionalEl análisis clúster jerárquico se clasifica en dos tipos principales: aglomerativo y divisivo.Aglomerativo: También conocido como AGNES (Agglomerative Nesting). En este enfoque, el proceso comienza desde la parte inferior, donde inicialmente cada individuo se considera un clúster individual (hoja). En cada paso, se combinan los dos clústeres más similares en uno más grande (nodo). Este procedimiento se repite hasta que todos los puntos forman parte de un único clúster grande (raíz), y el resultado se visualiza como un dendrograma.Divisivo: También conocido como DIANA (Divise Analysis). Aquí, el proceso se desarrolla de arriba hacia abajo. Comienza desde la raíz, donde todos los objetos están en un solo clúster. En cada iteración, el clúster más heterogéneo se divide en dos. Este proceso se repite hasta que cada objeto tiene su propio clúster.Es relevante destacar que la agrupación aglomerativa es eficiente para identificar clústeres pequeños, mientras que la agrupación divisiva es más efectiva para identificar clústeres grandes.obstante, una pregunta más significativa es: ¿Cómo medimos la diferencia entre dos conjuntos de observaciones?. Se han desarrollado diversos métodos de aglomeración de clústeres, también conocidos como métodos de enlace, para abordar esta interrogante. Entre los tipos más comunes de estos métodos se encuentran:Maximum complete linkage clustering: Calcula todas las diferencias entre los elementos del clúster 1 y los elementos del clúster 2, y toma el valor más grande (es decir, el máximo) de estas diferencias como la distancia entre los dos clústeres. Esto tiende generar clústeres más compactos.Minimum single linkage clustering: Calcula todas las diferencias entre los elementos del clúster 1 y los elementos del clúster 2, y toma la más pequeña de estas diferencias como criterio de enlace. Esto tiende producir clústeres largos y “sueltos”.Mean average linkage clustering: Calcula todas las diferencias entre los elementos del clúster 1 y los elementos del clúster 2, y toma el promedio de estas diferencias como la distancia entre los dos clústeres.Centroid linkage clustering: Calcula la diferencia entre el centroide del clúster 1 (un vector promedio de longitud p variables) y el centroide del clúster 2.Ward’s minimum variance method: Minimiza la varianza total dentro del clúster. En cada paso, se fusionan los pares de clústeres con la mínima distancia entre clústeres.","code":"\nlibrary(dendextend) # Para comparar dos dendrogramas"},{"path":"análisis-cluster.html","id":"análsis-clúster-jerárquico-con-r","chapter":"Capítulo 6 Análisis Cluster","heading":"6.5.1 Análsis clúster jerárquico con R","text":"Existen varias funciones de R para realizar análsis clúster jerárquico, pero las más comunes son hclust y agnes para clúster jerárquico aglomerativo y diana para clúster jerárquico divisivo.","code":""},{"path":"análisis-cluster.html","id":"análisis-clúster-jerárquico-aglomerativo","chapter":"Capítulo 6 Análisis Cluster","heading":"6.5.1.1 Análisis clúster jerárquico aglomerativo","text":"Podemos realizar el análisis jerárquico aglomerativo con la función hclust. En primer lugar calculamos ma matriz de disimilaridad con dist y lo introducimos en hclust y especificamos el método de aglomeración que queremos utilizar (.e. “complete”, “average”, “single”, “ward.D”). Una vez realizado esto podemos realizar el dendrograma.\nAlternativamente, podemos utilizar la función agnes. Estas funciones se comportan de manera muy similar, sin embargo, con la función agnes, también puedes obtener el coeficiente aglomerativo, que mide la cantidad de estructura de agrupamiento encontrada (valores más cercanos 1 sugieren una estructura de agrupamiento fuerte).Esto nos permite determinar que método de agrupación jerarquico obtiene estructuras de agrupación más fuertes. continuacion calculamos el coeficiente aglomerativo para 4 métodos diferentes obteniendo que el método Ward es el mejor.De forma analoga lo realizado anteriormente podemo visualizar el dendrograma.","code":"> # Matriz de disimilaridad\n> d <- dist(df, method = \"euclidean\")\n> \n> # Clúster jerárquico usando Complete Linkage\n> hc1 <- hclust(d, method = \"complete\" )\n> \n> # Graficamos el dendrograma\n> plot(hc1, cex = 0.6, hang = -1)> # Calculado con agnes\n> hc2 <- agnes(df, method = \"complete\")\n> \n> # Coeficiente aglomerativo\n> hc2$ac\n#> [1] 0.9061342> m <- c( \"average\", \"single\", \"complete\", \"ward\")\n> results=matrix(0,2,4)\n> for (i in 1:4){\n+   results[,i]=c(m[i],agnes(df, method = m[i])$ac)\n+   }\n> print(results)\n#>      [,1]                [,2]               \n#> [1,] \"average\"           \"single\"           \n#> [2,] \"0.843969754548982\" \"0.723129823717284\"\n#>      [,3]               [,4]               \n#> [1,] \"complete\"         \"ward\"             \n#> [2,] \"0.90613423703583\" \"0.949958399929762\"> # Clúster jerárquico usando Complete Linkage\n> hc3 <- hclust(d, method = \"ward.D\" )\n> \n> # Graficamos el dendrograma\n> plot(hc3, cex = 0.6, hang = -1, main=\"Dendrograma agnes\")"},{"path":"análisis-cluster.html","id":"análisis-clúster-jerárquico-divisivo","chapter":"Capítulo 6 Análisis Cluster","heading":"6.5.1.2 Análisis clúster jerárquico divisivo","text":"La funcion de R diana proporcionada en el paquete cluster nos permite realizar el análisis cluster jerarquico divisivo, el cual funciona similar que agnes perono hay método que proporcionar.","code":"> # Calculamos el análisis cluster jerarquico divisivo\n> hc4 <- diana(df)\n> # El coeficiente de  división; Cantidad de estructura de agrupamiento encontrada\n> hc4$dc\n#> [1] 0.89846\n> # plot dendrogram\n> pltree(hc4, cex = 0.6, hang = -1, main = \"Dendrograma de diana\")"},{"path":"análisis-cluster.html","id":"trabajando-con-dendrogramas","chapter":"Capítulo 6 Análisis Cluster","heading":"6.5.2 Trabajando con dendrogramas","text":"En el dendrograma que se muestra arriba, cada hoja representa una observación. Al ascender por el árbol, las observaciones similares se combinan en ramas, las cuales su vez se fusionan una mayor altura.La altura de la fusión, indicada en el eje vertical, refleja la (des)similitud entre dos observaciones. mayor altura de fusión, menor similitud entre las observaciones. Es importante señalar que las conclusiones sobre la proximidad de dos observaciones se derivan únicamente de la altura donde las ramas que contienen esas dos observaciones se fusionan inicialmente. podemos usar la proximidad lo largo del eje horizontal como criterio de similitud.El valor de la altura del corte en el dendrograma determina el número de clústeres obtenidos y desempeña un papel similar la k en la agrupación k-medias. Para identificar subgrupos (clústeres), podemos realizar un corte en el dendrograma utilizando la función cutree.Podemos añadir la base de datos la pertenencia al cluster de cada individuo de la siguiente formaTambién es posible dibujar el dendrograma con un rectángulo marcando cada cluster. El argumento border se usa para especificar el color de cada rectanguloPodemos usar la función fviz_cluster del paquete factoextra para visualizar el resultado.Para usar cutree con agnes y diana haremos lo siguiente:Finalmente podemos comparar dos dendrogramas. Aqui comparamos el cluster jerárquico con complete linkage versus el método de Ward. La función tanglegram dibuja dos dendrogramas cada lado con las etiquetas conectadas por lineas.La salida muestra la union mediante líneas de los mismos nodos en ambos árboles. La calidad de la alineación de los dos árboles se puede medir utilizando la función “entanglement” (enredo). El entanglement es una medida entre 1 (entrelazamiento completo) y 0 (sin entrelazamiento). Un coeficiente de entrelazamiento más bajo corresponde una buena alineación. La salida de tanglegram se puede personalizar utilizando muchas otras opciones de la siguiente manera:","code":"> # Método de  Ward\n> hc5 <- hclust(d, method = \"ward.D2\" )\n> \n> # Cortamos el árbol en 3 grupos\n> sub_grp <- cutree(hc5, k = 3)\n> \n> # Numero de miembros  en cada grupo\n> table(sub_grp)\n#> sub_grp\n#>  1  2  3 \n#> 23 62 32> BaseDatos=df\n> BaseDatos %>% mutate(cluster = sub_grp) %>%  head\n#>       ventas    reventa     precio    motor_s   caballos\n#> 1 -0.5621358 -0.1440282 -0.3158715 -1.1834292 -0.7045706\n#> 2 -0.2628377  0.1588420  0.1717713  0.1433723  0.7461447\n#> 4 -0.6731286  1.0075678  1.1329225  0.4276869  0.4901361\n#> 5 -0.5157989  0.3639148 -0.1398961 -1.1834292 -0.5338982\n#> 6 -0.5373420  0.4759294  0.5640058 -0.2357138  0.3194637\n#> 7 -0.7691598  1.8067488  2.5463801  1.0910877  2.1968600\n#>   BaseNeumatico     anchura   longitud peso_revestimiento\n#> 1   -0.76099980 -1.10186337 -1.1059950         -1.1471500\n#> 2    0.09608047 -0.25204126  0.3741573          0.3231015\n#> 4    0.90347493  0.05956018  0.6413068          0.8807254\n#> 5   -0.58709946 -0.84691674 -0.7016607         -0.5459879\n#> 6    0.17060919  1.39094816  0.3091750          0.3967816\n#> 7    0.70473168  0.79607268  0.7568308          0.9678018\n#>   tapón_combustible        kpl cluster\n#> 1       -1.21562468  0.8810010       1\n#> 2       -0.16149767  0.1998747       2\n#> 4        0.04932774 -0.4812516       2\n#> 5       -0.37232307  0.6539589       2\n#> 6        0.18109362 -0.4812516       3\n#> 7        1.55145874 -0.7082937       3> require(graphics)\n> plot(hc5, cex = 0.6)\n> rect.hclust(hc5, k = 3, border = 2:4)> fviz_cluster(list(data = df, cluster = sub_grp))> # Corta el árbol agnes() en 3 grupos\n> hc_a <- agnes(df, method = \"ward\")\n> cutree(as.hclust(hc_a), k = 3)\n#>   1   2   4   5   6   7   9  10  11  12  13  14  15  17  18 \n#>   1   2   2   2   3   3   2   2   2   2   3   2   3   3   2 \n#>  20  21  22  23  24  25  26  27  29  30  31  32  33  36  37 \n#>   1   2   2   2   2   3   1   1   2   2   2   2   3   1   2 \n#>  38  40  41  42  43  44  46  47  48  49  50  52  53  54  55 \n#>   2   3   3   3   3   3   3   1   2   2   2   3   2   3   3 \n#>  56  57  58  59  60  61  62  63  64  65  66  68  69  70  71 \n#>   2   3   1   2   2   2   3   1   1   2   2   2   2   2   2 \n#>  72  74  77  78  80  81  82  83  84  85  86  87  88  89  90 \n#>   2   3   3   3   1   2   2   2   2   2   2   2   2   2   3 \n#>  91  92  93  94  95  96 102 103 104 105 106 109 112 113 114 \n#>   2   3   2   2   3   3   1   2   2   3   2   2   3   2   3 \n#> 115 116 117 119 120 121 122 123 125 126 127 130 131 132 137 \n#>   1   2   3   2   2   2   2   2   2   3   3   1   1   1   1 \n#> 138 139 140 141 143 144 145 146 147 148 149 150 \n#>   2   2   1   1   1   2   3   1   1   2   1   1\n> \n> # Corta el árbol diana() en 3 grupos\n> hc_d <- diana(df)\n> cutree(as.hclust(hc_d), k = 3)\n#>   1   2   4   5   6   7   9  10  11  12  13  14  15  17  18 \n#>   1   1   2   1   1   2   1   1   1   1   2   1   2   2   1 \n#>  20  21  22  23  24  25  26  27  29  30  31  32  33  36  37 \n#>   1   1   1   1   1   2   1   1   1   1   1   1   1   1   1 \n#>  38  40  41  42  43  44  46  47  48  49  50  52  53  54  55 \n#>   1   2   3   2   2   1   1   1   1   1   1   2   1   2   2 \n#>  56  57  58  59  60  61  62  63  64  65  66  68  69  70  71 \n#>   1   3   1   1   1   1   2   1   1   1   1   1   1   1   1 \n#>  72  74  77  78  80  81  82  83  84  85  86  87  88  89  90 \n#>   1   2   2   2   1   1   1   1   1   2   1   1   1   1   2 \n#>  91  92  93  94  95  96 102 103 104 105 106 109 112 113 114 \n#>   1   1   1   2   2   2   1   1   1   1   1   1   2   1   2 \n#> 115 116 117 119 120 121 122 123 125 126 127 130 131 132 137 \n#>   1   1   1   1   1   1   1   1   1   2   2   1   1   1   1 \n#> 138 139 140 141 143 144 145 146 147 148 149 150 \n#>   1   1   1   1   1   1   2   1   1   1   1   1> # Calcula la matriz de distancia\n> res.dist <- dist(df, method = \"euclidean\")\n> \n> # Calculamos 2 cluster jerárquicos\n> hc1 <- hclust(res.dist, method = \"complete\")\n> hc2 <- hclust(res.dist, method = \"ward.D2\")\n> \n> # Creamos dos dendrogramas\n> dend1 <- as.dendrogram (hc1)\n> dend2 <- as.dendrogram (hc2)\n> \n> tanglegram(dend1, dend2)> if(!require(dendextend)) {install.packages(\"dendextend\")}\n> library(dendextend)\n> dend_list <- dendlist(dend1, dend2)\n> tanglegram(dend1, dend2,\n+   highlight_distinct_edges = FALSE, # Turn-off dashed lines\n+   common_subtrees_color_lines = FALSE, # Turn-off line colors\n+   common_subtrees_color_branches = TRUE, # Color common branches \n+   main = paste(\"Entrelazamiento =\", round(entanglement(dend_list), 2))\n+   )"},{"path":"métodos-basados-en-árboles.html","id":"métodos-basados-en-árboles","chapter":"Capítulo 7 Métodos basados en árboles","heading":"Capítulo 7 Métodos basados en árboles","text":"","code":""},{"path":"métodos-basados-en-árboles.html","id":"introducción-1","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.1 Introducción","text":"Los árboles de decisión son modelos predictivos formados por reglas binarias (si/) con las que se consigue repartir las observaciones en función de sus atributos y predecir así el valor de la variable respuesta.Los métodos basados en árboles se han convertido en uno de los referentes dentro del ámbito predictivo debido los buenos resultados que generan en problemas muy diversos.Los árboles de decisión son uno de los métodos más simples y fáciles de interpretar para realizar predicciones en problemas de clasificación y de regresión. Estos métodos fueron desarrollados partir de los años 70 del siglo XX como una alternativa versátil los métodos clásicos de la estadística, fuertemente basados en las hipótesis de linealidad y de normalidad, y enseguida se convierten en una técnica básica del aprendizaje automático. Aunque su calidad predictiva es mediocre (especialmente en el caso de regresión), constituyen la base de otros métodos altamente competitivos (métodos de ensamblado en paralelo o en serie) en los que se combinan múltiples árboles para mejorar la predicción, pagando el precio, eso sí, de hacer más difícil la interpretación del modelo resultante.Los métodos basados en árboles son simples y útiles para la interpretación. Sin embargo, por lo general son competitivos con los mejores enfoques de aprendizaje supervisado en términos de precisión de predicción. Por lo tanto, también presentamos métodos alternativos que implican la producción de múltiples árboles que luego se combinan para producir una sola predicción de consenso. Veremos que la combinación de una gran cantidad de árboles menudo puede resultar en mejoras importantes en la precisión de la predicción.Mientras que la Regresión tiene una doble función (interpretación/preducción) los árboles deshechan de forma autómatica todas aquellas variables que son últiples para la predicción","code":""},{"path":"métodos-basados-en-árboles.html","id":"origen-de-los-árboles-de-decisión","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.2 Origen de los Árboles de Decisión","text":"Es difícil datar el orígen de una metodología. El primer artículo que desarrolla un enfoque de “árbol de decisión” data de 1959 y un investigador británico, William Belson, en un artículo titulado Matching Prediction Principle Biological Classification,1 cuyo resumen describe su enfoque como una forma de emparejar muestras de población y desarrollar criterios para hacerlo.El uso de árboles de decisión tuvo su origen en las ciencias sociales con los trabajos de Sonquist y Morgan el año 1964 y Morgan y Messenger el año 1979, ambos realizados en la Universidad de Michigan. El programa para la “Detección de Interacciones Automáticas”, creada el año 1971 por los investigadores Sonquist, Baker y Morgan, fue uno de los primeros métodos de ajuste de los datos basados en árboles de clasificación. En estadística, el año 1980, Kass introdujo un algoritmo recursivo de clasificación binario, llamado “Detección de Interacciones Automáticas Chi-cuadrado”. Hacia el año 1984, los investigadores Breiman, Friedman, Olshen y Stone, introdujeron un nuevo algoritmo para la construcción de árboles y los aplicaron problemas de regresión y clasificación. El método es conocido como “Árboles de clasificación y regresión”. Casi al mismo tiempo el proceso de inducción mediante árboles de decisión comenzó ser usado por la comunidad de “Aprendizaje automático”.Los creadores de la metodología del árbol de clasificación con aplicación al aprendizaje automático, también llamada metodología CART, fueron Leo Breiman, Jerome Friedman, Richard Olshen y Charles Stone. Su aplicación en el ámbito de la Estadística se inició en 1984.2","code":""},{"path":"métodos-basados-en-árboles.html","id":"los-árboles-de-decisión-en-las-técnicas-de-machine-learning","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.3 Los Árboles de Decisión en las técnicas de Machine Learning","text":"Los algoritmos de aprendizaje automático pueden clasificarse en dos grupos:Supervisados.supervisados.reforzar el aprendizaje.Aprendizaje supervisado: El aprendizaje supervisado es un tipo de paradigma de aprendizaje automático en el que un modelo es entrenado utilizando un conjunto de datos etiquetado. En este enfoque, el algoritmo recibe un conjunto de ejemplos de entrada junto con sus correspondientes salidas deseadas (etiquetas) durante la fase de entrenamiento. El objetivo del modelo es aprender una función que mapee las entradas las salidas de manera que pueda hacer predicciones precisas sobre nuevos datos etiquetados.En términos más simples, en el aprendizaje supervisado, el modelo aprende partir de ejemplos conocidos y luego se le proporcionan nuevos ejemplos para hacer predicciones. La “supervisión” se refiere la idea de que el proceso de entrenamiento del modelo está supervisado por las etiquetas proporcionadas en el conjunto de datos de entrenamiento.Este enfoque se utiliza en una variedad de tareas, como clasificación y regresión. En la clasificación, el modelo se entrena para asignar las entradas categorías predefinidas, mientras que en la regresión, el objetivo es predecir valores numéricos continuos. El aprendizaje supervisado es fundamental en la construcción de muchos sistemas de inteligencia artificial y toma su nombre del hecho de que el modelo es “supervisado” durante el proceso de aprendizaje con la ayuda de las etiquetas.Un árbol de decisión es un algoritmo supervisado de aprendizaje automático porque para que el algoritmo aprenda es necesaria una variable dependiente y nuestra meta es obtener una ‘función’ que permita predecir, partir de variables independientes, el valor de la variable objetivo para casos desconocidos.Aprendizaje supervisado: Los algoritmos de aprendizaje supervisado trabajan de forma muy similar los supervisados, con la diferencia de que éstos sólo ajustan su modelo predictivo tomando en cuenta los datos de entrada, sin importar los de salida. Es decir, diferencia del supervisado, los datos de entrada están clasificados ni etiquetados, y son necesarias estas características para entrenar el modelo. Dentro de este tipo de algoritmos, el agrupamiento o clustering en inglés, es el más utilizado, ya que particiona los datos en grupos que posean características similares entre síLa Figura \\(\\ref{fig:algoritmos}\\) muestra esquemáticamente los principales algoritmos de Machine Learning.","code":""},{"path":"métodos-basados-en-árboles.html","id":"árboles-de-regresión","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.4 Árboles de regresión","text":"Los árboles de regresión se aplican cuando la variable respuesta es continua. En términos generales, en el entrenamiento de un árbol de regresión, las observaciones se van distribuyendo por bifurcaciones (nodos) generando la estructura del árbol hasta alcanzar un nodo terminal. Cuando se quiere predecir una nueva observación, se recorre el árbol acorde al valor de sus predicciones hasta alcanzar uno de los nodos terminales. La predicción del árbol es la media/mediana/moda de la variable respuesta de las observaciones de entrenamiento que están en ese mismo nodo terminal.","code":""},{"path":"métodos-basados-en-árboles.html","id":"un-ejemplo-de-juguete","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.4.1 Un ejemplo de juguete","text":"Usaremos el mismo ejemplo que aparece en James Gareth et al.3 (pag. 304) para ilustrar cómo funcionan los árboles. En este ejemplo, objetivo es predecir una variable cuantitativa, el salario de los jugadores de béisbol, según los Años (cantidad de años que ha jugado en las ligas principales) y la cantidad de Hits/aciertos del año anterior.El siguiente código muestra un ejemplo básico de obtención de un árbol. Nótese que antes de realizar el análisis se eliminarán las observaciones las que les faltan valores de Salario. También se transforma logarítmicamente el Salario para que su distribución tenga una forma de campana más típica (Recuerde que el salario se mide en miles de dólares).\nFigure 7.1: Árbol de regresión\nLa Figura \\(\\ref{fig:myfirsttree}\\) muestra un árbol de regresión ajustado estos datos. Nótese que tiene forma de árbol (ver Sección \\(\\ref{sec:terminologia}\\)). Consiste en una serie de reglas de división binaria, comenzando en la parte superior del árbol. La división superior asigna observaciones que tienen menos de 4,5 años (Years < 4.5) la rama izquierda. El salario previsto para estos jugadores viene dado por el valor de respuesta media para los jugadores en el conjunto de datos con (Años<4.5). Para tales jugadores, el salario logarítmico medio es 5,107, por lo que hacemos una predicción de 165174$, para estos jugadores. Los jugadores con (\\(Years \\geq 4.5\\)) se asignan la rama derecha, y luego ese grupo se subdivide aún más por ‘Hits’. En general, el árbol estratifica o segmenta los jugadores en tres regiones del espacio predictor: jugadores que han jugado durante cuatro años y medio o menos, jugadores que han jugado durante cuatro años y medio o más y que hicieron menos de 118 aciertos el año pasado y jugadores que han jugado durante cuatro años y medio o más y que hizo al menos 118 hits el año pasado.Estas tres regiones se pueden escribir como:\\(R_1\\) = { X | Años < 4.5 },\\(R_1\\) = { X | Años < 4.5 },\\(R_2\\)={X | Años>=4.5, Hits < 117.5 } y\\(R_2\\)={X | Años>=4.5, Hits < 117.5 } y\\(R_3\\) = {X | años >= 4.5, Hits >= 117.5 }.\\(R_3\\) = {X | años >= 4.5, Hits >= 117.5 }.La Figura \\(\\ref{fig:particion0}\\) ilustra las regiones en función de Años y Hits/Aciertos. Los salarios pronosticados para estos tres grupos son 1000 × exp(5,107) = 165 174\\(, 1000 × exp(5,999) = 402 834\\) y 1,000 × exp(6.740)=845,346$ respectivamente.\nFigure 7.2: Partición del espacio con predicciones\nPodríamos interpretar el árbol de regresión que se muestra en la Figura \\(\\ref{fig:myfirsttree}\\) de la siguiente manera: los años son el factor más importante para determinar el salario, y los jugadores con menos experiencia ganan salarios más bajos que los jugadores más experimentados. Dado que un jugador tiene menos experiencia, la cantidad de golpes que hizo en el año anterior parece jugar un papel pequeño en su salario. Pero entre los jugadores que han estado en las ligas mayores durante cinco años o más, la cantidad de Hits/aciertos hechos el año anterior sí afecta el salario, y los jugadores que hicieron más Hits/aciertos el año pasado tienden tener salarios más altos. El árbol de regresión que se muestra en la Figura \\(\\ref{fig:myfirsttree}\\) es probablemente una simplificación excesiva de la verdadera relación entre Aciertos, Años y Salario. Sin embargo, tiene ventajas sobre otros tipos de modelos de regresión.","code":"\nif(!require(ISLR)) {install.packages(\"ISLR\")}\n#> Loading required package: ISLR\nlibrary(ISLR) # provide the collection of data-sets used in the book 'An Introduction to Statistical Learning with Applications in R'\nif(!require(tree)) {install.packages(\"tree\")}\n#> Loading required package: tree\nlibrary(tree) # Classification and regression trees\ndata(Hitters)\nHitters <- Hitters[!is.na(Hitters$Salary),]\nformula <- log(Salary) ~ Years + Hits \nmyfirsttree <- tree(formula, data = Hitters, control=tree.control(300, mincut = 60))\nplot(x = myfirsttree, type = \"proportional\")\ntext(x = myfirsttree, splits = TRUE, pretty = 0, cex = 1, col = \"firebrick\")\nplot(Hitters$Years, Hitters$Hits, col=\"orange\", pch=16 ,xlab=\"Years\", ylab=\"Hits\")\npartition.tree(myfirsttree, cex = 2, add = TRUE)"},{"path":"métodos-basados-en-árboles.html","id":"sec:terminologia","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.4.2 Terminología","text":"Un árbol de decisión en Machine Learning es una estructura de árbol similar un diagrama de flujo donde un nodo interno representa una característica (o atributo), la rama representa una regla de decisión y cada nodo hoja representa el resultado.El nodo superior en un árbol de decisión en Machine Learning se conoce como el nodo raíz. Aprende particionar en función del valor del atributo. Divide el árbol de una manera recursiva llamada partición recursiva.Cada nodo en el árbol actúa como un caso de prueba para algún atributo, y cada borde que desciende de ese nodo corresponde una de las posibles respuestas al caso de prueba. Este proceso es recursivo y se repite para cada subárbol enraizado en los nuevos nodos.La Figura \\(\\ref{fig:arbol}\\) muestra gráficamente la estructura de árbol.Nodo raíz (nodo de decisión superior ): Representa toda la población o muestra y esto se divide en dos o más conjuntos homogéneos.Nodo raíz (nodo de decisión superior ): Representa toda la población o muestra y esto se divide en dos o más conjuntos homogéneos.División: Es un proceso de división de un nodo en dos o más subnodos.División: Es un proceso de división de un nodo en dos o más subnodos.Nodo de decisión: Cuando un subnodo se divide en subnodos adicionales, se llama nodo de decisión.Nodo de decisión: Cuando un subnodo se divide en subnodos adicionales, se llama nodo de decisión.Nodo de hoja / terminal: Los nodos sin hijos (sin división adicional) se llaman Hoja o nodo terminal.Nodo de hoja / terminal: Los nodos sin hijos (sin división adicional) se llaman Hoja o nodo terminal.Poda: Cuando reducimos el tamaño de los árboles de decisión eliminando nodos (opuesto la división), el proceso se llama poda.Poda: Cuando reducimos el tamaño de los árboles de decisión eliminando nodos (opuesto la división), el proceso se llama poda.Rama / Subárbol: Una subsección del árbol de decisión se denomina rama o subárbol.Rama / Subárbol: Una subsección del árbol de decisión se denomina rama o subárbol.Nodo padre e hijo: Un nodo, que se divide en subnodos se denomina nodo principal de subnodos, mientras que los subnodos son hijos de un nodo principal.Nodo padre e hijo: Un nodo, que se divide en subnodos se denomina nodo principal de subnodos, mientras que los subnodos son hijos de un nodo principal.","code":""},{"path":"métodos-basados-en-árboles.html","id":"como-se-crea-un-árbol","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.4.3 Como se crea un árbol","text":"En esta sección se presenta el proceso de construcción de un árbol de decisión. Básicamente, los árboles se construyen en dos dos pasos:Paso 1: Se divide el espacio de predicciones, es decir, el conjunto de valores posibles para \\(X_1, X_2,..., X_p\\), en J regiones distintas y solapadas, \\(R_1, R_2,..., R_J\\).Paso 1: Se divide el espacio de predicciones, es decir, el conjunto de valores posibles para \\(X_1, X_2,..., X_p\\), en J regiones distintas y solapadas, \\(R_1, R_2,..., R_J\\).Paso 2: Para cada observación que cae en la región \\(R_j\\), se hace la misma predicción, que simplemente es la media (o la mediana o la moda) de los valores de respuesta para las observaciones de entrenamiento en \\(R_j\\).Paso 2: Para cada observación que cae en la región \\(R_j\\), se hace la misma predicción, que simplemente es la media (o la mediana o la moda) de los valores de respuesta para las observaciones de entrenamiento en \\(R_j\\).¿Cómo se construyen las regiones \\(R_1, R_2,..., R_J\\)? En teoría, podrían tener cualquier forma. Sin embargo, por simplicidad se divide el espacio de predicciones en rectángulos o cajas (Figura \\(\\ref{fig:cajas}\\)). Esta división también ayuda en la interpretación del modelo predictivo resultante. Hay también posibilidad de realizar divisiones oblicuas4 o flexibles.5En el caso de los árboles de regresión, el criterio empleado con más frecuencia para identificar las divisiones es el observar la suma de los residuos al cuadrado (Residual Sum Squares, \\(RSS\\)) que es una medida de la discrepancia entre los datos reales y los predichos por el modelo. El objetivo es encontrar las J regiones \\((R_1,…, R_j)\\) que minimizan el \\(RSS\\) total. Un \\(RSS\\) bajo indica un buen ajuste del modelo los datos. Por tanto, el objetivo es encontrar cajas \\(R_1, R_2,..., R_J\\)? que minimicen el \\(RSS\\), dado por:\\[\nRSS=\\sum_{j=1}^J{\\sum_{\\R_j} (y_i-{\\hat y}_{R_j})^2}\n\\]\ndonde \\({\\hat y}_{R_j}\\) es el valor predicho (para las observaciones de entrenamiento) dentro de la \\(R_j\\) caja. Desafortunadamente, es computacionalmente inviable considerar cada posible partición del espacio de características en cajas J. Por esta razón, se adopta un enfoque de arriba-hacia-abajo (top-) que se conoce como división binaria recursiva. El enfoque es de arriba-hacia-abajo porque comienza en la parte superior del árbol (en cuyo punto todas las observaciones pertenecen una sola región) y luego divide sucesivamente el espacio predictor; cada división se indica través de dos nuevas ramas más abajo en el árbol.Para realizar la división binaria recursiva, primero seleccionamos el predictor \\(X_j\\) y el punto de corte \\(s\\) tal que dividiendo el espacio del predictor en las regiones \\(\\{X|X_j < s\\}\\) y \\(\\{X|X_j \\geq s\\}\\) conduce la mayor reducción posible en \\(RSS\\). (La notación \\(\\{X|X_j < s\\}\\) significa la región del predictor espacio en el que \\(X_j\\) toma un valor menor que \\(s\\)). Es decir, consideramos todos los predictores \\(X_1,..., X_p\\) y todos los valores posibles del punto de corte s para cada uno de los predictores, y luego elegimos el predictor y el punto de corte tales que el árbol resultante tiene el \\(RSS\\) más bajo. Con mayor detalle, para cualquier j y s, definimos el par de semiplanos\\[\nR_1(j,s)=\\{X|X_j < s\\} \\ \\ y \\ \\ R_2(j,s)=\\{X|X_j \\geq s\\}\n\\]y se buscan los valores de j y s que minimizan la ecuación\\[\nRSS={\\sum_{\\R_1} (y_i-{\\hat y}_{R_1})^2} + {\\sum_{\\R_2} (y_i-{\\hat y}_{R_2})^2}\n\\]","code":""},{"path":"métodos-basados-en-árboles.html","id":"evitar-el-overfitting","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.4.4 Evitar el overfitting","text":"El proceso de construcción de árboles tiende reducir rápidamente el error \\(SSR\\), de tal forma que el modelo se ajusta muy bien las observaciones empleadas como entrenamiento. Como consecuencia, se genera un sobreajuste (overfitting) que reduce su capacidad predictiva al aplicarlo nuevos datos. La razón de este comportamiento radica en la facilidad con la que los árboles se ramifican adquiriendo estructuras complejas. De hecho, si se limitan las divisiones, todo árbol termina ajustándose perfectamente las observaciones de entrenamiento creando un nodo terminal por observación. Existen dos estrategias para prevenir el problema de overfitting de los árboles:\n- limitar el tamaño del árbol (parada temprana o early stopping) y\n- el proceso de podado ( pruning).La Figura \\(\\ref{fig:sobreajuste}\\) muestra el comportamiento del overffiting.","code":""},{"path":"métodos-basados-en-árboles.html","id":"controlar-el-tamaño-del-árbol-parada-temprana","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.4.4.1 Controlar el tamaño del árbol (parada temprana)","text":"El tamaño final que adquiere un árbol puede controlarse mediante reglas de parada que detengan la división de los nodos dependiendo de si se cumplen o determinadas condiciones. El nombre de estas condiciones puede variar dependiendo del software o librería empleada, pero suelen estar presentes en todos ellos.Observaciones mínimas para división: define el número mínimo de observaciones que debe tener un nodo para poder ser dividido. Cuanto mayor el valor, menos flexible es el modelo.Observaciones mínimas de nodo terminal: define el número mínimo de observaciones que deben tener los nodos terminales. Su efecto es muy similar al de observaciones mínimas para división.Profundidad máxima del árbol: define la profundidad máxima del árbol, entendiendo por profundidad máxima el número de divisiones de la rama más larga (en sentido descendente) del árbol.Número máximo de nodos terminales: define el número máximo de nodos terminales que puede tener el árbol. Una vez alcanzado el límite, se detienen las divisiones. Su efecto es similar al de controlar la profundidad máxima del árbol.Reducción mínima de error: define la reducción mínima de error que tiene que conseguir una división para que se lleve cabo.todos estos parámetros se les conoce como hiperparámetros porque se aprenden durante el entrenamiento del modelo. Su valor tiene que ser especificado por el usuario en base su conocimiento del problema y mediante el uso de validación cruzada.","code":""},{"path":"métodos-basados-en-árboles.html","id":"post-pruning","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.4.4.2 Post-pruning","text":"El proceso descrito anteriormente puede producir buenas predicciones en el conjunto de entrenamiento, pero es probable que se produzca un sobreajuste los datos, lo que conducirá un rendimiento deficiente del conjunto de prueba. Esto se debe que el árbol resultante puede ser demasiado complejo. Un árbol más pequeño con menos divisiones (menos regiones \\(R_1,... , R_J\\)) podría generar una aceptable predicción y una mejor interpretación costa de un pequeño sesgo. Una posible alternativa al proceso descrito anteriormente es construir el árbol solo mientras la disminución en el \\(RSS\\) debido cada división exceda algún umbral (threshold). Esta estrategia dará como resultado árboles más pequeños, pero es demasiado miope, ya que una división aparentemente sin valor en un nodo al principio del árbol podría ser seguida por una muy buena división en otro nodo posterior que conduzca una gran reducción en \\(RSS\\).Una estrategia mejor sería hacer crecer un árbol \\(T_0\\) muy grande y luego podarlo para obtener un subárbol. La cuestión es: ¿Cómo determinamos la mejor manera de podar el árbol? Intuitivamente, nuestro objetivo es seleccionar un subárbol que conduzca la tasa de error más baja.Dado un subárbol, se podría por ejemplo estimar su error utilizando un proceso de validación cruzada. Sin embargo, aplicar este procedimiento para cada posible subárbol sería demasiado engorroso, ya que es posible identificar una cantidad extremadamente grande de subárboles. En su lugar, es necesario encontrar una forma de seleccionar un pequeño conjunto de subárboles para su consideración/evaluación. La reducción de la complejidad de los costos, también conocida como reducción del eslabón más débil, brinda una manera de hacer precisamente esto. En lugar de considerar todos los subárboles posibles, consideramos una secuencia de árboles indexados por un parámetro de ajuste negativo \\(\\alpha\\).En este caso, se busca el sub-árbol T que minimiza la ecuación (Cost complexity pruning)\\[\n\\sum_{j=1}^{|T|}{\\sum_{\\R_j} (y_i-{\\hat y}_{R_j})^2} + \\alpha|T|\n\\]\nes lo más pequeño posible.Aquí |T| indica el número de nodos terminales del árbol T.El primer término de la ecuación se corresponde con el sumatorio total de los residuos cuadrados RSS. Por definición, cuantos más nodos terminales tenga el modelo menor será esta parte de la ecuación. El segundo término es la restricción, que penaliza al modelo en función del número de nodos terminales (mayor número, mayor penalización). El grado de penalización se determina mediante el parámetro \\(\\alpha\\) que es la medida de costo-complejidad (cost complexity). Cuando \\(\\alpha\\)=0, la penalización es nula y el árbol resultante es equivalente al árbol original. medida que se incrementa \\(\\alpha\\) la penalización es mayor y, como consecuencia, los árboles resultantes son de menor tamaño. El valor óptimo de \\(\\alpha\\) puede identificarse mediante cross validation.El parámetro \\(\\alpha\\) controla un equilibrio entre la complejidad del subárbol y su ajuste los datos de entrenamiento. Cuando \\(\\alpha\\)= 0, entonces el subárbol T simplemente será igual \\(T_0\\), porque entonces solo mide el error de entrenamiento. Sin embargo, medida que aumenta \\(\\alpha\\), hay que pagar un precio por tener un árbol con muchos nodos terminales, por lo que la cantidad tenderá minimizarse para un subárbol más pequeño.Resulta que medida que se aumentea el valor de \\(\\alpha\\) desde cero, las ramas se eliminan del árbol de forma anidada y predecible, por lo que es fácil obtener la secuencia completa de subárboles en función de \\(\\alpha\\).Es posible seleccionar un valor de \\(\\alpha\\) usando un conjunto de validación o usando validación cruzada.","code":""},{"path":"métodos-basados-en-árboles.html","id":"validación-cruzada","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.4.5 Validación cruzada","text":"La validación cruzada o cross-validation es una técnica utilizada para evaluar los resultados de un análisis estadístico cuando el conjunto de datos se ha segmentado en una muestra de entrenamiento y otra de prueba, la validación cruzada (Figura ) comprueba si los resultados del análisis son independientes de la partición. Aunque la validación cruzada es una técnica diseñada para modelos de regresión y predicción, su uso se ha extendido muchos otros ejercicios de Machine Learning.La manera más sencilla de realizar la validación cruzada es una vez segmentado el conjunto de datos en la muestra de entrenamiento y test, consiste en resolver el modelo con los datos de entrenamiento, y probar el modelo estimado en la muestra de test, la simple comparación del resultado obtenido en dicha muestra con las observaciones reales nos permite validar el modelo en términos de error (proceso hold-). Una aplicación alternativa consiste en repetir el proceso anterior, seleccionando aleatoriamente distintos conjuntos de datos de entrenamiento, y calcular los estadísticos de validación partir de la media de los valores en cada una de las repeticiones. este método se le denomina Validación cruzada aleatoria (Figura \\(\\ref{fig:esquema}\\)). Los inconvenientes es que hay algunas muestras que quedan sin evaluar y otras que se evalúan más de una vez, es decir, los subconjuntos de prueba y entrenamiento se pueden solapar.","code":""},{"path":"métodos-basados-en-árboles.html","id":"algoritmo-para-crear-un-árbol-de-regresión-con-pruning","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.4.6 Algoritmo para crear un árbol de regresión con pruning","text":"1. Se emplea recursive binary splitting para crear un árbol grande y complejo (\\(T_0\\)) empleando los datos de training y reduciendo al máximo posible las condiciones de parada. Normalmente se emplea como única condición de parada el número mínimo de observaciones por nodo terminal.1. Se emplea recursive binary splitting para crear un árbol grande y complejo (\\(T_0\\)) empleando los datos de training y reduciendo al máximo posible las condiciones de parada. Normalmente se emplea como única condición de parada el número mínimo de observaciones por nodo terminal.2. Se aplica el cost complexity pruning al árbol \\(T_0\\) para obtener el mejor sub-árbol en función de \\(\\alpha\\). Es decir, se obtiene el mejor sub-árbol para un rango de valores de \\(\\alpha\\).2. Se aplica el cost complexity pruning al árbol \\(T_0\\) para obtener el mejor sub-árbol en función de \\(\\alpha\\). Es decir, se obtiene el mejor sub-árbol para un rango de valores de \\(\\alpha\\).3. Identificación del valor óptimo de \\(\\alpha\\) mediante k-cross-validation. Se divide la muestra de entrenamiento en K grupos. Para \\(k=1,...,K\\):3. Identificación del valor óptimo de \\(\\alpha\\) mediante k-cross-validation. Se divide la muestra de entrenamiento en K grupos. Para \\(k=1,...,K\\):4. Repetir pasos 1 y 2 empleando todas las observaciones excepto las del grupo -ésimo.4. Repetir pasos 1 y 2 empleando todas las observaciones excepto las del grupo -ésimo.5. Evaluar el \\(SSR\\) para el rango de valores de \\(\\alpha\\) empleando el grupo -ésimo.5. Evaluar el \\(SSR\\) para el rango de valores de \\(\\alpha\\) empleando el grupo -ésimo.6. Obtener el promedio de los K \\(SSR\\) calculados para cada valor \\(\\alpha\\).6. Obtener el promedio de los K \\(SSR\\) calculados para cada valor \\(\\alpha\\).Seleccionar el sub-árbol del paso 2 que se corresponde con el valor \\(\\alpha\\) que ha conseguido el menor cross-validation mean squared error en el paso 3.En el caso de los árboles de clasificación (ver más adelante), en lugar de emplear la suma de residuos cuadrados \\(SSR\\) como criterio de selección, se emplea una medida alternativa (ver árboles de clasificación).","code":""},{"path":"métodos-basados-en-árboles.html","id":"árboles-de-clasificación","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.5 Árboles de clasificación","text":"Los árboles de clasificación son el subtipo de árboles de decisión que se aplica cuando la variable respuesta es categórica.","code":""},{"path":"métodos-basados-en-árboles.html","id":"cómo-se-crea-un-árbol-de-clasificación","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.5.1 Cómo se crea un árbol de clasificación","text":"Para construir un árbol de clasificación se emplea el mismo método descrito en los árboles de regresión. Sin embargo, como la variable respuesta en cualitativa, es posible emplear el \\(RSS\\) como criterio de selección de las divisiones óptimas. Existen varias alternativas, todas ellas con el objetivo de encontrar nodos lo más puros/homogéneos posible. Las más empleadas son:Classification Error Rate: Se define como la proporción de observaciones que pertenecen la clase más común en el nodo.\n\\[\nE_m= 1- max (\\hat p_{mk})\n\\]\ndonde \\(\\hat p_{mk}\\) representa la proporción de observaciones del nodo m que pertenecen la clase k. pesar de la sencillez de esta medida, es suficientemente sensible para crear buenos árboles, por lo que, en la práctica, suelen emplearse otras medidas.Classification Error Rate: Se define como la proporción de observaciones que pertenecen la clase más común en el nodo.\n\\[\nE_m= 1- max (\\hat p_{mk})\n\\]\ndonde \\(\\hat p_{mk}\\) representa la proporción de observaciones del nodo m que pertenecen la clase k. pesar de la sencillez de esta medida, es suficientemente sensible para crear buenos árboles, por lo que, en la práctica, suelen emplearse otras medidas.Gini Index: Es una medida de la varianza total en el conjunto de las K clases del nodo m. Se considera una medida de pureza del nodo.\n\\[\nG = \\sum_{k=1}^K \\hat p_{mk}(1-\\hat p_{mk})\n\\]\nCuando \\(\\hat p_{mk}\\) es cercano 0 o 1 (el nodo contiene mayoritariamente observaciones de una clase), el término \\(\\hat p_{mk}(1-\\hat p_{mk})\\) es muy pequeño. Como consecuencia, cuanto mayor sea la pureza del nodo, menor el valor del índice Gini G.Gini Index: Es una medida de la varianza total en el conjunto de las K clases del nodo m. Se considera una medida de pureza del nodo.\n\\[\nG = \\sum_{k=1}^K \\hat p_{mk}(1-\\hat p_{mk})\n\\]\nCuando \\(\\hat p_{mk}\\) es cercano 0 o 1 (el nodo contiene mayoritariamente observaciones de una clase), el término \\(\\hat p_{mk}(1-\\hat p_{mk})\\) es muy pequeño. Como consecuencia, cuanto mayor sea la pureza del nodo, menor el valor del índice Gini G.Cross Entropy: La entropía es otra forma de cuantificar el desorden de un sistema. En el caso de los nodos, el desorden se corresponde con la impureza. Si un nodo es puro, contiene únicamente observaciones de una clase, su entropía es cero. Por el contrario, si la frecuencia de cada clase es la misma, el valor de la entropía alcanza el valor máximo de 1.\n\\[\nD = \\sum_{k=1}^K \\hat p_{mk}log(\\hat p_{mk})\n\\]Cross Entropy: La entropía es otra forma de cuantificar el desorden de un sistema. En el caso de los nodos, el desorden se corresponde con la impureza. Si un nodo es puro, contiene únicamente observaciones de una clase, su entropía es cero. Por el contrario, si la frecuencia de cada clase es la misma, el valor de la entropía alcanza el valor máximo de 1.\n\\[\nD = \\sum_{k=1}^K \\hat p_{mk}log(\\hat p_{mk})\n\\]Chi-Square: Esta aproximación consiste en identificar si existe una diferencia significativa entre los nodos hijos y el nodo parental, es decir, si hay evidencias de que la división consigue una mejora. Para ello, se aplica un test estadístico chi-square goodness fit empleando como distribución esperada\n\\(H_0\\) la frecuencia de cada clase en el nodo parental. Cuanto mayor el estadístico \\(\\chi^2\\), mayor la evidencia estadística de que existe una diferencia.\n\\[\n\\chi^2=\\sum_k{(observado_k-esperado_k) \\esperado_k}\n\\]\nLos árboles generados con este criterio de división reciben el nombre de CHAID (Chi-square Automatic Interaction Detector).Chi-Square: Esta aproximación consiste en identificar si existe una diferencia significativa entre los nodos hijos y el nodo parental, es decir, si hay evidencias de que la división consigue una mejora. Para ello, se aplica un test estadístico chi-square goodness fit empleando como distribución esperada\n\\(H_0\\) la frecuencia de cada clase en el nodo parental. Cuanto mayor el estadístico \\(\\chi^2\\), mayor la evidencia estadística de que existe una diferencia.\n\\[\n\\chi^2=\\sum_k{(observado_k-esperado_k) \\esperado_k}\n\\]\nLos árboles generados con este criterio de división reciben el nombre de CHAID (Chi-square Automatic Interaction Detector).Independientemente de la medida empleada como criterio de selección de las divisiones, el proceso siempre es el mismo.","code":""},{"path":"métodos-basados-en-árboles.html","id":"predicción-del-árbol","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.5.2 Predicción del árbol","text":"Tras la creación de un árbol, las observaciones de entrenamiento quedan agrupadas en los nodos terminales. Para predecir una nueva observación, se recorre el árbol en función del valor de sus predictores hasta llegar uno de los nodos terminales. En el caso de clasificación, suele emplearse la moda de la variable respuesta como valor de predicción, es decir, la clase más frecuente del nodo. Además, puede acompañarse con el porcentaje de cada clase en el nodo terminal, lo que aporta información sobre la confianza de la predicción.","code":""},{"path":"métodos-basados-en-árboles.html","id":"ventajas-y-desventajas-de-los-árboles-de-decisión","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.6 Ventajas y desventajas de los Árboles de Decisión","text":"","code":""},{"path":"métodos-basados-en-árboles.html","id":"ventajas-algunas","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.6.1 Ventajas (algunas)","text":"Fácil de entenderUtil en exploración de datos:identificar importancia de variables partir de cientos de variables.Menos limpieza de datos: outliers y valores faltantes influencian el modelo (un cierto grado)El tipo de datos es una restricciónEs un método paramétrico","code":""},{"path":"métodos-basados-en-árboles.html","id":"desventajas-algunas","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.6.2 Desventajas (algunas)","text":"SobreajustePérdida de información al categorizar variables continuasInestabilidad: un pequeño cambio en los datos puede modificar ampliamente la estructura del árbol. Por lo tanto la interpretación es tan directa como parece.Un árbol de decisión puede llegar ser demasiado complejo con facilidad, perdiendo su utilidad.","code":""},{"path":"métodos-basados-en-árboles.html","id":"paquetes-de-r","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.7 Paquetes de R","text":"Algunas librerías que permiten crear y representar árboles de regresión y clasificación son: tree,6 rpart7 o party:8rpart: Este es el primer paquete de R para árboles de clasificación y árboles de regresión. Tiene funciones de poda y gráficos muy generales. Se complementa con otro paquete rpart.plot9party: El núcleo del paquete es la función ctree(), una implementación de árboles de inferencia condicional que integran modelos de regresión estructurados en árbol en una teoría bien definida de procedimientos de inferencia condicional. Esta clase paramétrica de árboles de regresión es aplicable todo tipo de problemas de regresión, incluidas variables de respuesta nominales, ordinales, numéricas, censuradas y multivariadas y escalas de medición arbitrarias de las covariables. Una de las principales diferencias que tiene con otros métodos de partición es que hace prunning debido su sustento estadístico. Es decir, el árbol sigue creciendo hasta que quede ninguna variable que posea una relación significativa con el target.tree: La función para crear un árbol de regresión es tree, continuación la estructura de la función.randomForest: dispone de los principales algoritmos para crear modelos Random Forest. Destaca por su fácil uso, pero por su rapidez.ranger: algoritmos para crear modelos random forest. Es similar randomForest pero mucho más rápido. Además, incorpora extremely randomized trees y quantile regression forests.gbm: dispone de los principales algoritmos de boosting. Este paquete ya está mantenido, aunque es útil para explicar los conceptos, se recomienda su uso en producción.XGBoost: esta librería permite acceder al algoritmo XGboost (Extra Gradient boosting). Una adaptación de gradient boosting que destaca por su eficiencia y rapidez.H2O: implementaciones muy optimizadas de los principales algoritmos de machine learning, entre ellos random forest, gradient boosting y XGBoost. Para conocer más detalles sobre cómo utilizar esta librería consultar Machine Learning con H2O y R.C50: implementación de los algoritmos C5.0 para árboles de clasificación.Existen otros paquetes que el lector puede consultar en la sección Recursive Partitioning de CRAN Task View: Machine Learning & Statistical Learning.","code":""},{"path":"métodos-basados-en-árboles.html","id":"ejemplo-árbol-de-clasificación","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.8 Ejemplo Árbol de Clasificación","text":"Un conjunto de datos clásico en Machine Learning son los datos del Titanic. Este set de datos contiene información sobre los pasajeros del RMS Titanic, el transatlántico británico que se hundió en abril de 1912 durante su viaje inaugural desde Southampton Nueva York. Entre la información almacenada se encuentran la edad, género, características socio-económicas de los pasajeros y si sobrevivieron o al naufragio. Aunque pueda resultar un clásico poco original, estos datos tienen una serie de características que los hacen idóneos para ser utilizados como ejemplo introductorio. Se trata de un problema y de unos datos cuyas variables pueden entenderse de forma sencilla. Es intuitivo comprender el impacto que puede tener la edad, el sexo, la localización del camarote en la supervivencia de los pasajeros. Aunque lo parezca, comprender fondo el problema que se pretende modelar es lo más importante para lograr buenos resultados.En este ejemplo utilizaremos la librería rpart y rpart.plot para obtener los gráficos de árboles.En primer lugar obtendremos un simple árbol sin ningún tipo de restriccionesPor defecto, rpart() usa la medida de Gini para la división de los nodos.Los resultados que ofrece el algoritmo son:La librería rpart.plot muestra los resultados gráficamente lo que puede ayudar comprender el funcionamiento del algoritmo.En cada nodo se indica como se clasifican los individuosEn segundo lugar se muestra el error de clasificarlos como se indica en la primera filaEn la tercera fila el % elementos en ese nodo\nFigure 7.3: Árbol base.\nPor ejemplo, el la primera rama de la izquierda se seleccionan “Hombres” que supone un 64% de la muestra. Todos los hombres (n=843) se clasifican como “Muertos”. La realidad es que de los 843 hombres, se salvaron 161 y por tanto el error = 161/843 = 0.19Para mejorar este resultado, esta muestra de hombres puede subdividirse dependiendo de la edad para mejorar la predicción. Si la edad es mayor o igual 9,5 años se clasifican como “Muertos” y si la edad es menor de 9.5, se clasifican como Sobrevivientes”Conforme el arbol va “creciendo” en “ramas” cada vez se comente menores errores.La capacidad predictiva puede ser muy alta si el árbol se deja crecer sin control.El árbol sin podado clasifica correctamente el 83% de las observaciones","code":"\nlibrary(rpart)\nlibrary(rpart.plot)\ndata(\"ptitanic\")\narbol_1 <- rpart(survived ~., data = ptitanic, method = 'class')\narbol_1\n#> n= 1309 \n#> \n#> node), split, n, loss, yval, (yprob)\n#>       * denotes terminal node\n#> \n#>   1) root 1309 500 died (0.6180290 0.3819710)  \n#>     2) sex=male 843 161 died (0.8090154 0.1909846)  \n#>       4) age>=9.5 796 136 died (0.8291457 0.1708543) *\n#>       5) age< 9.5 47  22 survived (0.4680851 0.5319149)  \n#>        10) sibsp>=2.5 20   1 died (0.9500000 0.0500000) *\n#>        11) sibsp< 2.5 27   3 survived (0.1111111 0.8888889) *\n#>     3) sex=female 466 127 survived (0.2725322 0.7274678)  \n#>       6) pclass=3rd 216 106 died (0.5092593 0.4907407)  \n#>        12) sibsp>=2.5 21   3 died (0.8571429 0.1428571) *\n#>        13) sibsp< 2.5 195  92 survived (0.4717949 0.5282051)  \n#>          26) age>=16.5 162  79 died (0.5123457 0.4876543)  \n#>            52) parch>=3.5 9   1 died (0.8888889 0.1111111) *\n#>            53) parch< 3.5 153  75 survived (0.4901961 0.5098039)  \n#>             106) age>=27.5 44  17 died (0.6136364 0.3863636) *\n#>             107) age< 27.5 109  48 survived (0.4403670 0.5596330)  \n#>               214) age< 21.5 28  11 died (0.6071429 0.3928571) *\n#>               215) age>=21.5 81  31 survived (0.3827160 0.6172840) *\n#>          27) age< 16.5 33   9 survived (0.2727273 0.7272727) *\n#>       7) pclass=1st,2nd 250  17 survived (0.0680000 0.9320000) *\nrpart.plot(arbol_1)\n# Predicción con sobrajuste\n# ==============================================================================\nprediccion_0 <- predict(arbol_1, type = \"class\")\ntable_mat <- table(ptitanic$survived, prediccion_0)\ntable_mat\n#>           prediccion_0\n#>            died survived\n#>   died      749       60\n#>   survived  169      331\naccuracy_Test <- sum(diag(table_mat)) / sum(table_mat)\nprint(paste('Accuracy for test', round(accuracy_Test,3)))\n#> [1] \"Accuracy for test 0.825\""},{"path":"métodos-basados-en-árboles.html","id":"pre-podado","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.8.1 Pre-podado","text":"La Figura \\(\\ref{fig:arbol1}\\) muestra un árbol sin restricciones. Se puede hacer un prepodado controlando el número mínimo de observaciones en un nodo terminal:\nFigure 7.4: Árbol base 2.\nOtra alternativa sería controlar tanto el número mínimo de observaciones en un nodo (“minbucket=50”) como el número mínimo de observaciones en un nodo para poder ser dividido (“minsplit = 250” si en un nodo hay 250 observaciones o menos, entoncen éste se dividirá y el arbol crecerá más por esa rama).\nFigure 7.5: Árbol base 3.\n","code":"\n# Control de prepodado\n# ==============================================================================\ncontrol <- list(minbucket = 50)\narbol_2 <- rpart(survived ~., data = ptitanic, method = 'class', control = control)\nlibrary(rpart.plot)\nrpart.plot(arbol_2, digits = 3)\n# Control de prepodado\n# ==============================================================================\ncontrol <- list(minbucket = 50, minsplit = 250)\narbol_3 <- rpart(survived ~., data = ptitanic, method = 'class', control = control)\nlibrary(rpart.plot)\nrpart.plot(arbol_3, digits = 3)"},{"path":"métodos-basados-en-árboles.html","id":"entrenamiento-y-test","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.8.2 Entrenamiento y test","text":"En esta subsección se dividirá la muestra en submuestras de entrenamiento y test y se evaluará la capacidad predictiva del árbolEl siguiente código obtiene un árbol con la muestra de entrenamiento. Los resultados se muestran en la Figura \\(\\ref{fig:arbol4}\\).\nFigure 7.6: Árbol base 4.\nLa función predict() obtiene las predicciones para la muestra de test y la tabla muestra los resultados de la predicciónLa accuracy de este árbol se obtiene con el siguiente código:Lo ideal sería aplicar un proceso de validación cruzada para ajustar correctamente y encontrar así la mejor combinación de hiperparámetros.","code":"\n# División de los datos en train y test\n# ==============================================================================\nset.seed(123)\ntrain <- sample(1:nrow(ptitanic), size = .7*nrow(ptitanic))\nptitanic_train <- ptitanic[train,]\nptitanic_test  <- ptitanic[-train,]\ncontrol <- list(minbucket = 50, minsplit = 50)\narbol_4 <- rpart(survived ~., data = ptitanic_train, method = 'class', control = control)\nrpart.plot(arbol_4, digits = 3)\nprediccion_1 <- predict(arbol_4, newdata = ptitanic_test, type = 'class')\ntable_mat <- table(ptitanic_test$survived, prediccion_1)\ntable_mat\n#>           prediccion_1\n#>            died survived\n#>   died      210       20\n#>   survived   68       95\naccuracy_Test <- sum(diag(table_mat)) / sum(table_mat)\nprint(paste('Accuracy for test', round(accuracy_Test,3)))\n#> [1] \"Accuracy for test 0.776\""},{"path":"métodos-basados-en-árboles.html","id":"ejemplo-árbol-de-regresión","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.9 Ejemplo Árbol de Regresión","text":"","code":""},{"path":"métodos-basados-en-árboles.html","id":"datos","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.9.1 Datos","text":"El set de datos Boston disponible en el paquete MASS contiene precios de viviendas de la ciudad de Boston, así como información socioeconómica del barrio en el que se encuentran. Se pretende ajustar un modelo de regresión que permita predecir el precio medio de una vivienda (medv) en función de las variables disponibles.","code":"\nif (!require(\"MASS\")) install.packages(\"MASS\")\n#> Loading required package: MASS\nlibrary(MASS)\ndata(\"Boston\", package = \"MASS\")"},{"path":"métodos-basados-en-árboles.html","id":"ajuste-del-modelo-con-la-librería-tree","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.9.2 Ajuste del modelo con la librería ‘tree’","text":"La función tree() del paquete tree permite ajustar árboles de decisión. La elección entre árbol de regresión o árbol de clasificación se hace automáticamente dependiendo de si la variable respuesta es de tipo numeric o factor. Es importante tener en cuenta que solo estos dos tipos de vectores están permitidos, si se pasa uno de tipo character se devuelve un error.continuación, se ajusta un árbol de regresión empleando como variable respuesta medv y como predictores todas las variables disponibles. Como en todo estudio de regresión, solo es importante ajustar el modelo, sino también cuantificar su capacidad para predecir nuevas observaciones. Para poder hacer la posterior evaluación, se dividen los datos en dos grupos, uno de entrenamiento y otro de test.La función tree() crece el árbol hasta que encuentra una condición de stop. Por defecto, estas condiciones son:mincut: Número mínimo de observaciones que debe de tener al menos uno de los nodos hijos para que se produzca la división. Si al dividir el nodo, uno de los subnodos tiene menos de mincut observaciones, entonces el nodo se divide.mincut: Número mínimo de observaciones que debe de tener al menos uno de los nodos hijos para que se produzca la división. Si al dividir el nodo, uno de los subnodos tiene menos de mincut observaciones, entonces el nodo se divide.minsize: Número mínimo de observaciones que debe de tener un nodo para que pueda dividirse.minsize: Número mínimo de observaciones que debe de tener un nodo para que pueda dividirse.","code":"\n# División de los datos en train y test equilibrando por cuartiles\n# ==============================================================================\nlibrary(rsample)\nset.seed(123)\nsplit <- initial_split(Boston, prop = 0.7, strata = medv)\nBoston_train <- training(split)\nBoston_test <- testing(split)"},{"path":"métodos-basados-en-árboles.html","id":"prepodado","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.9.2.1 Prepodado","text":"El siguiente chunk muestra un código para obtener un modelo con pre-podado en función del número mínimo de observacionesLa función summary() muestra que, el árbol entrenado, tiene un total de 7 nodos terminales y que se han empleado como predictores las variables “rm”, “lstat”, “nox” y “crim”. En el contexto de árboles de regresión, el término Residual mean deviance es la suma de cuadrados residuales dividida entre (número de observaciones - número de nodos terminales). Cuanto menor es la ‘deviance’, mejor es el ajuste del árbol las observaciones de entrenamiento.El paquete tree posee una buena capacidad para representar los árboles, pero una vez creado el árbol, se puede representar mediante la combinación de las funciones plot() y text(). La función plot() dibuja la estructura del árbol, las ramas y los nodos. Mediante su argumento type se puede especificar si se quiere que todas las ramas tengan el mismo tamaño (type = “uniform”) o que su longitud sea proporcional la reducción de impureza (heterogeneidad) de los nodos terminales (type = “proportional”). Esta segunda opción permite identificar visualmente el impacto de cada división en el modelo. La función text() añade la descripción de cada nodo interno y el valor de cada nodo terminal.","code":"\n# Creación y entrenamiento del modelo\n# ==============================================================================\nlibrary(tree)\nset.seed(123)\ncontrol <- tree.control(dim(Boston_train)[1], \n                        mincut = 20, \n                        minsize = 50)\narbol_regresion <- tree::tree(formula = medv ~ .,\n                              data    = Boston_train,\n                              control = control)\nsummary(arbol_regresion)\n#> \n#> Regression tree:\n#> tree::tree(formula = medv ~ ., data = Boston_train, control = control)\n#> Variables actually used in tree construction:\n#> [1] \"rm\"    \"lstat\" \"nox\"   \"crim\" \n#> Number of terminal nodes:  7 \n#> Residual mean deviance:  19.77 = 6821 / 345 \n#> Distribution of residuals:\n#>     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n#> -22.1800  -2.1320   0.1656   0.0000   2.0050  27.7700\n# Estructura del árbol con prepodado\n# ==============================================================================\nplot(x = arbol_regresion, type = \"proportional\")\ntext(x = arbol_regresion, splits = TRUE, pretty = 0, cex = 0.8, col = \"firebrick\")"},{"path":"métodos-basados-en-árboles.html","id":"podado-del-árbol-pruning","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.9.3 Podado del árbol (pruning)","text":"Con la finalidad de reducir la varianza del modelo y así mejorar la capacidad predictiva, se somete al árbol un proceso de podado (pruning). El proceso de podado intenta encontrar el árbol más sencillo (menor tamaño) que consigue los mejores resultados de predicción.Para podar un árbol es necesario indicar el grado de penalización por complejidad \\(\\alpha\\). Cuanto mayor sea este valor, más agresivo es el podado y menor el tamaño del árbol resultante. Dado que hay forma de conocer de antemano el valor óptimo de \\(\\alpha\\), se recurre validación cruzada para identificarlo.La función cv.tree() emplea validación cruzada (cross validation) para identificar el valor óptimo de penalización. Por defecto, esta función emplea la ‘deviance’ para guiar el proceso de pruning.El podado se realiza sobre un árbol sin apenas requisitos de prepodado como muestra el siguiente código:El árbol sin restricciones de prepodado tiene 281 nodos y el valor de ‘Residual mean deviance’ ha disminuido mucho, pero obviamente hay sobreajuste.Para resucir el sobreajuste de este modelo se aplica un proceso de validación cruzada como se indicó el la Figura . En este caso se toman 5 folks.El objeto devuelto por cv.tree() contiene:size: el tamaño (número de nodos terminales) de cada árbol.dev: la estimación de cross-validation test error para cada tamaño de árbol.k: El rango de valores de penalización \\(\\alpha\\) evaluados.method: El criterio empleado para seleccionar el mejor árbol.\nFigure 7.7: Partición del espacio con predicciones\nUna vez identificado el valor óptimo de \\(\\alpha\\), con la función prune.tree() se aplica el podado final. En este caso se impone la condición de que obtenga el árbol con el valor óptimo de ramas encontrado el el proceso de validación cruzada con cvtree().","code":"\n# Pruning (const complexity pruning) por validación cruzada\n# ==============================================================================\n\n# El árbol se crece al máximo posible para luego aplicar el pruning\ncontrol <- tree.control(dim(Boston_train)[1], \n                        mincut = 1, \n                        minsize = 2,\n                        mindev  = 0)\narbol_regresion_base<- tree(formula = medv ~ .,\n                            data    = Boston_train,\n                            control = control)\nsummary(arbol_regresion_base)\n#> \n#> Regression tree:\n#> tree(formula = medv ~ ., data = Boston_train, control = control)\n#> Variables actually used in tree construction:\n#>  [1] \"rm\"      \"lstat\"   \"dis\"     \"indus\"   \"black\"  \n#>  [6] \"age\"     \"crim\"    \"ptratio\" \"tax\"     \"rad\"    \n#> [11] \"zn\"      \"nox\"    \n#> Number of terminal nodes:  281 \n#> Residual mean deviance:  0.008568 = 0.6083 / 71 \n#> Distribution of residuals:\n#>    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n#> -0.1000  0.0000  0.0000  0.0000  0.0000  0.1333\n# Búsqueda por validación cruzada\n# ==============================================================================\nset.seed(123)\ncv_arbol <- cv.tree(arbol_regresion_base, K = 5)\n# Tamaño óptimo encontrado\n# ==============================================================================\nsize_optimo <- rev(cv_arbol$size)[which.min(rev(cv_arbol$dev))]\npaste(\"Tamaño óptimo encontrado:\", size_optimo)\n#> [1] \"Tamaño óptimo encontrado: 11\"\nlibrary(gridExtra)\nlibrary(ggplot2)\nresultados_cv <- data.frame(n_nodos  = cv_arbol$size,\n                            deviance = cv_arbol$dev,\n                            alpha    = cv_arbol$k)\n\np1 <- ggplot(data = resultados_cv, aes(x = n_nodos, y = deviance)) +\n      geom_line() +\n      ylim(8000,12000)+\n      xlim(0,20)+\n      geom_point() +\n      geom_vline(xintercept = size_optimo, color = \"red\") +\n      labs(title = \"Error vs tamaño del árbol\") +\n      theme_bw() \n  \np2 <- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) +\n      geom_line() + \n      # ylim(8290,8300)+\n      # xlim(0,1)+\n      geom_point() +\n      labs(title = \"Error vs penalización alpha\") +\n      theme_bw() \ngrid.arrange(p1, p2, ncol = 2, nrow = 1)\n# Estructura del árbol creado final\n# ==============================================================================\narbol_final <- prune.tree(tree = arbol_regresion_base,\n                          best = size_optimo) \nplot(x = arbol_final, type = \"uniform\")\ntext(x = arbol_final, splits = TRUE, pretty = 0, cex = 0.8, col = \"firebrick\")"},{"path":"métodos-basados-en-árboles.html","id":"predicción-y-evaluación-del-modelo","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.9.4 Predicción y evaluación del modelo","text":"Por último, se evalúa la capacidad predictiva de los tres árboles empleando el conjunto de test.En este caso el modelo los modelos con pre-podado y post-podado tiene un comportamiento muy similar en términos de RMSE y ambas opciones mejoran el RMSE del modelo sin podar.","code":"\n# Error de test del modelo base con pre-podado y sin post-podado\n# ==============================================================================\npredicciones <- predict(arbol_regresion, newdata = Boston_test)\ntest_rmse    <- sqrt(mean((predicciones - Boston_test$medv)^2))\npaste(\"Error de test (RMSE) del árbol solo CON pre-podado:\", round(test_rmse,4))\n#> [1] \"Error de test (RMSE) del árbol solo CON pre-podado: 4.6083\"\n# Error de test del modelo base sin pre-podado y sin post-podado\n# ==============================================================================\npredicciones <- predict(arbol_regresion_base, newdata = Boston_test)\ntest_rmse    <- sqrt(mean((predicciones - Boston_test$medv)^2))\npaste(\"Error de test (RMSE) del árbol SIN pre-podado y SIN post-podado:\", round(test_rmse,4))\n#> [1] \"Error de test (RMSE) del árbol SIN pre-podado y SIN post-podado: 4.9402\"\n# Error de test del modelo final\n# ==============================================================================\npredicciones <- predict(arbol_final, newdata = Boston_test)\ntest_rmse    <- sqrt(mean((predicciones - Boston_test$medv)^2))\npaste(\"Error de test (RMSE) del árbol sin pre-podado y CON post-podado:\", round(test_rmse,4))\n#> [1] \"Error de test (RMSE) del árbol sin pre-podado y CON post-podado: 4.6198\""},{"path":"métodos-basados-en-árboles.html","id":"webs","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.10 Webs","text":"https://rpubs.com/fdvm/clase_arboles_uba_dmhttps://rpubs.com/mpfoley73/529130","code":""},{"path":"métodos-basados-en-árboles.html","id":"referencias","chapter":"Capítulo 7 Métodos basados en árboles","heading":"7.11 Referencias","text":"","code":""}]
