<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 7 Métodos basados en árboles | Herramientas… MBA</title>
<meta name="author" content="Fernando López   Manuel Reuz">
<meta name="description" content="7.1 Introducción  Los árboles de decisión son modelos predictivos formados por reglas binarias (si/no) con las que se consigue repartir las observaciones en función de sus atributos y predecir así...">
<meta name="generator" content="bookdown 0.40 with bs4_book()">
<meta property="og:title" content="Capítulo 7 Métodos basados en árboles | Herramientas… MBA">
<meta property="og:type" content="book">
<meta property="og:description" content="7.1 Introducción  Los árboles de decisión son modelos predictivos formados por reglas binarias (si/no) con las que se consigue repartir las observaciones en función de sus atributos y predecir así...">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 7 Métodos basados en árboles | Herramientas… MBA">
<meta name="twitter:description" content="7.1 Introducción  Los árboles de decisión son modelos predictivos formados por reglas binarias (si/no) con las que se consigue repartir las observaciones en función de sus atributos y predecir así...">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.1/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script><script>
        $(function() {
            $("#toc h2").html("En este tema");
        });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Herramientas… MBA</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Presentación</a></li>
<li><a class="" href="introducci%C3%B3n-a-r.html"><span class="header-section-number">2</span> Introducción a R</a></li>
<li><a class="" href="estad%C3%ADstica-con-r.html"><span class="header-section-number">3</span> Estadística con R</a></li>
<li><a class="" href="regresi%C3%B3n-lineal-m%C3%BAltiple.html"><span class="header-section-number">4</span> Regresión Lineal Múltiple</a></li>
<li><a class="" href="an%C3%A1lisis-de-componentes-principales.html"><span class="header-section-number">5</span> Análisis de Componentes Principales</a></li>
<li><a class="" href="an%C3%A1lisis-cluster.html"><span class="header-section-number">6</span> Análisis Cluster</a></li>
<li><a class="active" href="m%C3%A9todos-basados-en-%C3%A1rboles.html"><span class="header-section-number">7</span> Métodos basados en árboles</a></li>
<li><a class="" href="m%C3%A9todos-chingones.html"><span class="header-section-number">8</span> Métodos chingones</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/rstudio/bookdown-demo">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="métodos-basados-en-árboles" class="section level1" number="7">
<h1>
<span class="header-section-number">Capítulo 7</span> Métodos basados en árboles<a class="anchor" aria-label="anchor" href="#m%C3%A9todos-basados-en-%C3%A1rboles"><i class="fas fa-link"></i></a>
</h1>
<div id="introducción-1" class="section level2" number="7.1">
<h2>
<span class="header-section-number">7.1</span> Introducción<a class="anchor" aria-label="anchor" href="#introducci%C3%B3n-1"><i class="fas fa-link"></i></a>
</h2>
<!-- En esta sección, se describen métodos basados en árboles de decisión para regresión y clasificación. Estos métodos tienen como objetivo estratificar o segmentar el espacio predictor en varias regiones simples. La predicciones para cada observación se asignarán dependiendo de la región que le corresponda, asignándole un mismo valor (media, mediana, moda) a todas las observaciones que pertenezcan a la misma zona. -->
<!-- de Para hacer una predicción para una observación dada, normalmente usamos la media o la moda de las observaciones de entrenamiento en la región a la que pertenece. Dado que el conjunto de reglas de división utilizadas para segmentar el espacio predictor se puede resumir en un árbol, este tipo de enfoques se conocen como métodos de árbol de decisión. -->
<p>Los árboles de decisión son modelos predictivos formados por reglas binarias (si/no) con las que se consigue repartir las observaciones en función de sus atributos y predecir así el valor de la variable respuesta.</p>
<blockquote>
<p><strong>Los métodos basados en árboles se han convertido en uno de los referentes dentro del ámbito predictivo debido a los buenos resultados que generan en problemas muy diversos.</strong></p>
</blockquote>
<p>Los árboles de decisión son uno de los métodos más simples y fáciles de interpretar para realizar predicciones en problemas de clasificación y de regresión. Estos métodos fueron desarrollados a partir de los años 70 del siglo XX como una alternativa versátil a los métodos clásicos de la estadística, fuertemente basados en las hipótesis de linealidad y de normalidad, y enseguida se convierten en una técnica básica del aprendizaje automático. Aunque su calidad predictiva es mediocre (especialmente en el caso de regresión), constituyen la base de otros métodos altamente competitivos (métodos de ensamblado en paralelo o en serie) en los que se combinan múltiples árboles para mejorar la predicción, pagando el precio, eso sí, de hacer más difícil la interpretación del modelo resultante.</p>
<p>Los métodos basados en árboles son simples y útiles para la interpretación. Sin embargo, por lo general no son competitivos con los mejores enfoques de aprendizaje supervisado en términos de precisión de predicción. Por lo tanto, también presentamos métodos alternativos que implican la producción de múltiples árboles que luego se combinan para producir una sola predicción de consenso. Veremos que la combinación de una gran cantidad de árboles a menudo puede resultar en mejoras importantes en la precisión de la predicción.</p>
<blockquote>
<p><strong>Mientras que la Regresión tiene una doble función (interpretación/preducción) los árboles deshechan de forma autómatica todas aquellas variables que no son últiples para la predicción</strong></p>
</blockquote>
<!-- https://www.cienciadedatos.net/documentos/33_arboles_decision_random_forest_gradient_boosting_c50 -->
</div>
<div id="origen-de-los-árboles-de-decisión" class="section level2" number="7.2">
<h2>
<span class="header-section-number">7.2</span> Origen de los Árboles de Decisión<a class="anchor" aria-label="anchor" href="#origen-de-los-%C3%A1rboles-de-decisi%C3%B3n"><i class="fas fa-link"></i></a>
</h2>
<p>Es difícil datar el orígen de una metodología. El primer artículo que desarrolla un enfoque de “árbol de decisión” data de 1959 y un investigador británico, William Belson, en un artículo titulado <em>Matching and Prediction on the Principle of Biological Classification</em>,<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;William A Belson, &lt;span&gt;“Matching and Prediction on the Principle of Biological Classification,”&lt;/span&gt; &lt;em&gt;Journal of the Royal Statistical Society: Series C (Applied Statistics)&lt;/em&gt; 8, no. 2 (1959): 65–75.&lt;/p&gt;"><sup>1</sup></a></span> cuyo resumen describe su enfoque como una forma de emparejar muestras de población y desarrollar criterios para hacerlo.</p>
<p>El uso de árboles de decisión tuvo su origen en las ciencias sociales con los trabajos de Sonquist y Morgan el año 1964 y Morgan y Messenger el año 1979, ambos realizados en la Universidad de Michigan. El programa para la “Detección de Interacciones Automáticas”, creada el año 1971 por los investigadores Sonquist, Baker y Morgan, fue uno de los primeros métodos de ajuste de los datos basados en árboles de clasificación. En estadística, el año 1980, Kass introdujo un algoritmo recursivo de clasificación no binario, llamado “Detección de Interacciones Automáticas Chi-cuadrado”. Hacia el año 1984, los investigadores Breiman, Friedman, Olshen y Stone, introdujeron un nuevo algoritmo para la construcción de árboles y los aplicaron a problemas de regresión y clasificación. El método es conocido como “Árboles de clasificación y regresión”. Casi al mismo tiempo el proceso de inducción mediante árboles de decisión comenzó a ser usado por la comunidad de “Aprendizaje automático”.</p>
<p>Los creadores de la metodología del árbol de clasificación con aplicación al aprendizaje automático, también llamada metodología CART, fueron Leo Breiman, Jerome Friedman, Richard Olshen y Charles Stone. Su aplicación en el ámbito de la Estadística se inició en 1984.<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Leo Breiman, &lt;em&gt;Classification and Regression Trees&lt;/em&gt; (Belmont, Calif: Wadsworth International Group, 1984).&lt;/p&gt;"><sup>2</sup></a></span></p>
</div>
<div id="los-árboles-de-decisión-en-las-técnicas-de-machine-learning" class="section level2" number="7.3">
<h2>
<span class="header-section-number">7.3</span> Los Árboles de Decisión en las técnicas de Machine Learning<a class="anchor" aria-label="anchor" href="#los-%C3%A1rboles-de-decisi%C3%B3n-en-las-t%C3%A9cnicas-de-machine-learning"><i class="fas fa-link"></i></a>
</h2>
<p>Los algoritmos de aprendizaje automático pueden clasificarse en dos grupos:</p>
<ul>
<li>Supervisados.</li>
<li>No supervisados.</li>
<li>reforzar el aprendizaje.</li>
</ul>
<p><strong>Aprendizaje supervisado</strong>: El aprendizaje supervisado es un tipo de paradigma de aprendizaje automático en el que un modelo es entrenado utilizando un conjunto de datos etiquetado. En este enfoque, el algoritmo recibe un conjunto de ejemplos de entrada junto con sus correspondientes salidas deseadas (etiquetas) durante la fase de entrenamiento. El objetivo del modelo es aprender una función que mapee las entradas a las salidas de manera que pueda hacer predicciones precisas sobre nuevos datos no etiquetados.</p>
<p>En términos más simples, en el aprendizaje supervisado, el modelo aprende a partir de ejemplos conocidos y luego se le proporcionan nuevos ejemplos para hacer predicciones. La “supervisión” se refiere a la idea de que el proceso de entrenamiento del modelo está supervisado por las etiquetas proporcionadas en el conjunto de datos de entrenamiento.</p>
<p>Este enfoque se utiliza en una variedad de tareas, como clasificación y regresión. En la clasificación, el modelo se entrena para asignar las entradas a categorías predefinidas, mientras que en la regresión, el objetivo es predecir valores numéricos continuos. El aprendizaje supervisado es fundamental en la construcción de muchos sistemas de inteligencia artificial y toma su nombre del hecho de que el modelo es “supervisado” durante el proceso de aprendizaje con la ayuda de las etiquetas.</p>
<p>Un árbol de decisión es un <strong>algoritmo supervisado</strong> de aprendizaje automático porque para que el algoritmo aprenda es necesaria una <strong>variable dependiente</strong> y nuestra meta es obtener una ‘función’ que permita predecir, a partir de <strong>variables independientes</strong>, el valor de la variable objetivo para casos desconocidos.</p>
<p><strong>Aprendizaje no supervisado</strong>: Los algoritmos de aprendizaje no supervisado trabajan de forma muy similar a los supervisados, con la diferencia de que éstos sólo ajustan su modelo predictivo tomando en cuenta los datos de entrada, sin importar los de salida. Es decir, a diferencia del supervisado, los datos de entrada no están clasificados ni etiquetados, y no son necesarias estas características para entrenar el modelo. Dentro de este tipo de algoritmos, el agrupamiento o clustering en inglés, es el más utilizado, ya que particiona los datos en grupos que posean características similares entre sí</p>
<p>La Figura <span class="math inline">\(\ref{fig:algoritmos}\)</span> muestra esquemáticamente los principales algoritmos de Machine Learning.</p>
<div class="float">
<img src="Figures/Algoritmos%20de%20Machine%20Learning.png" width="800" alt="Algoritmos de Machine Learning según su método de aprendizaje"><div class="figcaption">Algoritmos de Machine Learning según su método de aprendizaje</div>
</div>
</div>
<div id="árboles-de-regresión" class="section level2" number="7.4">
<h2>
<span class="header-section-number">7.4</span> Árboles de regresión<a class="anchor" aria-label="anchor" href="#%C3%A1rboles-de-regresi%C3%B3n"><i class="fas fa-link"></i></a>
</h2>
<p>Los árboles de regresión se aplican cuando la variable respuesta es continua. En términos generales, en el entrenamiento de un árbol de regresión, las observaciones se van distribuyendo por bifurcaciones (nodos) generando la estructura del árbol hasta alcanzar un nodo terminal. Cuando se quiere predecir una nueva observación, se recorre el árbol acorde al valor de sus predicciones hasta alcanzar uno de los nodos terminales. La predicción del árbol es la media/mediana/moda de la variable respuesta de las observaciones de entrenamiento que están en ese mismo nodo terminal.</p>
<div id="un-ejemplo-de-juguete" class="section level3" number="7.4.1">
<h3>
<span class="header-section-number">7.4.1</span> Un ejemplo de juguete<a class="anchor" aria-label="anchor" href="#un-ejemplo-de-juguete"><i class="fas fa-link"></i></a>
</h3>
<p>Usaremos el mismo ejemplo que aparece en <span class="citation">James Gareth et al.<a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;&lt;em&gt;An Introduction to Statistical Learning: With Applications in r&lt;/em&gt; (Spinger, 2013).&lt;/p&gt;"><sup>3</sup></a></span> (pag. 304) para ilustrar cómo funcionan los árboles. En este ejemplo, objetivo es predecir una variable cuantitativa, el salario de los jugadores de béisbol, según los Años (cantidad de años que ha jugado en las ligas principales) y la cantidad de Hits/aciertos del año anterior.</p>
<p>El siguiente código muestra un ejemplo básico de obtención de un árbol. Nótese que antes de realizar el análisis se eliminarán las observaciones a las que les faltan valores de Salario. También se transforma logarítmicamente el Salario para que su distribución tenga una forma de campana más típica (Recuerde que el salario se mide en miles de dólares).</p>
<div class="sourceCode" id="cb118"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">if</span><span class="op">(</span><span class="op">!</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR</a></span><span class="op">)</span><span class="op">)</span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"ISLR"</span><span class="op">)</span><span class="op">}</span></span>
<span><span class="co">#&gt; Loading required package: ISLR</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://www.statlearning.com">ISLR</a></span><span class="op">)</span> <span class="co"># provide the collection of data-sets used in the book 'An Introduction to Statistical Learning with Applications in R'</span></span>
<span><span class="kw">if</span><span class="op">(</span><span class="op">!</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="va">tree</span><span class="op">)</span><span class="op">)</span> <span class="op">{</span><span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"tree"</span><span class="op">)</span><span class="op">}</span></span>
<span><span class="co">#&gt; Loading required package: tree</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">tree</span><span class="op">)</span> <span class="co"># Classification and regression trees</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="va">Hitters</span><span class="op">)</span></span>
<span><span class="va">Hitters</span> <span class="op">&lt;-</span> <span class="va">Hitters</span><span class="op">[</span><span class="op">!</span><span class="fu"><a href="https://rdrr.io/r/base/NA.html">is.na</a></span><span class="op">(</span><span class="va">Hitters</span><span class="op">$</span><span class="va">Salary</span><span class="op">)</span>,<span class="op">]</span></span>
<span><span class="va">formula</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/Log.html">log</a></span><span class="op">(</span><span class="va">Salary</span><span class="op">)</span> <span class="op">~</span> <span class="va">Years</span> <span class="op">+</span> <span class="va">Hits</span> </span>
<span><span class="va">myfirsttree</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tree/man/tree.html">tree</a></span><span class="op">(</span><span class="va">formula</span>, data <span class="op">=</span> <span class="va">Hitters</span>, control<span class="op">=</span><span class="fu"><a href="https://rdrr.io/pkg/tree/man/tree.control.html">tree.control</a></span><span class="op">(</span><span class="fl">300</span>, mincut <span class="op">=</span> <span class="fl">60</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">myfirsttree</span>, type <span class="op">=</span> <span class="st">"proportional"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">myfirsttree</span>, splits <span class="op">=</span> <span class="cn">TRUE</span>, pretty <span class="op">=</span> <span class="fl">0</span>, cex <span class="op">=</span> <span class="fl">1</span>, col <span class="op">=</span> <span class="st">"firebrick"</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:myfirsttree"></span>
<img src="06-Sesion6-Arboles-de-Decision_files/figure-html/myfirsttree-1.png" alt="\label{fig:myfirsttree}Árbol de regresión" width="768"><p class="caption">
Figure 7.1: Árbol de regresión
</p>
</div>
<p>La Figura <span class="math inline">\(\ref{fig:myfirsttree}\)</span> muestra un árbol de regresión ajustado a estos datos. Nótese que tiene forma de árbol (ver Sección <span class="math inline">\(\ref{sec:terminologia}\)</span>). Consiste en una serie de reglas de división <strong>binaria</strong>, comenzando en la parte superior del árbol. La división superior asigna observaciones que tienen menos de 4,5 años (Years &lt; 4.5) a la rama izquierda. El salario previsto para estos jugadores viene dado por el valor de respuesta media para los jugadores en el conjunto de datos con (Años&lt;4.5). Para tales jugadores, el salario logarítmico medio es 5,107, por lo que hacemos una predicción de 165174$, para estos jugadores. Los jugadores con (<span class="math inline">\(Years \geq 4.5\)</span>) se asignan a la rama derecha, y luego ese grupo se subdivide aún más por ‘Hits’. En general, el árbol estratifica o segmenta a los jugadores en tres regiones del espacio predictor: jugadores que han jugado durante cuatro años y medio o menos, jugadores que han jugado durante cuatro años y medio o más y que hicieron menos de 118 aciertos el año pasado y jugadores que han jugado durante cuatro años y medio o más y que hizo al menos 118 hits el año pasado.</p>
<p>Estas tres regiones se pueden escribir como:</p>
<ul>
<li><p><span class="math inline">\(R_1\)</span> = { X | Años &lt; 4.5 },</p></li>
<li><p><span class="math inline">\(R_2\)</span>={X | Años&gt;=4.5, Hits &lt; 117.5 } y</p></li>
<li><p><span class="math inline">\(R_3\)</span> = {X | años &gt;= 4.5, Hits &gt;= 117.5 }.</p></li>
</ul>
<p>La Figura <span class="math inline">\(\ref{fig:particion0}\)</span> ilustra las regiones en función de Años y Hits/Aciertos. Los salarios pronosticados para estos tres grupos son 1000 × exp(5,107) = 165 174<span class="math inline">\(, 1000 × exp(5,999) = 402 834\)</span> y 1,000 × exp(6.740)=845,346$ respectivamente.</p>
<div class="sourceCode" id="cb119"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span><span class="va">Hitters</span><span class="op">$</span><span class="va">Years</span>, <span class="va">Hitters</span><span class="op">$</span><span class="va">Hits</span>, col<span class="op">=</span><span class="st">"orange"</span>, pch<span class="op">=</span><span class="fl">16</span> ,xlab<span class="op">=</span><span class="st">"Years"</span>, ylab<span class="op">=</span><span class="st">"Hits"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/tree/man/partition.tree.html">partition.tree</a></span><span class="op">(</span><span class="va">myfirsttree</span>, cex <span class="op">=</span> <span class="fl">2</span>, add <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-1"></span>
<img src="06-Sesion6-Arboles-de-Decision_files/figure-html/unnamed-chunk-1-1.png" alt="\label{fig:particion0}Partición del espacio con predicciones" width="480"><p class="caption">
Figure 7.2: Partición del espacio con predicciones
</p>
</div>
<p>Podríamos interpretar el árbol de regresión que se muestra en la Figura <span class="math inline">\(\ref{fig:myfirsttree}\)</span> de la siguiente manera: los años son el factor más importante para determinar el salario, y los jugadores con menos experiencia ganan salarios más bajos que los jugadores más experimentados. Dado que un jugador tiene menos experiencia, la cantidad de golpes que hizo en el año anterior parece jugar un papel pequeño en su salario. Pero entre los jugadores que han estado en las ligas mayores durante cinco años o más, la cantidad de Hits/aciertos hechos el año anterior sí afecta el salario, y los jugadores que hicieron más Hits/aciertos el año pasado tienden a tener salarios más altos. El árbol de regresión que se muestra en la Figura <span class="math inline">\(\ref{fig:myfirsttree}\)</span> es probablemente una simplificación excesiva de la verdadera relación entre Aciertos, Años y Salario. Sin embargo, tiene ventajas sobre otros tipos de modelos de regresión.</p>
</div>
<div id="sec:terminologia" class="section level3" number="7.4.2">
<h3>
<span class="header-section-number">7.4.2</span> Terminología<a class="anchor" aria-label="anchor" href="#sec:terminologia"><i class="fas fa-link"></i></a>
</h3>
<p>Un árbol de decisión en Machine Learning es una estructura de árbol similar a un diagrama de flujo donde un nodo interno representa una característica (o atributo), la rama representa una regla de decisión y cada nodo hoja representa el resultado.</p>
<p>El nodo superior en un árbol de decisión en Machine Learning se conoce como el nodo raíz. Aprende a particionar en función del valor del atributo. Divide el árbol de una manera recursiva llamada partición recursiva.</p>
<p>Cada nodo en el árbol actúa como un caso de prueba para algún atributo, y cada borde que desciende de ese nodo corresponde a una de las posibles respuestas al caso de prueba. Este proceso es recursivo y se repite para cada subárbol enraizado en los nuevos nodos.</p>
<p>La Figura <span class="math inline">\(\ref{fig:arbol}\)</span> muestra gráficamente la estructura de árbol.</p>
<div class="float">
<img src="Figures/Estructura-de-un-arbol-de-decision-2.png" width="500" height="500" alt="Ramas y nodos de un árbol de decisión"><div class="figcaption">Ramas y nodos de un árbol de decisión</div>
</div>
<ul>
<li><p><strong>Nodo raíz (nodo de decisión superior ):</strong> Representa a toda la población o muestra y esto se divide en dos o más conjuntos homogéneos.</p></li>
<li><p><strong>División:</strong> Es un proceso de división de un nodo en dos o más subnodos.</p></li>
<li><p><strong>Nodo de decisión:</strong> Cuando un subnodo se divide en subnodos adicionales, se llama nodo de decisión.</p></li>
<li><p><strong>Nodo de hoja / terminal:</strong> Los nodos sin hijos (sin división adicional) se llaman Hoja o nodo terminal.</p></li>
<li><p><strong>Poda:</strong> Cuando reducimos el tamaño de los árboles de decisión eliminando nodos (opuesto a la división), el proceso se llama poda.</p></li>
<li><p><strong>Rama / Subárbol:</strong> Una subsección del árbol de decisión se denomina rama o subárbol.</p></li>
<li><p><strong>Nodo padre e hijo:</strong> Un nodo, que se divide en subnodos se denomina nodo principal de subnodos, mientras que los subnodos son hijos de un nodo principal.</p></li>
</ul>
</div>
<div id="como-se-crea-un-árbol" class="section level3" number="7.4.3">
<h3>
<span class="header-section-number">7.4.3</span> Como se crea un árbol<a class="anchor" aria-label="anchor" href="#como-se-crea-un-%C3%A1rbol"><i class="fas fa-link"></i></a>
</h3>
<p>En esta sección se presenta el proceso de construcción de un árbol de decisión. Básicamente, los árboles se construyen en dos dos pasos:</p>
<ul>
<li><p><strong>Paso 1:</strong> Se divide el espacio de predicciones, es decir, el conjunto de valores posibles para <span class="math inline">\(X_1, X_2,..., X_p\)</span>, en J regiones distintas y no solapadas, <span class="math inline">\(R_1, R_2,..., R_J\)</span>.</p></li>
<li><p><strong>Paso 2:</strong> Para cada observación que cae en la región <span class="math inline">\(R_j\)</span>, se hace <strong>la misma predicción</strong>, que simplemente es la media (o la mediana o la moda) de los valores de respuesta para las observaciones de entrenamiento en <span class="math inline">\(R_j\)</span>.</p></li>
</ul>
<p>¿Cómo se construyen las regiones <span class="math inline">\(R_1, R_2,..., R_J\)</span>? En teoría, podrían tener cualquier forma. Sin embargo, por simplicidad se divide el espacio de predicciones en rectángulos o cajas (Figura <span class="math inline">\(\ref{fig:cajas}\)</span>). Esta división también ayuda en la interpretación del modelo predictivo resultante. Hay también posibilidad de realizar divisiones oblicuas<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;D. C. Wickramarachchi et al., &lt;span&gt;“&lt;span&gt;HHCART&lt;/span&gt;: An Oblique Decision Tree,”&lt;/span&gt; &lt;em&gt;Computational Statistics &lt;span&gt;&amp;amp;&lt;/span&gt; Data Analysis&lt;/em&gt; 96 (April 2016): 12–23, &lt;a href="https://doi.org/10.1016/j.csda.2015.11.006"&gt;https://doi.org/10.1016/j.csda.2015.11.006&lt;/a&gt;.&lt;/p&gt;'><sup>4</sup></a></span> o flexibles.<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Antonio Paez et al., &lt;span&gt;“Inducing Non-Orthogonal and Non-Linear Decision Boundaries in Decision Trees via Interactive Basis Functions,”&lt;/span&gt; &lt;em&gt;Expert Systems with Applications&lt;/em&gt; 122 (May 2019): 183–206, &lt;a href="https://doi.org/10.1016/j.eswa.2018.12.041"&gt;https://doi.org/10.1016/j.eswa.2018.12.041&lt;/a&gt;.&lt;/p&gt;'><sup>5</sup></a></span></p>
<div class="float">
<img src="Figures/Cajas%20Arboles%20Decision.png" width="800" alt="División binaria recursiva"><div class="figcaption">División binaria recursiva</div>
</div>
<p>En el caso de los árboles de regresión, el criterio empleado con más frecuencia para identificar las divisiones es el observar la suma de los residuos al cuadrado (Residual Sum of Squares, <span class="math inline">\(RSS\)</span>) que es una medida de la discrepancia entre los datos reales y los predichos por el modelo. El objetivo es encontrar las J regiones <span class="math inline">\((R_1,…, R_j)\)</span> que minimizan el <span class="math inline">\(RSS\)</span> total. Un <span class="math inline">\(RSS\)</span> bajo indica un buen ajuste del modelo a los datos. Por tanto, el objetivo es encontrar cajas <span class="math inline">\(R_1, R_2,..., R_J\)</span>? que minimicen el <span class="math inline">\(RSS\)</span>, dado por:</p>
<p><span class="math display">\[
RSS=\sum_{j=1}^J{\sum_{i \in R_j} (y_i-{\hat y}_{R_j})^2}
\]</span>
donde <span class="math inline">\({\hat y}_{R_j}\)</span> es el valor predicho (para las observaciones de entrenamiento) dentro de la <span class="math inline">\(R_j\)</span> caja. Desafortunadamente, es computacionalmente inviable considerar cada posible partición del espacio de características en cajas J. Por esta razón, se adopta un enfoque de <em>arriba-hacia-abajo</em> (top-down) que se conoce como división binaria recursiva. El enfoque es de arriba-hacia-abajo porque comienza en la parte superior del árbol (en cuyo punto todas las observaciones pertenecen a una sola región) y luego divide sucesivamente el espacio predictor; cada división se indica a través de dos nuevas ramas más abajo en el árbol.</p>
<p>Para realizar la división binaria recursiva, primero seleccionamos el predictor <span class="math inline">\(X_j\)</span> y el punto de corte <span class="math inline">\(s\)</span> tal que dividiendo el espacio del predictor en las regiones <span class="math inline">\(\{X|X_j &lt; s\}\)</span> y <span class="math inline">\(\{X|X_j \geq s\}\)</span> conduce a la mayor reducción posible en <span class="math inline">\(RSS\)</span>. (La notación <span class="math inline">\(\{X|X_j &lt; s\}\)</span> significa la región del predictor espacio en el que <span class="math inline">\(X_j\)</span> toma un valor menor que <span class="math inline">\(s\)</span>). Es decir, consideramos todos los predictores <span class="math inline">\(X_1,..., X_p\)</span> y todos los valores posibles del punto de corte s para cada uno de los predictores, y luego elegimos el predictor y el punto de corte tales que el árbol resultante tiene el <span class="math inline">\(RSS\)</span> más bajo. Con mayor detalle, para cualquier j y s, definimos el par de semiplanos</p>
<p><span class="math display">\[
R_1(j,s)=\{X|X_j &lt; s\} \ \ y \ \ R_2(j,s)=\{X|X_j \geq s\}
\]</span></p>
<p>y se buscan los valores de j y s que minimizan la ecuación</p>
<p><span class="math display">\[
RSS={\sum_{i \in R_1} (y_i-{\hat y}_{R_1})^2} + {\sum_{i \in R_2} (y_i-{\hat y}_{R_2})^2}
\]</span></p>
</div>
<div id="evitar-el-overfitting" class="section level3" number="7.4.4">
<h3>
<span class="header-section-number">7.4.4</span> Evitar el overfitting<a class="anchor" aria-label="anchor" href="#evitar-el-overfitting"><i class="fas fa-link"></i></a>
</h3>
<p>El proceso de construcción de árboles tiende a reducir rápidamente el error <span class="math inline">\(SSR\)</span>, de tal forma que el modelo se ajusta muy bien a las observaciones empleadas como entrenamiento. Como consecuencia, se genera un sobreajuste (<strong><em>overfitting</em></strong>) que <strong>reduce su capacidad predictiva al aplicarlo a nuevos datos</strong>. La razón de este comportamiento radica en la facilidad con la que los árboles se ramifican adquiriendo estructuras complejas. De hecho, si no se limitan las divisiones, todo árbol termina ajustándose perfectamente a las observaciones de entrenamiento creando un nodo terminal por observación. Existen dos estrategias para prevenir el problema de overfitting de los árboles:
- limitar el tamaño del árbol (parada temprana o <em>early stopping</em>) y
- el proceso de podado ( <em>pruning</em>).</p>
<p>La Figura <span class="math inline">\(\ref{fig:sobreajuste}\)</span> muestra el comportamiento del overffiting.</p>
<div class="float">
<img src="Figures/pruning.png" width="600" alt="Sobre-ajuste"><div class="figcaption">Sobre-ajuste</div>
</div>
<div id="controlar-el-tamaño-del-árbol-parada-temprana" class="section level4" number="7.4.4.1">
<h4>
<span class="header-section-number">7.4.4.1</span> Controlar el tamaño del árbol (parada temprana)<a class="anchor" aria-label="anchor" href="#controlar-el-tama%C3%B1o-del-%C3%A1rbol-parada-temprana"><i class="fas fa-link"></i></a>
</h4>
<p>El tamaño final que adquiere un árbol puede controlarse mediante reglas de parada que detengan la división de los nodos dependiendo de si se cumplen o no determinadas condiciones. El nombre de estas condiciones puede variar dependiendo del software o librería empleada, pero suelen estar presentes en todos ellos.</p>
<blockquote>
<p><strong>Observaciones mínimas para división:</strong> define el número mínimo de observaciones que debe tener un nodo para poder ser dividido. Cuanto mayor el valor, menos flexible es el modelo.</p>
</blockquote>
<blockquote>
<p><strong>Observaciones mínimas de nodo terminal:</strong> define el número mínimo de observaciones que deben tener los nodos terminales. Su efecto es muy similar al de observaciones mínimas para división.</p>
</blockquote>
<blockquote>
<p><strong>Profundidad máxima del árbol:</strong> define la profundidad máxima del árbol, entendiendo por profundidad máxima el número de divisiones de la rama más larga (en sentido descendente) del árbol.</p>
</blockquote>
<blockquote>
<p><strong>Número máximo de nodos terminales:</strong> define el número máximo de nodos terminales que puede tener el árbol. Una vez alcanzado el límite, se detienen las divisiones. Su efecto es similar al de controlar la profundidad máxima del árbol.</p>
</blockquote>
<blockquote>
<p><strong>Reducción mínima de error:</strong> define la reducción mínima de error que tiene que conseguir una división para que se lleve a cabo.</p>
</blockquote>
<p>A todos estos parámetros se les conoce como <strong>hiperparámetros</strong> porque no se aprenden durante el entrenamiento del modelo. Su valor tiene que ser especificado por el usuario en base a su conocimiento del problema y mediante el uso de validación cruzada.</p>
</div>
<div id="post-pruning" class="section level4" number="7.4.4.2">
<h4>
<span class="header-section-number">7.4.4.2</span> Post-pruning<a class="anchor" aria-label="anchor" href="#post-pruning"><i class="fas fa-link"></i></a>
</h4>
<p>El proceso descrito anteriormente puede producir buenas predicciones en el conjunto de entrenamiento, pero es probable que se produzca un sobreajuste los datos, lo que conducirá a un rendimiento deficiente del conjunto de prueba. Esto se debe a que el árbol resultante puede ser demasiado complejo. Un árbol más pequeño con menos divisiones (menos regiones <span class="math inline">\(R_1,... , R_J\)</span>) podría generar una aceptable predicción y una mejor interpretación a costa de un pequeño sesgo. Una posible alternativa al proceso descrito anteriormente es construir el árbol solo mientras la disminución en el <span class="math inline">\(RSS\)</span> debido a cada división exceda algún umbral (<em>threshold</em>). Esta estrategia dará como resultado árboles más pequeños, <strong>pero es demasiado miope, ya que una división aparentemente sin valor en un nodo al principio del árbol podría ser seguida por una muy buena división en otro nodo posterior que conduzca a una gran reducción en <span class="math inline">\(RSS\)</span>.</strong></p>
<p>Una estrategia mejor sería hacer crecer un árbol <span class="math inline">\(T_0\)</span> muy grande y luego podarlo para obtener un subárbol. La cuestión es: <strong>¿Cómo determinamos la mejor manera de podar el árbol?</strong> Intuitivamente, nuestro objetivo es seleccionar un subárbol que conduzca a la tasa de error más baja.</p>
<p>Dado un subárbol, se podría por ejemplo estimar su error utilizando un proceso de validación cruzada. Sin embargo, aplicar este procedimiento para cada posible subárbol sería demasiado engorroso, ya que es posible identificar una cantidad extremadamente grande de subárboles. En su lugar, es necesario encontrar una forma de seleccionar un pequeño conjunto de subárboles para su consideración/evaluación. La reducción de la complejidad de los costos, también conocida como reducción del eslabón más débil, brinda una manera de hacer precisamente esto. En lugar de considerar todos los subárboles posibles, consideramos una secuencia de árboles indexados por un parámetro de ajuste no negativo <span class="math inline">\(\alpha\)</span>.</p>
<p>En este caso, se busca el sub-árbol T que minimiza la ecuación (<strong><em>Cost complexity pruning</em></strong>)</p>
<p><span class="math display">\[
\sum_{j=1}^{|T|}{\sum_{i \in R_j} (y_i-{\hat y}_{R_j})^2} + \alpha|T|
\]</span>
es lo más pequeño posible.</p>
<p>Aquí |T| indica el número de nodos terminales del árbol T.</p>
<p>El primer término de la ecuación se corresponde con el sumatorio total de los residuos cuadrados RSS. Por definición, cuantos más nodos terminales tenga el modelo menor será esta parte de la ecuación. El segundo término es la restricción, que penaliza al modelo en función del número de nodos terminales (a mayor número, mayor penalización). El grado de penalización se determina mediante el parámetro <span class="math inline">\(\alpha\)</span> que es la <strong>medida de costo-complejidad</strong> (<strong>cost complexity</strong>). Cuando <span class="math inline">\(\alpha\)</span>=0, la penalización es nula y el árbol resultante es equivalente al árbol original. A medida que se incrementa <span class="math inline">\(\alpha\)</span> la penalización es mayor y, como consecuencia, los árboles resultantes son de menor tamaño. El valor óptimo de <span class="math inline">\(\alpha\)</span> puede identificarse mediante cross validation.</p>
<p>El parámetro <span class="math inline">\(\alpha\)</span> controla un equilibrio entre la complejidad del subárbol y su ajuste a los datos de entrenamiento. Cuando <span class="math inline">\(\alpha\)</span>= 0, entonces el subárbol T simplemente será igual a <span class="math inline">\(T_0\)</span>, porque entonces solo mide el error de entrenamiento. Sin embargo, a medida que aumenta <span class="math inline">\(\alpha\)</span>, hay que pagar un precio por tener un árbol con muchos nodos terminales, por lo que la cantidad tenderá a minimizarse para un subárbol más pequeño.</p>
<p>Resulta que a medida que se aumentea el valor de <span class="math inline">\(\alpha\)</span> desde cero, las ramas se eliminan del árbol de forma anidada y predecible, por lo que es fácil obtener la secuencia completa de subárboles en función de <span class="math inline">\(\alpha\)</span>.</p>
<p>Es posible seleccionar un valor de <span class="math inline">\(\alpha\)</span> usando un conjunto de validación o usando validación cruzada.</p>
</div>
</div>
<div id="validación-cruzada" class="section level3" number="7.4.5">
<h3>
<span class="header-section-number">7.4.5</span> Validación cruzada<a class="anchor" aria-label="anchor" href="#validaci%C3%B3n-cruzada"><i class="fas fa-link"></i></a>
</h3>
<p>La validación cruzada o cross-validation es una técnica utilizada para evaluar los resultados de un análisis estadístico cuando el conjunto de datos se ha segmentado en una muestra de entrenamiento y otra de prueba, la validación cruzada (Figura ) comprueba si los resultados del análisis son independientes de la partición. Aunque la validación cruzada es una técnica diseñada para modelos de regresión y predicción, su uso se ha extendido a muchos otros ejercicios de Machine Learning.</p>
<div class="float">
<img src="Figures/Esquema_validacion_cruzada.jpg" width="400" alt="Esquema de validación cruzada https://commons.wikimedia.org/w/index.php?curid=17617674"><div class="figcaption">Esquema de validación cruzada <a href="https://commons.wikimedia.org/w/index.php?curid=17617674" class="uri">https://commons.wikimedia.org/w/index.php?curid=17617674</a>
</div>
</div>
<p>La manera más sencilla de realizar la validación cruzada es una vez segmentado el conjunto de datos en la muestra de entrenamiento y test, consiste en resolver el modelo con los datos de entrenamiento, y probar el modelo estimado en la muestra de test, la simple comparación del resultado obtenido en dicha muestra con las observaciones reales nos permite validar el modelo en términos de error (proceso hold-out). Una aplicación alternativa consiste en repetir el proceso anterior, seleccionando aleatoriamente distintos conjuntos de datos de entrenamiento, y calcular los estadísticos de validación a partir de la media de los valores en cada una de las repeticiones. A este método se le denomina Validación cruzada aleatoria (Figura <span class="math inline">\(\ref{fig:esquema}\)</span>). Los inconvenientes es que hay algunas muestras que quedan sin evaluar y otras que se evalúan más de una vez, es decir, los subconjuntos de prueba y entrenamiento se pueden solapar.</p>
<div class="float">
<img src="Figures/Random_cross_validation.jpg" width="400" alt="Esquema de validación cruzada https://commons.wikimedia.org/w/index.php?curid=17616794"><div class="figcaption">Esquema de validación cruzada <a href="https://commons.wikimedia.org/w/index.php?curid=17616794" class="uri">https://commons.wikimedia.org/w/index.php?curid=17616794</a>
</div>
</div>
<!-- ver -->
<!-- https://bookdown.org/content/2274/metodos-de-clasificacion.html#validacion-cruzada -->
</div>
<div id="algoritmo-para-crear-un-árbol-de-regresión-con-pruning" class="section level3" number="7.4.6">
<h3>
<span class="header-section-number">7.4.6</span> Algoritmo para crear un árbol de regresión con pruning<a class="anchor" aria-label="anchor" href="#algoritmo-para-crear-un-%C3%A1rbol-de-regresi%C3%B3n-con-pruning"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li><p><em>1.</em> Se emplea <em>recursive binary splitting</em> para crear un árbol grande y complejo (<span class="math inline">\(T_0\)</span>) empleando los datos de training y reduciendo al máximo posible las condiciones de parada. Normalmente se emplea como única condición de parada el número mínimo de observaciones por nodo terminal.</p></li>
<li><p><em>2.</em> Se aplica el cost complexity pruning al árbol <span class="math inline">\(T_0\)</span> para obtener el mejor sub-árbol en función de <span class="math inline">\(\alpha\)</span>. Es decir, se obtiene el mejor sub-árbol para un rango de valores de <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p><em>3.</em> Identificación del valor óptimo de <span class="math inline">\(\alpha\)</span> mediante k-cross-validation. Se divide la muestra de entrenamiento en K grupos. Para <span class="math inline">\(k=1,...,K\)</span>:</p></li>
<li><p><em>4.</em> Repetir pasos 1 y 2 empleando todas las observaciones excepto las del grupo i-ésimo.</p></li>
<li><p><em>5.</em> Evaluar el <span class="math inline">\(SSR\)</span> para el rango de valores de <span class="math inline">\(\alpha\)</span> empleando el grupo i-ésimo.</p></li>
<li><p><em>6.</em> Obtener el promedio de los K <span class="math inline">\(SSR\)</span> calculados para cada valor <span class="math inline">\(\alpha\)</span>.</p></li>
</ul>
<p>Seleccionar el sub-árbol del paso 2 que se corresponde con el valor <span class="math inline">\(\alpha\)</span> que ha conseguido el menor cross-validation mean squared error en el paso 3.</p>
<p>En el caso de los árboles de clasificación (ver más adelante), en lugar de emplear la suma de residuos cuadrados <span class="math inline">\(SSR\)</span> como criterio de selección, se emplea una medida alternativa (ver árboles de clasificación).</p>
</div>
</div>
<div id="árboles-de-clasificación" class="section level2" number="7.5">
<h2>
<span class="header-section-number">7.5</span> Árboles de clasificación<a class="anchor" aria-label="anchor" href="#%C3%A1rboles-de-clasificaci%C3%B3n"><i class="fas fa-link"></i></a>
</h2>
<p>Los árboles de clasificación son el subtipo de árboles de decisión que se aplica cuando la variable respuesta es categórica.</p>
<div id="cómo-se-crea-un-árbol-de-clasificación" class="section level3" number="7.5.1">
<h3>
<span class="header-section-number">7.5.1</span> Cómo se crea un árbol de clasificación<a class="anchor" aria-label="anchor" href="#c%C3%B3mo-se-crea-un-%C3%A1rbol-de-clasificaci%C3%B3n"><i class="fas fa-link"></i></a>
</h3>
<p>Para construir un árbol de clasificación se emplea el mismo método descrito en los árboles de regresión. Sin embargo, como la variable respuesta en cualitativa, no es posible emplear el <span class="math inline">\(RSS\)</span> como criterio de selección de las divisiones óptimas. Existen varias alternativas, todas ellas con el objetivo de encontrar nodos lo más puros/homogéneos posible. Las más empleadas son:</p>
<ul>
<li><p><strong>Classification Error Rate:</strong> Se define como la proporción de observaciones que no pertenecen a la clase más común en el nodo.
<span class="math display">\[
E_m= 1- max (\hat p_{mk})
\]</span>
donde <span class="math inline">\(\hat p_{mk}\)</span> representa la proporción de observaciones del nodo m que pertenecen a la clase k. A pesar de la sencillez de esta medida, no es suficientemente sensible para crear buenos árboles, por lo que, en la práctica, suelen emplearse otras medidas.</p></li>
<li><p><strong>Gini Index:</strong> Es una medida de la varianza total en el conjunto de las K clases del nodo m. Se considera una medida de pureza del nodo.
<span class="math display">\[
G = \sum_{k=1}^K \hat p_{mk}(1-\hat p_{mk})
\]</span>
Cuando <span class="math inline">\(\hat p_{mk}\)</span> es cercano a 0 o a 1 (el nodo contiene mayoritariamente observaciones de una clase), el término <span class="math inline">\(\hat p_{mk}(1-\hat p_{mk})\)</span> es muy pequeño. Como consecuencia, cuanto mayor sea la pureza del nodo, menor el valor del índice Gini G.</p></li>
<li><p><strong>Cross Entropy:</strong> La entropía es otra forma de cuantificar el desorden de un sistema. En el caso de los nodos, el desorden se corresponde con la impureza. Si un nodo es puro, contiene únicamente observaciones de una clase, su entropía es cero. Por el contrario, si la frecuencia de cada clase es la misma, el valor de la entropía alcanza el valor máximo de 1.
<span class="math display">\[
D = \sum_{k=1}^K \hat p_{mk}log(\hat p_{mk})
\]</span></p></li>
<li><p><strong>Chi-Square:</strong> Esta aproximación consiste en identificar si existe una diferencia significativa entre los nodos hijos y el nodo parental, es decir, si hay evidencias de que la división consigue una mejora. Para ello, se aplica un test estadístico chi-square goodness of fit empleando como distribución esperada
<span class="math inline">\(H_0\)</span> la frecuencia de cada clase en el nodo parental. Cuanto mayor el estadístico <span class="math inline">\(\chi^2\)</span>, mayor la evidencia estadística de que existe una diferencia.
<span class="math display">\[
\chi^2=\sum_k{(observado_k-esperado_k) \over esperado_k}
\]</span>
Los árboles generados con este criterio de división reciben el nombre de CHAID (Chi-square Automatic Interaction Detector).</p></li>
</ul>
<p>Independientemente de la medida empleada como criterio de selección de las divisiones, el proceso siempre es el mismo.</p>
</div>
<div id="predicción-del-árbol" class="section level3" number="7.5.2">
<h3>
<span class="header-section-number">7.5.2</span> Predicción del árbol<a class="anchor" aria-label="anchor" href="#predicci%C3%B3n-del-%C3%A1rbol"><i class="fas fa-link"></i></a>
</h3>
<p>Tras la creación de un árbol, las observaciones de entrenamiento quedan agrupadas en los nodos terminales. Para predecir una nueva observación, se recorre el árbol en función del valor de sus predictores hasta llegar a uno de los nodos terminales. En el caso de clasificación, suele emplearse la moda de la variable respuesta como valor de predicción, es decir, la clase más frecuente del nodo. Además, puede acompañarse con el porcentaje de cada clase en el nodo terminal, lo que aporta información sobre la confianza de la predicción.</p>
</div>
</div>
<div id="ventajas-y-desventajas-de-los-árboles-de-decisión" class="section level2" number="7.6">
<h2>
<span class="header-section-number">7.6</span> Ventajas y desventajas de los Árboles de Decisión<a class="anchor" aria-label="anchor" href="#ventajas-y-desventajas-de-los-%C3%A1rboles-de-decisi%C3%B3n"><i class="fas fa-link"></i></a>
</h2>
<div id="ventajas-algunas" class="section level3" number="7.6.1">
<h3>
<span class="header-section-number">7.6.1</span> Ventajas (algunas)<a class="anchor" aria-label="anchor" href="#ventajas-algunas"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Fácil de entender</li>
<li>Util en exploración de datos:identificar importancia de variables a partir de cientos de variables.</li>
<li>Menos limpieza de datos: outliers y valores faltantes no influencian el modelo (A un cierto grado)</li>
<li>El tipo de datos no es una restricción</li>
<li>Es un método no paramétrico</li>
</ul>
</div>
<div id="desventajas-algunas" class="section level3" number="7.6.2">
<h3>
<span class="header-section-number">7.6.2</span> Desventajas (algunas)<a class="anchor" aria-label="anchor" href="#desventajas-algunas"><i class="fas fa-link"></i></a>
</h3>
<ul>
<li>Sobreajuste</li>
<li>Pérdida de información al categorizar variables continuas</li>
<li>Inestabilidad: un pequeño cambio en los datos puede modificar ampliamente la estructura del árbol. Por lo tanto la interpretación no es tan directa como parece.</li>
<li>Un árbol de decisión puede llegar a ser demasiado complejo con facilidad, perdiendo su utilidad.</li>
</ul>
</div>
</div>
<div id="paquetes-de-r" class="section level2" number="7.7">
<h2>
<span class="header-section-number">7.7</span> Paquetes de R<a class="anchor" aria-label="anchor" href="#paquetes-de-r"><i class="fas fa-link"></i></a>
</h2>
<p>Algunas librerías que permiten crear y representar árboles de regresión y clasificación son: tree,<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Brian Ripley, &lt;em&gt;Tree: Classification and Regression Trees&lt;/em&gt;, 2021, &lt;a href="https://CRAN.R-project.org/package=tree"&gt;https://CRAN.R-project.org/package=tree&lt;/a&gt;.&lt;/p&gt;'><sup>6</sup></a></span> rpart<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Terry Therneau and Beth Atkinson, &lt;em&gt;Rpart: Recursive Partitioning and Regression Trees&lt;/em&gt;, 2019, &lt;a href="https://CRAN.R-project.org/package=rpart"&gt;https://CRAN.R-project.org/package=rpart&lt;/a&gt;.&lt;/p&gt;'><sup>7</sup></a></span> o party:<span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content="&lt;p&gt;Torsten Hothorn et al., &lt;span&gt;“Party: A Laboratory for Recursive Partytioning,”&lt;/span&gt; 2010.&lt;/p&gt;"><sup>8</sup></a></span></p>
<p><strong>rpart</strong>: Este es el primer paquete de R para árboles de clasificación y árboles de regresión. Tiene funciones de poda y gráficos muy generales. Se complementa con otro paquete <strong>rpart.plot</strong><span class="citation"><a class="footnote-ref" tabindex="0" data-toggle="popover" data-content='&lt;p&gt;Stephen Milborrow, &lt;em&gt;Rpart.plot: Plot ’Rpart’ Models: An Enhanced Version of ’Plot.rpart’&lt;/em&gt;, 2021, &lt;a href="https://CRAN.R-project.org/package=rpart.plot"&gt;https://CRAN.R-project.org/package=rpart.plot&lt;/a&gt;.&lt;/p&gt;'><sup>9</sup></a></span></p>
<p><strong>party</strong>: El núcleo del paquete es la función ctree(), una implementación de árboles de inferencia condicional que integran modelos de regresión estructurados en árbol en una teoría bien definida de procedimientos de inferencia condicional. Esta clase no paramétrica de árboles de regresión es aplicable a todo tipo de problemas de regresión, incluidas variables de respuesta nominales, ordinales, numéricas, censuradas y multivariadas y escalas de medición arbitrarias de las covariables. Una de las principales diferencias que tiene con otros métodos de partición es que no hace prunning debido a su sustento estadístico. Es decir, el árbol sigue creciendo hasta que no quede ninguna variable que posea una relación significativa con el target.</p>
<p><strong>tree</strong>: La función para crear un árbol de regresión es tree, a continuación la estructura de la función.</p>
<p><strong>randomForest:</strong> dispone de los principales algoritmos para crear modelos Random Forest. Destaca por su fácil uso, pero no por su rapidez.</p>
<p><strong>ranger:</strong> algoritmos para crear modelos random forest. Es similar a randomForest pero mucho más rápido. Además, incorpora extremely randomized trees y quantile regression forests.</p>
<p><strong>gbm:</strong> dispone de los principales algoritmos de boosting. Este paquete ya no está mantenido, aunque es útil para explicar los conceptos, no se recomienda su uso en producción.</p>
<p><strong>XGBoost:</strong> esta librería permite acceder al algoritmo XGboost (Extra Gradient boosting). Una adaptación de gradient boosting que destaca por su eficiencia y rapidez.</p>
<p><strong>H2O:</strong> implementaciones muy optimizadas de los principales algoritmos de machine learning, entre ellos random forest, gradient boosting y XGBoost. Para conocer más detalles sobre cómo utilizar esta librería consultar Machine Learning con H2O y R.</p>
<p><strong>C50:</strong> implementación de los algoritmos C5.0 para árboles de clasificación.</p>
<p>Existen otros paquetes que el lector puede consultar en la sección Recursive Partitioning de <a href="https://cran.r-project.org/web/views/MachineLearning.html">CRAN Task View: Machine Learning &amp; Statistical Learning.</a></p>
</div>
<div id="ejemplo-árbol-de-clasificación" class="section level2" number="7.8">
<h2>
<span class="header-section-number">7.8</span> Ejemplo Árbol de Clasificación<a class="anchor" aria-label="anchor" href="#ejemplo-%C3%A1rbol-de-clasificaci%C3%B3n"><i class="fas fa-link"></i></a>
</h2>
<p>Un conjunto de datos clásico en Machine Learning son los datos del <strong>Titanic</strong>. Este set de datos contiene información sobre los pasajeros del RMS Titanic, el transatlántico británico que se hundió en abril de 1912 durante su viaje inaugural desde Southampton a Nueva York. Entre la información almacenada se encuentran la edad, género, características socio-económicas de los pasajeros y si sobrevivieron o no al naufragio. Aunque pueda resultar un clásico poco original, estos datos tienen una serie de características que los hacen idóneos para ser utilizados como ejemplo introductorio. Se trata de un problema y de unos datos cuyas variables pueden entenderse de forma sencilla. Es intuitivo comprender el impacto que puede tener la edad, el sexo, la localización del camarote en la supervivencia de los pasajeros. Aunque no lo parezca, comprender a fondo el problema que se pretende modelar es lo más importante para lograr buenos resultados.</p>
<p>En este ejemplo utilizaremos la librería <strong>rpart</strong> y <strong>rpart.plot</strong> para obtener los gráficos de árboles.</p>
<p>En primer lugar obtendremos un simple árbol sin ningún tipo de restricciones</p>
<div class="sourceCode" id="cb120"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bethatkinson/rpart">rpart</a></span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.milbo.org/rpart-plot/index.html">rpart.plot</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"ptitanic"</span><span class="op">)</span></span>
<span><span class="va">arbol_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span><span class="va">survived</span> <span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">ptitanic</span>, method <span class="op">=</span> <span class="st">'class'</span><span class="op">)</span></span></code></pre></div>
<p>Por defecto, rpart() usa la medida de Gini para la división de los nodos.</p>
<p>Los resultados que ofrece el algoritmo son:</p>
<div class="sourceCode" id="cb121"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">arbol_1</span></span>
<span><span class="co">#&gt; n= 1309 </span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; node), split, n, loss, yval, (yprob)</span></span>
<span><span class="co">#&gt;       * denotes terminal node</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt;   1) root 1309 500 died (0.6180290 0.3819710)  </span></span>
<span><span class="co">#&gt;     2) sex=male 843 161 died (0.8090154 0.1909846)  </span></span>
<span><span class="co">#&gt;       4) age&gt;=9.5 796 136 died (0.8291457 0.1708543) *</span></span>
<span><span class="co">#&gt;       5) age&lt; 9.5 47  22 survived (0.4680851 0.5319149)  </span></span>
<span><span class="co">#&gt;        10) sibsp&gt;=2.5 20   1 died (0.9500000 0.0500000) *</span></span>
<span><span class="co">#&gt;        11) sibsp&lt; 2.5 27   3 survived (0.1111111 0.8888889) *</span></span>
<span><span class="co">#&gt;     3) sex=female 466 127 survived (0.2725322 0.7274678)  </span></span>
<span><span class="co">#&gt;       6) pclass=3rd 216 106 died (0.5092593 0.4907407)  </span></span>
<span><span class="co">#&gt;        12) sibsp&gt;=2.5 21   3 died (0.8571429 0.1428571) *</span></span>
<span><span class="co">#&gt;        13) sibsp&lt; 2.5 195  92 survived (0.4717949 0.5282051)  </span></span>
<span><span class="co">#&gt;          26) age&gt;=16.5 162  79 died (0.5123457 0.4876543)  </span></span>
<span><span class="co">#&gt;            52) parch&gt;=3.5 9   1 died (0.8888889 0.1111111) *</span></span>
<span><span class="co">#&gt;            53) parch&lt; 3.5 153  75 survived (0.4901961 0.5098039)  </span></span>
<span><span class="co">#&gt;             106) age&gt;=27.5 44  17 died (0.6136364 0.3863636) *</span></span>
<span><span class="co">#&gt;             107) age&lt; 27.5 109  48 survived (0.4403670 0.5596330)  </span></span>
<span><span class="co">#&gt;               214) age&lt; 21.5 28  11 died (0.6071429 0.3928571) *</span></span>
<span><span class="co">#&gt;               215) age&gt;=21.5 81  31 survived (0.3827160 0.6172840) *</span></span>
<span><span class="co">#&gt;          27) age&lt; 16.5 33   9 survived (0.2727273 0.7272727) *</span></span>
<span><span class="co">#&gt;       7) pclass=1st,2nd 250  17 survived (0.0680000 0.9320000) *</span></span></code></pre></div>
<p>La librería <strong>rpart.plot</strong> muestra los resultados gráficamente lo que puede ayudar a comprender el funcionamiento del algoritmo.</p>
<ul>
<li>En cada nodo se indica como se clasifican los individuos</li>
<li>En segundo lugar se muestra el error de clasificarlos como se indica en la primera fila</li>
<li>En la tercera fila el % elementos en ese nodo</li>
</ul>
<div class="sourceCode" id="cb122"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">arbol_1</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-4"></span>
<img src="06-Sesion6-Arboles-de-Decision_files/figure-html/unnamed-chunk-4-1.png" alt="\label{fig:arbol1}Árbol base." width="768"><p class="caption">
Figure 7.3: Árbol base.
</p>
</div>
<p>Por ejemplo, el la primera rama de la izquierda se seleccionan “Hombres” que supone un 64% de la muestra. Todos los hombres (n=843) se clasifican como “Muertos”. La realidad es que de los 843 hombres, se salvaron 161 y por tanto el error = 161/843 = 0.19</p>
<p>Para mejorar este resultado, esta muestra de hombres puede subdividirse dependiendo de la edad para mejorar la predicción. Si la edad es mayor o igual a 9,5 años se clasifican como “Muertos” y si la edad es menor de 9.5, se clasifican como Sobrevivientes”</p>
<p>Conforme el arbol va “creciendo” en “ramas” cada vez se comente menores errores.</p>
<p>La capacidad predictiva puede ser muy alta si el árbol se deja crecer sin control.</p>
<div class="sourceCode" id="cb123"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Predicción con sobrajuste</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="va">prediccion_0</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">arbol_1</span>, type <span class="op">=</span> <span class="st">"class"</span><span class="op">)</span></span>
<span><span class="va">table_mat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">ptitanic</span><span class="op">$</span><span class="va">survived</span>, <span class="va">prediccion_0</span><span class="op">)</span></span>
<span><span class="va">table_mat</span></span>
<span><span class="co">#&gt;           prediccion_0</span></span>
<span><span class="co">#&gt;            died survived</span></span>
<span><span class="co">#&gt;   died      749       60</span></span>
<span><span class="co">#&gt;   survived  169      331</span></span>
<span><span class="va">accuracy_Test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">table_mat</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">table_mat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">'Accuracy for test'</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">accuracy_Test</span>,<span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Accuracy for test 0.825"</span></span></code></pre></div>
<p>El árbol sin podado clasifica correctamente el 83% de las observaciones</p>
<div id="pre-podado" class="section level3" number="7.8.1">
<h3>
<span class="header-section-number">7.8.1</span> Pre-podado<a class="anchor" aria-label="anchor" href="#pre-podado"><i class="fas fa-link"></i></a>
</h3>
<p>La Figura <span class="math inline">\(\ref{fig:arbol1}\)</span> muestra un árbol sin restricciones. Se puede hacer un prepodado controlando el número mínimo de observaciones en un nodo terminal:</p>
<div class="sourceCode" id="cb124"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Control de prepodado</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="va">control</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>minbucket <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span><span class="va">arbol_2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span><span class="va">survived</span> <span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">ptitanic</span>, method <span class="op">=</span> <span class="st">'class'</span>, control <span class="op">=</span> <span class="va">control</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb125"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.milbo.org/rpart-plot/index.html">rpart.plot</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">arbol_2</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-7"></span>
<img src="06-Sesion6-Arboles-de-Decision_files/figure-html/unnamed-chunk-7-1.png" alt="\label{fig:arbol2}Árbol base 2." width="336"><p class="caption">
Figure 7.4: Árbol base 2.
</p>
</div>
<p>Otra alternativa sería controlar tanto el número mínimo de observaciones en un nodo (“minbucket=50”) como el número mínimo de observaciones en un nodo para poder ser dividido (“minsplit = 250” si en un nodo hay 250 observaciones o menos, entoncen éste no se dividirá y el arbol no crecerá más por esa rama).</p>
<div class="sourceCode" id="cb126"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Control de prepodado</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="va">control</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>minbucket <span class="op">=</span> <span class="fl">50</span>, minsplit <span class="op">=</span> <span class="fl">250</span><span class="op">)</span></span>
<span><span class="va">arbol_3</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span><span class="va">survived</span> <span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">ptitanic</span>, method <span class="op">=</span> <span class="st">'class'</span>, control <span class="op">=</span> <span class="va">control</span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.milbo.org/rpart-plot/index.html">rpart.plot</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">arbol_3</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-9"></span>
<img src="06-Sesion6-Arboles-de-Decision_files/figure-html/unnamed-chunk-9-1.png" alt="\label{fig:arbol3}Árbol base 3." width="240"><p class="caption">
Figure 7.5: Árbol base 3.
</p>
</div>
</div>
<div id="entrenamiento-y-test" class="section level3" number="7.8.2">
<h3>
<span class="header-section-number">7.8.2</span> Entrenamiento y test<a class="anchor" aria-label="anchor" href="#entrenamiento-y-test"><i class="fas fa-link"></i></a>
</h3>
<p>En esta subsección se dividirá la muestra en submuestras de entrenamiento y test y se evaluará la capacidad predictiva del árbol</p>
<div class="sourceCode" id="cb128"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># División de los datos en train y test</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sample.html">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">ptitanic</span><span class="op">)</span>, size <span class="op">=</span> <span class="fl">.7</span><span class="op">*</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html">nrow</a></span><span class="op">(</span><span class="va">ptitanic</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="va">ptitanic_train</span> <span class="op">&lt;-</span> <span class="va">ptitanic</span><span class="op">[</span><span class="va">train</span>,<span class="op">]</span></span>
<span><span class="va">ptitanic_test</span>  <span class="op">&lt;-</span> <span class="va">ptitanic</span><span class="op">[</span><span class="op">-</span><span class="va">train</span>,<span class="op">]</span></span></code></pre></div>
<p>El siguiente código obtiene un árbol con la muestra de entrenamiento. Los resultados se muestran en la Figura <span class="math inline">\(\ref{fig:arbol4}\)</span>.</p>
<div class="sourceCode" id="cb129"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">control</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/list.html">list</a></span><span class="op">(</span>minbucket <span class="op">=</span> <span class="fl">50</span>, minsplit <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span><span class="va">arbol_4</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/rpart/man/rpart.html">rpart</a></span><span class="op">(</span><span class="va">survived</span> <span class="op">~</span><span class="va">.</span>, data <span class="op">=</span> <span class="va">ptitanic_train</span>, method <span class="op">=</span> <span class="st">'class'</span>, control <span class="op">=</span> <span class="va">control</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/pkg/rpart.plot/man/rpart.plot.html">rpart.plot</a></span><span class="op">(</span><span class="va">arbol_4</span>, digits <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-11"></span>
<img src="06-Sesion6-Arboles-de-Decision_files/figure-html/unnamed-chunk-11-1.png" alt="\label{fig:arbol4}Árbol base 4." width="336"><p class="caption">
Figure 7.6: Árbol base 4.
</p>
</div>
<p>La función predict() obtiene las predicciones para la muestra de test y la tabla muestra los resultados de la predicción</p>
<div class="sourceCode" id="cb130"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">prediccion_1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">arbol_4</span>, newdata <span class="op">=</span> <span class="va">ptitanic_test</span>, type <span class="op">=</span> <span class="st">'class'</span><span class="op">)</span></span>
<span><span class="va">table_mat</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/table.html">table</a></span><span class="op">(</span><span class="va">ptitanic_test</span><span class="op">$</span><span class="va">survived</span>, <span class="va">prediccion_1</span><span class="op">)</span></span>
<span><span class="va">table_mat</span></span>
<span><span class="co">#&gt;           prediccion_1</span></span>
<span><span class="co">#&gt;            died survived</span></span>
<span><span class="co">#&gt;   died      210       20</span></span>
<span><span class="co">#&gt;   survived   68       95</span></span></code></pre></div>
<p>La accuracy de este árbol se obtiene con el siguiente código:</p>
<div class="sourceCode" id="cb131"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">accuracy_Test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/diag.html">diag</a></span><span class="op">(</span><span class="va">table_mat</span><span class="op">)</span><span class="op">)</span> <span class="op">/</span> <span class="fu"><a href="https://rdrr.io/r/base/sum.html">sum</a></span><span class="op">(</span><span class="va">table_mat</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html">print</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">'Accuracy for test'</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">accuracy_Test</span>,<span class="fl">3</span><span class="op">)</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Accuracy for test 0.776"</span></span></code></pre></div>
<p>Lo ideal sería aplicar un proceso de validación cruzada para ajustar correctamente y encontrar así la mejor combinación de hiperparámetros.</p>
</div>
</div>
<div id="ejemplo-árbol-de-regresión" class="section level2" number="7.9">
<h2>
<span class="header-section-number">7.9</span> Ejemplo Árbol de Regresión<a class="anchor" aria-label="anchor" href="#ejemplo-%C3%A1rbol-de-regresi%C3%B3n"><i class="fas fa-link"></i></a>
</h2>
<div id="datos" class="section level3" number="7.9.1">
<h3>
<span class="header-section-number">7.9.1</span> Datos<a class="anchor" aria-label="anchor" href="#datos"><i class="fas fa-link"></i></a>
</h3>
<p>El set de datos Boston disponible en el paquete MASS contiene precios de viviendas de la ciudad de Boston, así como información socioeconómica del barrio en el que se encuentran. Se pretende ajustar un modelo de regresión que permita predecir el precio medio de una vivienda (medv) en función de las variables disponibles.</p>
<div class="sourceCode" id="cb132"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">if</span> <span class="op">(</span><span class="op">!</span><span class="kw"><a href="https://rdrr.io/r/base/library.html">require</a></span><span class="op">(</span><span class="st"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">"MASS"</a></span><span class="op">)</span><span class="op">)</span> <span class="fu"><a href="https://rdrr.io/r/utils/install.packages.html">install.packages</a></span><span class="op">(</span><span class="st">"MASS"</span><span class="op">)</span></span>
<span><span class="co">#&gt; Loading required package: MASS</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="http://www.stats.ox.ac.uk/pub/MASS4/">MASS</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html">data</a></span><span class="op">(</span><span class="st">"Boston"</span>, package <span class="op">=</span> <span class="st">"MASS"</span><span class="op">)</span></span></code></pre></div>
</div>
<div id="ajuste-del-modelo-con-la-librería-tree" class="section level3" number="7.9.2">
<h3>
<span class="header-section-number">7.9.2</span> Ajuste del modelo con la librería ‘tree’<a class="anchor" aria-label="anchor" href="#ajuste-del-modelo-con-la-librer%C3%ADa-tree"><i class="fas fa-link"></i></a>
</h3>
<p>La función <strong>tree()</strong> del paquete tree permite ajustar árboles de decisión. La elección entre árbol de regresión o árbol de clasificación se hace automáticamente dependiendo de si la variable respuesta es de tipo numeric o factor. Es importante tener en cuenta que solo estos dos tipos de vectores están permitidos, si se pasa uno de tipo character se devuelve un error.</p>
<p>A continuación, se ajusta un árbol de regresión empleando como variable respuesta medv y como predictores todas las variables disponibles. Como en todo estudio de regresión, no solo es importante ajustar el modelo, sino también cuantificar su capacidad para predecir nuevas observaciones. Para poder hacer la posterior evaluación, se dividen los datos en dos grupos, uno de entrenamiento y otro de test.</p>
<p>La función <strong>tree()</strong> crece el árbol hasta que encuentra una condición de stop. Por defecto, estas condiciones son:</p>
<ul>
<li><p><strong>mincut:</strong> Número mínimo de observaciones que debe de tener al menos uno de los nodos hijos para que se produzca la división. Si al dividir el nodo, uno de los subnodos tiene menos de mincut observaciones, entonces el nodo no se divide.</p></li>
<li><p><strong>minsize:</strong> Número mínimo de observaciones que debe de tener un nodo para que pueda dividirse.</p></li>
</ul>
<div class="sourceCode" id="cb133"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># División de los datos en train y test equilibrando por cuartiles</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://rsample.tidymodels.org">rsample</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">split</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">initial_split</a></span><span class="op">(</span><span class="va">Boston</span>, prop <span class="op">=</span> <span class="fl">0.7</span>, strata <span class="op">=</span> <span class="va">medv</span><span class="op">)</span></span>
<span><span class="va">Boston_train</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">training</a></span><span class="op">(</span><span class="va">split</span><span class="op">)</span></span>
<span><span class="va">Boston_test</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rsample.tidymodels.org/reference/initial_split.html">testing</a></span><span class="op">(</span><span class="va">split</span><span class="op">)</span></span></code></pre></div>
<div id="prepodado" class="section level4" number="7.9.2.1">
<h4>
<span class="header-section-number">7.9.2.1</span> Prepodado<a class="anchor" aria-label="anchor" href="#prepodado"><i class="fas fa-link"></i></a>
</h4>
<p>El siguiente chunk muestra un código para obtener un modelo con pre-podado en función del número mínimo de observaciones</p>
<div class="sourceCode" id="cb134"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Creación y entrenamiento del modelo</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">tree</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">control</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tree/man/tree.control.html">tree.control</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Boston_train</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, </span>
<span>                        mincut <span class="op">=</span> <span class="fl">20</span>, </span>
<span>                        minsize <span class="op">=</span> <span class="fl">50</span><span class="op">)</span></span>
<span><span class="va">arbol_regresion</span> <span class="op">&lt;-</span> <span class="fu">tree</span><span class="fu">::</span><span class="fu"><a href="https://rdrr.io/pkg/tree/man/tree.html">tree</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>                              data    <span class="op">=</span> <span class="va">Boston_train</span>,</span>
<span>                              control <span class="op">=</span> <span class="va">control</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">arbol_regresion</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Regression tree:</span></span>
<span><span class="co">#&gt; tree::tree(formula = medv ~ ., data = Boston_train, control = control)</span></span>
<span><span class="co">#&gt; Variables actually used in tree construction:</span></span>
<span><span class="co">#&gt; [1] "rm"    "lstat" "nox"   "crim" </span></span>
<span><span class="co">#&gt; Number of terminal nodes:  7 </span></span>
<span><span class="co">#&gt; Residual mean deviance:  19.77 = 6821 / 345 </span></span>
<span><span class="co">#&gt; Distribution of residuals:</span></span>
<span><span class="co">#&gt;     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. </span></span>
<span><span class="co">#&gt; -22.1800  -2.1320   0.1656   0.0000   2.0050  27.7700</span></span></code></pre></div>
<p>La función summary() muestra que, el árbol entrenado, tiene un total de 7 nodos terminales y que se han empleado como predictores las variables “rm”, “lstat”, “nox” y “crim”. En el contexto de árboles de regresión, el término <em>Residual mean deviance</em> es la suma de cuadrados residuales dividida entre (número de observaciones - número de nodos terminales). Cuanto menor es la ‘deviance’, mejor es el ajuste del árbol a las observaciones de entrenamiento.</p>
<p>El paquete tree no posee una buena capacidad para representar los árboles, pero una vez creado el árbol, se puede representar mediante la combinación de las funciones plot() y text(). La función plot() dibuja la estructura del árbol, las ramas y los nodos. Mediante su argumento type se puede especificar si se quiere que todas las ramas tengan el mismo tamaño (type = “uniform”) o que su longitud sea proporcional a la reducción de impureza (heterogeneidad) de los nodos terminales (type = “proportional”). Esta segunda opción permite identificar visualmente el impacto de cada división en el modelo. La función text() añade la descripción de cada nodo interno y el valor de cada nodo terminal.</p>
<div class="sourceCode" id="cb135"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Estructura del árbol con prepodado</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">arbol_regresion</span>, type <span class="op">=</span> <span class="st">"proportional"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">arbol_regresion</span>, splits <span class="op">=</span> <span class="cn">TRUE</span>, pretty <span class="op">=</span> <span class="fl">0</span>, cex <span class="op">=</span> <span class="fl">0.8</span>, col <span class="op">=</span> <span class="st">"firebrick"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="06-Sesion6-Arboles-de-Decision_files/figure-html/unnamed-chunk-17-1.png" width="672"></div>
</div>
</div>
<div id="podado-del-árbol-pruning" class="section level3" number="7.9.3">
<h3>
<span class="header-section-number">7.9.3</span> Podado del árbol (pruning)<a class="anchor" aria-label="anchor" href="#podado-del-%C3%A1rbol-pruning"><i class="fas fa-link"></i></a>
</h3>
<p>Con la finalidad de reducir la varianza del modelo y así mejorar la capacidad predictiva, se somete al árbol a un proceso de podado (pruning). El proceso de podado intenta encontrar el árbol más sencillo (menor tamaño) que consigue los mejores resultados de predicción.</p>
<p>Para podar un árbol es necesario indicar el grado de penalización por complejidad <span class="math inline">\(\alpha\)</span>. Cuanto mayor sea este valor, más agresivo es el podado y menor el tamaño del árbol resultante. Dado que no hay forma de conocer de antemano el valor óptimo de <span class="math inline">\(\alpha\)</span>, se recurre a validación cruzada para identificarlo.</p>
<p>La función cv.tree() emplea validación cruzada (cross validation) para identificar el valor óptimo de penalización. Por defecto, esta función emplea la ‘deviance’ para guiar el proceso de pruning.</p>
<p>El podado se realiza sobre un árbol sin apenas requisitos de prepodado como muestra el siguiente código:</p>
<div class="sourceCode" id="cb136"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Pruning (const complexity pruning) por validación cruzada</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span></span>
<span><span class="co"># El árbol se crece al máximo posible para luego aplicar el pruning</span></span>
<span><span class="va">control</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tree/man/tree.control.html">tree.control</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/dim.html">dim</a></span><span class="op">(</span><span class="va">Boston_train</span><span class="op">)</span><span class="op">[</span><span class="fl">1</span><span class="op">]</span>, </span>
<span>                        mincut <span class="op">=</span> <span class="fl">1</span>, </span>
<span>                        minsize <span class="op">=</span> <span class="fl">2</span>,</span>
<span>                        mindev  <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span>
<span><span class="va">arbol_regresion_base</span><span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tree/man/tree.html">tree</a></span><span class="op">(</span>formula <span class="op">=</span> <span class="va">medv</span> <span class="op">~</span> <span class="va">.</span>,</span>
<span>                            data    <span class="op">=</span> <span class="va">Boston_train</span>,</span>
<span>                            control <span class="op">=</span> <span class="va">control</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/summary.html">summary</a></span><span class="op">(</span><span class="va">arbol_regresion_base</span><span class="op">)</span></span>
<span><span class="co">#&gt; </span></span>
<span><span class="co">#&gt; Regression tree:</span></span>
<span><span class="co">#&gt; tree(formula = medv ~ ., data = Boston_train, control = control)</span></span>
<span><span class="co">#&gt; Variables actually used in tree construction:</span></span>
<span><span class="co">#&gt;  [1] "rm"      "lstat"   "dis"     "indus"   "black"  </span></span>
<span><span class="co">#&gt;  [6] "age"     "crim"    "ptratio" "tax"     "rad"    </span></span>
<span><span class="co">#&gt; [11] "zn"      "nox"    </span></span>
<span><span class="co">#&gt; Number of terminal nodes:  281 </span></span>
<span><span class="co">#&gt; Residual mean deviance:  0.008568 = 0.6083 / 71 </span></span>
<span><span class="co">#&gt; Distribution of residuals:</span></span>
<span><span class="co">#&gt;    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. </span></span>
<span><span class="co">#&gt; -0.1000  0.0000  0.0000  0.0000  0.0000  0.1333</span></span></code></pre></div>
<p>El árbol sin restricciones de prepodado tiene 281 nodos y el valor de ‘Residual mean deviance’ ha disminuido mucho, pero obviamente hay sobreajuste.</p>
<p>Para resucir el sobreajuste de este modelo se aplica un proceso de validación cruzada como se indicó el la Figura . En este caso se toman 5 folks.</p>
<div class="sourceCode" id="cb137"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Búsqueda por validación cruzada</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/Random.html">set.seed</a></span><span class="op">(</span><span class="fl">123</span><span class="op">)</span></span>
<span><span class="va">cv_arbol</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tree/man/cv.tree.html">cv.tree</a></span><span class="op">(</span><span class="va">arbol_regresion_base</span>, K <span class="op">=</span> <span class="fl">5</span><span class="op">)</span></span></code></pre></div>
<p>El objeto devuelto por <strong>cv.tree()</strong> contiene:</p>
<ul>
<li>
<strong>size:</strong> el tamaño (número de nodos terminales) de cada árbol.</li>
<li>
<strong>dev:</strong> la estimación de cross-validation test error para cada tamaño de árbol.</li>
<li>
<strong>k:</strong> El rango de valores de penalización <span class="math inline">\(\alpha\)</span> evaluados.</li>
<li>
<strong>method:</strong> El criterio empleado para seleccionar el mejor árbol.</li>
</ul>
<div class="sourceCode" id="cb138"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Tamaño óptimo encontrado</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="va">size_optimo</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/rev.html">rev</a></span><span class="op">(</span><span class="va">cv_arbol</span><span class="op">$</span><span class="va">size</span><span class="op">)</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/which.min.html">which.min</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rev.html">rev</a></span><span class="op">(</span><span class="va">cv_arbol</span><span class="op">$</span><span class="va">dev</span><span class="op">)</span><span class="op">)</span><span class="op">]</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Tamaño óptimo encontrado:"</span>, <span class="va">size_optimo</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Tamaño óptimo encontrado: 11"</span></span></code></pre></div>
<div class="sourceCode" id="cb139"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va">gridExtra</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html">library</a></span><span class="op">(</span><span class="va"><a href="https://ggplot2.tidyverse.org">ggplot2</a></span><span class="op">)</span></span>
<span><span class="va">resultados_cv</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html">data.frame</a></span><span class="op">(</span>n_nodos  <span class="op">=</span> <span class="va">cv_arbol</span><span class="op">$</span><span class="va">size</span>,</span>
<span>                            deviance <span class="op">=</span> <span class="va">cv_arbol</span><span class="op">$</span><span class="va">dev</span>,</span>
<span>                            alpha    <span class="op">=</span> <span class="va">cv_arbol</span><span class="op">$</span><span class="va">k</span><span class="op">)</span></span>
<span></span>
<span><span class="va">p1</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">resultados_cv</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">n_nodos</span>, y <span class="op">=</span> <span class="va">deviance</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">ylim</a></span><span class="op">(</span><span class="fl">8000</span>,<span class="fl">12000</span><span class="op">)</span><span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/lims.html">xlim</a></span><span class="op">(</span><span class="fl">0</span>,<span class="fl">20</span><span class="op">)</span><span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_abline.html">geom_vline</a></span><span class="op">(</span>xintercept <span class="op">=</span> <span class="va">size_optimo</span>, color <span class="op">=</span> <span class="st">"red"</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Error vs tamaño del árbol"</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span> </span>
<span>  </span>
<span><span class="va">p2</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggplot.html">ggplot</a></span><span class="op">(</span>data <span class="op">=</span> <span class="va">resultados_cv</span>, <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/aes.html">aes</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">alpha</span>, y <span class="op">=</span> <span class="va">deviance</span><span class="op">)</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_path.html">geom_line</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span> </span>
<span>      <span class="co"># ylim(8290,8300)+</span></span>
<span>      <span class="co"># xlim(0,1)+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/geom_point.html">geom_point</a></span><span class="op">(</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/labs.html">labs</a></span><span class="op">(</span>title <span class="op">=</span> <span class="st">"Error vs penalización alpha"</span><span class="op">)</span> <span class="op">+</span></span>
<span>      <span class="fu"><a href="https://ggplot2.tidyverse.org/reference/ggtheme.html">theme_bw</a></span><span class="op">(</span><span class="op">)</span> </span>
<span><span class="fu"><a href="https://rdrr.io/pkg/gridExtra/man/arrangeGrob.html">grid.arrange</a></span><span class="op">(</span><span class="va">p1</span>, <span class="va">p2</span>, ncol <span class="op">=</span> <span class="fl">2</span>, nrow <span class="op">=</span> <span class="fl">1</span><span class="op">)</span></span></code></pre></div>
<div class="figure">
<span style="display:block;" id="fig:unnamed-chunk-21"></span>
<img src="06-Sesion6-Arboles-de-Decision_files/figure-html/unnamed-chunk-21-1.png" alt="\label{fig:particion}Partición del espacio con predicciones" width="672"><p class="caption">
Figure 7.7: Partición del espacio con predicciones
</p>
</div>
<p>Una vez identificado el valor óptimo de <span class="math inline">\(\alpha\)</span>, con la función prune.tree() se aplica el podado final. En este caso se impone la condición de que obtenga el árbol con el valor óptimo de ramas encontrado el el proceso de validación cruzada con cvtree().</p>
<div class="sourceCode" id="cb140"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Estructura del árbol creado final</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="va">arbol_final</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/pkg/tree/man/prune.tree.html">prune.tree</a></span><span class="op">(</span>tree <span class="op">=</span> <span class="va">arbol_regresion_base</span>,</span>
<span>                          best <span class="op">=</span> <span class="va">size_optimo</span><span class="op">)</span> </span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html">plot</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">arbol_final</span>, type <span class="op">=</span> <span class="st">"uniform"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/text.html">text</a></span><span class="op">(</span>x <span class="op">=</span> <span class="va">arbol_final</span>, splits <span class="op">=</span> <span class="cn">TRUE</span>, pretty <span class="op">=</span> <span class="fl">0</span>, cex <span class="op">=</span> <span class="fl">0.8</span>, col <span class="op">=</span> <span class="st">"firebrick"</span><span class="op">)</span></span></code></pre></div>
<div class="inline-figure"><img src="06-Sesion6-Arboles-de-Decision_files/figure-html/unnamed-chunk-22-1.png" width="576"></div>
</div>
<div id="predicción-y-evaluación-del-modelo" class="section level3" number="7.9.4">
<h3>
<span class="header-section-number">7.9.4</span> Predicción y evaluación del modelo<a class="anchor" aria-label="anchor" href="#predicci%C3%B3n-y-evaluaci%C3%B3n-del-modelo"><i class="fas fa-link"></i></a>
</h3>
<p>Por último, se evalúa la capacidad predictiva de los tres árboles empleando el conjunto de test.</p>
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Error de test del modelo base con pre-podado y sin post-podado</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="va">predicciones</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">arbol_regresion</span>, newdata <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span></span>
<span><span class="va">test_rmse</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">predicciones</span> <span class="op">-</span> <span class="va">Boston_test</span><span class="op">$</span><span class="va">medv</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Error de test (RMSE) del árbol solo CON pre-podado:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">test_rmse</span>,<span class="fl">4</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Error de test (RMSE) del árbol solo CON pre-podado: 4.6083"</span></span></code></pre></div>
<div class="sourceCode" id="cb142"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Error de test del modelo base sin pre-podado y sin post-podado</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="va">predicciones</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">arbol_regresion_base</span>, newdata <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span></span>
<span><span class="va">test_rmse</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">predicciones</span> <span class="op">-</span> <span class="va">Boston_test</span><span class="op">$</span><span class="va">medv</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Error de test (RMSE) del árbol SIN pre-podado y SIN post-podado:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">test_rmse</span>,<span class="fl">4</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Error de test (RMSE) del árbol SIN pre-podado y SIN post-podado: 4.9402"</span></span></code></pre></div>
<div class="sourceCode" id="cb143"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Error de test del modelo final</span></span>
<span><span class="co"># ==============================================================================</span></span>
<span><span class="va">predicciones</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/stats/predict.html">predict</a></span><span class="op">(</span><span class="va">arbol_final</span>, newdata <span class="op">=</span> <span class="va">Boston_test</span><span class="op">)</span></span>
<span><span class="va">test_rmse</span>    <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html">sqrt</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/mean.html">mean</a></span><span class="op">(</span><span class="op">(</span><span class="va">predicciones</span> <span class="op">-</span> <span class="va">Boston_test</span><span class="op">$</span><span class="va">medv</span><span class="op">)</span><span class="op">^</span><span class="fl">2</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/paste.html">paste</a></span><span class="op">(</span><span class="st">"Error de test (RMSE) del árbol sin pre-podado y CON post-podado:"</span>, <span class="fu"><a href="https://rdrr.io/r/base/Round.html">round</a></span><span class="op">(</span><span class="va">test_rmse</span>,<span class="fl">4</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "Error de test (RMSE) del árbol sin pre-podado y CON post-podado: 4.6198"</span></span></code></pre></div>
<p>En este caso el modelo los modelos con pre-podado y post-podado tiene un comportamiento muy similar en términos de RMSE y ambas opciones mejoran el RMSE del modelo sin podar.</p>
</div>
</div>
<div id="webs" class="section level2" number="7.10">
<h2>
<span class="header-section-number">7.10</span> Webs<a class="anchor" aria-label="anchor" href="#webs"><i class="fas fa-link"></i></a>
</h2>
<p><a href="https://rpubs.com/fdvm/clase_arboles_uba_dm" class="uri">https://rpubs.com/fdvm/clase_arboles_uba_dm</a></p>
<p><a href="https://rpubs.com/mpfoley73/529130" class="uri">https://rpubs.com/mpfoley73/529130</a></p>
</div>
<div id="referencias" class="section level2" number="7.11">
<h2>
<span class="header-section-number">7.11</span> Referencias<a class="anchor" aria-label="anchor" href="#referencias"><i class="fas fa-link"></i></a>
</h2>

</div>
</div>

  <div class="chapter-nav">
<div class="prev"><a href="an%C3%A1lisis-cluster.html"><span class="header-section-number">6</span> Análisis Cluster</a></div>
<div class="next"><a href="m%C3%A9todos-chingones.html"><span class="header-section-number">8</span> Métodos chingones</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#m%C3%A9todos-basados-en-%C3%A1rboles"><span class="header-section-number">7</span> Métodos basados en árboles</a></li>
<li><a class="nav-link" href="#introducci%C3%B3n-1"><span class="header-section-number">7.1</span> Introducción</a></li>
<li><a class="nav-link" href="#origen-de-los-%C3%A1rboles-de-decisi%C3%B3n"><span class="header-section-number">7.2</span> Origen de los Árboles de Decisión</a></li>
<li><a class="nav-link" href="#los-%C3%A1rboles-de-decisi%C3%B3n-en-las-t%C3%A9cnicas-de-machine-learning"><span class="header-section-number">7.3</span> Los Árboles de Decisión en las técnicas de Machine Learning</a></li>
<li>
<a class="nav-link" href="#%C3%A1rboles-de-regresi%C3%B3n"><span class="header-section-number">7.4</span> Árboles de regresión</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#un-ejemplo-de-juguete"><span class="header-section-number">7.4.1</span> Un ejemplo de juguete</a></li>
<li><a class="nav-link" href="#sec:terminologia"><span class="header-section-number">7.4.2</span> Terminología</a></li>
<li><a class="nav-link" href="#como-se-crea-un-%C3%A1rbol"><span class="header-section-number">7.4.3</span> Como se crea un árbol</a></li>
<li><a class="nav-link" href="#evitar-el-overfitting"><span class="header-section-number">7.4.4</span> Evitar el overfitting</a></li>
<li><a class="nav-link" href="#validaci%C3%B3n-cruzada"><span class="header-section-number">7.4.5</span> Validación cruzada</a></li>
<li><a class="nav-link" href="#algoritmo-para-crear-un-%C3%A1rbol-de-regresi%C3%B3n-con-pruning"><span class="header-section-number">7.4.6</span> Algoritmo para crear un árbol de regresión con pruning</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#%C3%A1rboles-de-clasificaci%C3%B3n"><span class="header-section-number">7.5</span> Árboles de clasificación</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#c%C3%B3mo-se-crea-un-%C3%A1rbol-de-clasificaci%C3%B3n"><span class="header-section-number">7.5.1</span> Cómo se crea un árbol de clasificación</a></li>
<li><a class="nav-link" href="#predicci%C3%B3n-del-%C3%A1rbol"><span class="header-section-number">7.5.2</span> Predicción del árbol</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#ventajas-y-desventajas-de-los-%C3%A1rboles-de-decisi%C3%B3n"><span class="header-section-number">7.6</span> Ventajas y desventajas de los Árboles de Decisión</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#ventajas-algunas"><span class="header-section-number">7.6.1</span> Ventajas (algunas)</a></li>
<li><a class="nav-link" href="#desventajas-algunas"><span class="header-section-number">7.6.2</span> Desventajas (algunas)</a></li>
</ul>
</li>
<li><a class="nav-link" href="#paquetes-de-r"><span class="header-section-number">7.7</span> Paquetes de R</a></li>
<li>
<a class="nav-link" href="#ejemplo-%C3%A1rbol-de-clasificaci%C3%B3n"><span class="header-section-number">7.8</span> Ejemplo Árbol de Clasificación</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#pre-podado"><span class="header-section-number">7.8.1</span> Pre-podado</a></li>
<li><a class="nav-link" href="#entrenamiento-y-test"><span class="header-section-number">7.8.2</span> Entrenamiento y test</a></li>
</ul>
</li>
<li>
<a class="nav-link" href="#ejemplo-%C3%A1rbol-de-regresi%C3%B3n"><span class="header-section-number">7.9</span> Ejemplo Árbol de Regresión</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#datos"><span class="header-section-number">7.9.1</span> Datos</a></li>
<li><a class="nav-link" href="#ajuste-del-modelo-con-la-librer%C3%ADa-tree"><span class="header-section-number">7.9.2</span> Ajuste del modelo con la librería ‘tree’</a></li>
<li><a class="nav-link" href="#podado-del-%C3%A1rbol-pruning"><span class="header-section-number">7.9.3</span> Podado del árbol (pruning)</a></li>
<li><a class="nav-link" href="#predicci%C3%B3n-y-evaluaci%C3%B3n-del-modelo"><span class="header-section-number">7.9.4</span> Predicción y evaluación del modelo</a></li>
</ul>
</li>
<li><a class="nav-link" href="#webs"><span class="header-section-number">7.10</span> Webs</a></li>
<li><a class="nav-link" href="#referencias"><span class="header-section-number">7.11</span> Referencias</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/rstudio/bookdown-demo/blob/master/06-Sesion6-Arboles-de-Decision.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/rstudio/bookdown-demo/edit/master/06-Sesion6-Arboles-de-Decision.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Herramientas… MBA</strong>" was written by Fernando López <br> Manuel Reuz. It was last built on 2024-08-09.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
