[["index.html", "Herramientas… MBA Capítulo 1 Presentación", " Herramientas… MBA Fernando López Manuel Reuz 2024-08-09 Capítulo 1 Presentación En el mundo dinámico y competitivo de los negocios, la capacidad de tomar decisiones informadas basadas en datos es crucial. La asignatura “Herramientas Cuantitativo Informáticas para la Toma de Decisiones en la Empresa” está diseñada para equipar a los estudiantes del MBA con las habilidades y conocimientos necesarios para analizar, interpretar y aplicar datos estadísticos de manera efectiva en diversas áreas empresariales. El análisis de la información cuantitativa es una herramienta poderosa que permite a los gerentes y profesionales tomar decisiones basadas en evidencia, identificando tendencias, patrones y relaciones significativas en los datos. Esta asignatura se centra en proporcionar una comprensión sólida de los conceptos estadísticos fundamentales, así como en la aplicación práctica de estas herramientas utilizando software informático especializado. A lo largo esta asignatura, los estudiantes explorarán una amplia gama de temas estadísticos, comenzando con los principios básicos de la estadística descriptiva y la inferencia estadística. Aprenderán a calcular y interpretar medidas de tendencia central y dispersión, a realizar pruebas de hipótesis y a construir intervalos de confianza. Además, se abordarán técnicas más avanzadas como el análisis de regresión, la correlación y la estadística multivariante, proporcionando a los estudiantes una comprensión profunda de cómo estas herramientas pueden ser aplicadas en contextos empresariales. Una parte esencial del curso es la integración de herramientas informáticas cuantitativas, como R, Python y Excel. Estas plataformas no solo facilitan el análisis de grandes volúmenes de datos, sino que también permiten la visualización de resultados de manera clara y concisa. Los estudiantes tendrán la oportunidad de trabajar con datos reales y desarrollar sus habilidades prácticas a través de ejercicios y proyectos que simulan situaciones del mundo real. El lenguaje R, en particular, se destaca como una herramienta potente y versátil para impartir esta asignatura. R es ampliamente reconocido por su capacidad para manejar grandes conjuntos de datos y realizar análisis estadísticos complejos con una eficiencia notable. Además, su vasto ecosistema de paquetes y bibliotecas especializadas permite a los estudiantes aplicar una variedad de técnicas estadísticas avanzadas con facilidad. R también ofrece robustas capacidades de visualización de datos, permitiendo la creación de gráficos y reportes altamente personalizados y de gran impacto visual. La combinación de estas características hace de R una herramienta ideal para enseñar y aplicar estadísticas en un entorno académico y profesional, asegurando que los estudiantes adquieran habilidades prácticas y relevantes para sus futuras carreras. La asignatura también enfatiza la interpretación y comunicación de los resultados estadísticos. Es fundamental que los profesionales no solo sepan cómo realizar análisis, sino también cómo presentar sus hallazgos de manera que sean comprensibles y útiles para la toma de decisiones estratégicas. "],["introducción-a-r.html", "Capítulo 2 Introducción a R 2.1 Objetivos 2.2 El lenguaje R 2.3 ¿Por qué aprender R? 2.4 RStudio 2.5 Qué cosas hago con R 2.6 Creando el entorno de trabajo en RStudio. 2.7 Primeros pasos 2.8 Un for en R 2.9 Leer un Excel 2.10 Actividad 1: 2.11 Actividad 2: 2.12 Actividad 3:", " Capítulo 2 Introducción a R 2.1 Objetivos ¿Qué pretendemos en 4 horas? Conocer un lenguaje de programación (R) extremadamente flexible, gratuito con altas prestaciones. ¿Qué NO pretendemos? Manejar R con soltura Desarrollar programas para manejar grandes volúmenes de datos y analizar procesos usando herramientas de aprendizaje automático…. 2.2 El lenguaje R R es un lenguaje de programación y un entorno de software libre para el análisis estadístico y la visualización de datos. Desarrollado originalmente por Ross Ihaka y Robert Gentleman en la Universidad de Auckland en los años 90, R se ha convertido en una herramienta esencial en la estadística, la ciencia de datos y la investigación académica. Su principal fortaleza reside en su capacidad para manejar, analizar y graficar grandes volúmenes de datos de manera eficiente. R es especialmente valorado por su amplia gama de paquetes y librerías que extienden su funcionalidad básica, permitiendo a los usuarios realizar tareas complejas de análisis de datos, modelado estadístico, minería de datos y aprendizaje automático. Además, la comunidad de usuarios de R es muy activa, contribuyendo constantemente con nuevos paquetes y actualizaciones, lo que mantiene al lenguaje en la vanguardia de las innovaciones en análisis de datos. El entorno de desarrollo integrado más popular para R es RStudio, que proporciona una interfaz amigable y herramientas adicionales que facilitan la escritura y ejecución de código R. La versatilidad de R y su capacidad para integrarse con otros lenguajes de programación y sistemas de bases de datos lo convierten en una opción preferida para analistas, científicos de datos y estadísticos de todo el mundo. 2.3 ¿Por qué aprender R? Aprender a usar R puede ser beneficioso por varias razones, especialmente si estás interesado en la estadística, la ciencia de datos, la investigación científica o cualquier campo relacionado con el análisis de datos. A continuación, se presentan algunas razones para aprender a usar R: Ciencia de Datos: R se ha convertido en una herramienta fundamental en el campo de la ciencia de datos. Es utilizado para la limpieza de datos, la exploración de datos, el aprendizaje automático y la generación de informes. Big Data R es una herramienta poderosa cuando se trata de trabajar con grandes volúmenes de datos, también conocidos como “big data”. A través de sus numerosos paquetes y extensiones, R ofrece capacidades avanzadas para la manipulación, procesamiento y análisis de conjuntos de datos masivos. Su capacidad para cargar, gestionar y realizar cálculos en datos de gran tamaño es fundamental en campos como la ciencia de datos y el análisis estadístico. Además, R se integra bien con herramientas de big data como Hadoop y Spark, lo que permite a los profesionales de datos abordar proyectos que involucran la recopilación y análisis de datos a escala empresarial. En resumen, R es una opción sólida para aquellos que deseen trabajar con big data y obtener información valiosa a partir de conjuntos de datos extensos y complejos. Poderosa Herramienta Estadística R es un lenguaje de programación y un entorno de desarrollo diseñado específicamente para estadísticas y análisis de datos. Ofrece una amplia gama de funciones estadísticas y técnicas de modelado que son esenciales para la investigación y el análisis de datos. Existe una Comunidad Activa R cuenta con una comunidad de usuarios y desarrolladores activa y diversa. Esto significa que hay una gran cantidad de recursos, paquetes y documentación disponibles en línea. Puedes encontrar soluciones para una variedad de problemas y obtener ayuda de la comunidad cuando lo necesites. Paquetes Especializados R tiene una gran cantidad de paquetes diseñados para tareas específicas. Ya sea que necesites realizar análisis de series temporales, gráficos avanzados, aprendizaje automático o análisis bioestadísticos, es probable que encuentres un paquete en R que se adapte a tus necesidades. Es Multiplataforma ¿Usas Windows? Muy bien. ¿Tienes Mac? No hay problema. ¿Eres puro Linux? No pasa nada. R está disponible y funcionando en todos estos sistemas operativos. Visualización de Datos R ofrece capacidades avanzadas de visualización de datos. Puedes crear gráficos y visualizaciones de alta calidad para comunicar tus resultados de manera efectiva. Librerías como ggplot2 son ampliamente utilizadas para crear visualizaciones personalizadas y elegantes. Flexibilidad y Personalización R es altamente personalizable y extensible. Puedes escribir tus propias funciones y paquetes para adaptar R a tus necesidades específicas. Esto es especialmente útil si estás realizando investigaciones originales o trabajando en proyectos específicos. Uso en la Industria y la Academia R es ampliamente utilizado tanto en la industria como en la academia. Aprender R puede abrirte puertas en una variedad de campos, incluyendo la ciencia de datos, la investigación académica, la consultoría y más. Gratuito y de Código Abierto R es un software de código abierto y es gratuito para su uso. Esto lo hace accesible para una amplia gama de usuarios y organizaciones sin incurrir en costos de licencia. Herramientas de Integración Existen numerosas herramientas y entornos que se integran fácilmente con R, como RStudio, que proporciona un entorno de desarrollo amigable y funcionalidad adicional para facilitar la programación en R. Replicabilidad y Documentación: R fomenta la replicabilidad de investigaciones y análisis al permitir que los usuarios documenten sus pasos y resultados de manera efectiva en forma de scripts y documentos R Markdown. 2.4 RStudio RStudio es un entorno de desarrollo integrado (IDE, GUI en ingles) diseñado específicamente para el lenguaje de programación R. Lanzado en 2011, RStudio proporciona una interfaz amigable y herramientas robustas que facilitan la escritura, depuración y ejecución de código R. Entre sus características destacan el editor de scripts con resaltado de sintaxis, la consola interactiva, herramientas para la gestión de proyectos y la visualización integrada de gráficos y datos. Además, RStudio soporta la integración con sistemas de control de versiones como Git y facilita la creación de documentos reproducibles a través de R Markdown. Estas capacidades hacen de RStudio una elección predilecta para estadísticos y científicos de datos. 2.5 Qué cosas hago con R En la empresa, antes era imprescindible saber inglés. Ahora, se da por descontado y cada vez es más importante conocer un lenguaje de programación que te permita manejar información numérica (asociada a los grandes volúmenes de información que se generan) R se ha convertido en mi lenguaje de programación favorito (hay otros) Puedo escribir artículos científicos combinando texto y datos Puedo hacer reproducible la investigación Puedo generar potentes, y llamativos gráficos Hacer gráficos dinámicos e interactivos Integrar lenguajes html, latex, texto plano,.. Generar mapas interactivos Generar páginas webs (como esta!) Manejar millones de observaciones fácilmente! Un lenguaje altamente flexible… 2.6 Creando el entorno de trabajo en RStudio. R es un lenguaje de programación y entorno de software ampliamente utilizado en estadísticas y análisis de datos. Lo que lo hace aún más poderoso es su integración con RStudio, un entorno de desarrollo integrado (IDE, abreviatura de Integrated Development Environment) diseñado específicamente para trabajar con R. RStudio proporciona una interfaz amigable y altamente funcional que facilita la escritura, prueba y depuración de código R. Puedes descargar R aquí . Puedes descarta RStudio aquí https://www.uv.es/vcoll/primeros-pasos.html Trabajamos con la interfaz RStudio antes que con la de R porque es “más amigable”. Un tipo básico de archivo de texto es un script R. Los scripts R tienen la extensión “.R” y reconocen todo el texto como si fuera código R. Cuando trabaja con código R, puede agregar comentarios usando este nombre: “#”. Cualquier línea que comience con esta marca se ignora al ejecutar el código. Una vez estamos en RStudio, podemos escribir y ejecutar las órdenes de varias formas: directamente en la consola a través de un script (.R) con ficheros Rmarkdown (.Rmd) Como podemos ver, RStudio está (normalmente) dividido en 4 paneles. Imagen 2.6.1 Consola Por defecto, la consola se encuentra en el panel inferior-izquierdo. ¿Vemos la pestaña que pone Console? Inmediatamente debajo aparece un texto informativo y, finalmente, el símbolo “&gt;”. Aquí es donde R espera que le demos instrucciones. Para ejecutarlas y obtener el resultado pulsamos enter. Vamos a hacer este ejemplo: 2+2 #&gt; [1] 4 5*(3-1)^2 #&gt; [1] 20 sqrt(4) #&gt; [1] 2 En el ejemplo anterior se han ido introduciendo y ejecutando las instrucciones una a una. También es posible ejecutar desde la consola más de una instrucciones. Para ello, las instrucciones deben separarse con un “;”. 2 + 2 ; 5*(3-1)^2 ; sqrt(4) #&gt; [1] 4 #&gt; [1] 20 #&gt; [1] 2 2.6.2 Scripts Trabajar en la consola es muy limitado ya que las instrucciones se han de introducir una a una. Lo habitual es trabajar con scripts o ficheros de instrucciones. Estos ficheros tienen extensión .R. Se puede crear una script con cualquier editor de texto, pero nosotros lo haremos desde RStudio. Para ello, seleccionamos la siguiente ruta de menús: File &gt; New File &gt; R script El panel del script se sitúa en la parte superior-izquierda de RStudio. Ahora podemos escribir las instrucciones línea por línea. Las instrucciones las podemos ejecutar una a una o las podemos seleccionar y ejecutar en bloque. Para ejecutar las instrucciones tenemos varias alternativas: Hacemos clic en el botón: Run (botón situado en la parte derecha de las opciones del panel de script) Pulsamos Ctrl+r 2.6.3 Entorno El panel, llamémoslo de entorno esta compuesto de varias pestañas: Environment History Otras cosas … En el “Environment” se irán registrando los objetos que vayamos creando en la sesión de trabajo. También tenemos la opción de cargar y guardar una sesión de trabajo, importar datos y limpiar los objetos de la sesión. Estas opciones están accesibles a través de la cinta de opciones de la pestaña. 2.6.4 Miscelánea: Archivos, Gráficos, Paquetes, Ayuda, Visor Con el nombre de Misceléna nos referimos al otro panel (que se encuentra en la parte inferior-derecha) del escritorio de RStudio. En este panel cabe destacar las siguientes pestañas, cada una con diferentes opciones: Files: es una especie de explotador de ficheros. Plots: donde se visualizan los gráficos que creamos. Entre las opciones disponibles se encuentran: Zoom: para agrandar el gráfico y verlo en otra ventana. Export: para exportar/guardar el gráfico. Se puede guardar el gráfico como imagen, pdf o copiarlo al portapapeles. Packages: proporciona un listado de los paquetes instalados en R y los que han sido cargado en la sesión. A través de las opciones de esta pestaña podemos instalar nuevos paquetes o actualizar los existentes. Help: Para obtener ayuda sobre una determinada función. Otros: 2.6.5 Configuración del directorio de trabajo. Antes de comenzar a trabajar debemos fijar el directorio donde queremos guardar nuestros ficheros. Básicamente, dos alternativas. 2.6.5.1 Opción 1. Fijar directorio. Opción 1. Indicamos a R la ruta donde queremos trabajar y la fijamos con la función setwd(). setwd(“C:/ruta del directorio de trabajo”) Para comprobar el directorio de trabajo utilizamos la función getwd(): getwd() Para obtener un listado de los ficheros que contiene la ruta establecida se usa la función dir(). dir() 2.6.5.2 Opción 2. Proyecto de R. Al crear un proyecto todos los ficheros quedan vinculados directamente al proyecto. Para crear un proyecto selección File &gt; New project… Se abrirá un menú que guia en la generación del proyecto Para crear un proyecto en un nuevo directorio, hacemos clic en el botón New Directory. Seguidamente, seleccionamos el tipo de proyecto, en nuestro caso Empty Project. Ahora, asignamos un nombre al directorio (carpeta) que se va a crear y que al mismo tiempo será el nombre del proyecto de R. Para terminar, hacemos clic en el botón Create Project. Al seguir este proceso se habrá creado una carpeta en Documentos y un fichero nombre_carpeta.Rproj. Para crear un proyecto en una carpeta que ya existe, hacemos clic en el botón Existing Directory y después seleccionamos la carpeta ayudándonos del Browse.. si fuera necesario. Una vez elegida la carpeta, clicamos en Create Project. Para abrir un proyecto hacemos doble clic sobre el archivo con extensión .Rproj o lo abrimos desde el menú de RStudio: File &gt; Open Project… Ventaja de los proyectos: cualquier fichero que creemos (script de R, documento de Rmarkdown, etc.) y guardemos se guardará en la carpeta del proyecto. 2.6.6 Instalar y cargar paquetes. R está compuesto por un sistema base, pero para extender su funcionalidad es necesario instalar paquetes adicionales. Podemos instalar paquetes de varias formas: A través del menú: Tools &gt; Install packages… En el escritorio de RStudio: Packages/Install. Vemos los paquetes que tenemos actualmente instalados y aquellos que se encuentran cargados. Utilizando la función install.packages(). El nombre del paquete que queremos instalar debe ir entre comillas. # dplyr es un paquete que se utiliza para manipular/gestionar datos # install.packages(&quot;dplyr&quot;) En ocasiones, para nuestra sesión de trabajo necesitamos instalar varios paquetes. # install.packages(c(&quot;dplyr&quot;,&quot;ggplot2&quot;)) Es habitual iniciar la sesión de trabajo en R con un “pequeño programa” en el que se indica que para la sesión se requiere una serie de paquetes y que si no están instalados los instale. Aquí tenemos la versión más sencilla para hacer esto if(!require(dplyr)) {install.packages(&quot;dplyr&quot;)} Una vez instalado el paquete, hay que cargarlo para poderlo utilizar. Esto se hace con la función library(). library(dplyr) # observad que el nombre del paquete no se pone entre comillas para cargarlo. 2.6.7 Ayuda en R. En muchas ocasiones necesitamos ayuda sobre cómo funciona una determinada función, cuáles son sus argumentos, etc. Hay varias formas de pedir la ayuda de R. Vamos a pedir la ayuda de la función mean(). help(mean) ?mean Si ejecutamos directamente la función library() se abrirá una ventana listando los paquetes que tenemos instalados en R. En el escritorio de RStudio, en la pestaña Packages también tenemos en listado de paquetes instalados (organizados en dos bloques: User Library y System Library) library() Para obtener ayuda sobre un determinado paquete… library(help=“foreign”) En ocasiones junto con los paquetes se facilita una documentación sobre su uso, a esto se le llama vignettes. vignette() # Para ver una lista de las vignettes a las que podemos acceder por paquete y a una vignette concreta de un paquete: Pero sin duda, una de las mejores fuentes de ayuda en R nos la proporciona internet. Bien haciendo directamente en google la búsqueda sobre el tema que estamos interesados, bien acudiendo a algunas de las muchas webs que ofrecen ayuda. Algunas de las más populares y recomendables webs son: https://es.stackoverflow.com/ https://stackoverflow.co/ https://chat.openai.com/ 2.7 Primeros pasos 2.7.1 Script básico para apreder R # Comentarios en R se realizan con el símbolo &#39;#&#39; al principio de la línea # Asignación de variables x &lt;- 5 # Asigna el valor 5 a la variable x y &lt;- 3 # Asigna el valor 3 a la variable y # Realizar cálculos suma &lt;- x + y # Suma x e y y guarda el resultado en la variable suma resta &lt;- x - y # Resta y de x y guarda el resultado en la variable resta producto &lt;- x * y # Multiplica x por y y guarda el resultado en la variable producto division &lt;- x / y # Divide x por y y guarda el resultado en la variable division # Imprimir resultados cat(&quot;La suma es:&quot;, suma, &quot;\\n&quot;) #&gt; La suma es: 8 cat(&quot;La resta es:&quot;, resta, &quot;\\n&quot;) #&gt; La resta es: 2 cat(&quot;El producto es:&quot;, producto, &quot;\\n&quot;) #&gt; El producto es: 15 cat(&quot;La división es:&quot;, division, &quot;\\n&quot;) #&gt; La división es: 1.666667 # Crear un vector de números del 1 al 10 mi_vector &lt;- 1:10 # Imprimir el vector cat(&quot;Mi vector:&quot;, mi_vector, &quot;\\n&quot;) #&gt; Mi vector: 1 2 3 4 5 6 7 8 9 10 # Crear un gráfico de dispersión simple plot(mi_vector, mi_vector, main=&quot;Gráfico de Dispersión&quot;, xlab=&quot;Eje X&quot;, ylab=&quot;Eje Y&quot;, col=&quot;blue&quot;, pch=19) # Guardar el gráfico en un archivo PNG png(filename=&quot;grafico.png&quot;) plot(mi_vector, mi_vector, main=&quot;Gráfico de Dispersión&quot;, xlab=&quot;Eje X&quot;, ylab=&quot;Eje Y&quot;, col=&quot;blue&quot;, pch=19) dev.off() # Cerrar el archivo PNG #&gt; quartz_off_screen #&gt; 2 2.7.2 Tipos de Datos en R En R, existen varios tipos de datos que se utilizan para almacenar y manipular información. A continuación, se presentan algunos de los tipos de datos más comunes en R: Numérico (numeric): Representa números reales o decimales. Por ejemplo, x &lt;- 3.14. Entero (integer): Representa números enteros. Por ejemplo, y &lt;- 5L (la “L” indica que es un número entero). Caracteres (character): Almacena texto o cadenas de caracteres. Se utiliza comillas simples o dobles. Por ejemplo, nombre &lt;- \"Juan\". Lógico (logical): Representa valores lógicos Verdadero (TRUE) o Falso (FALSE). Se utiliza para evaluaciones condicionales. Por ejemplo, es_mayor &lt;- TRUE. Factor: Se utiliza para representar datos categóricos o variables cualitativas. Los factores tienen niveles que indican las categorías. Por ejemplo, genero &lt;- factor(c(\"Masculino\", \"Femenino\")). Fecha y hora: R tiene tipos de datos específicos para fechas y horas, como Date y POSIXct, que permiten realizar operaciones y cálculos con fechas y horas. Lista (list): Permite almacenar una colección heterogénea de objetos (como vectores, matrices u otras listas) en una sola estructura de datos. Por ejemplo, mi_lista &lt;- list(1, \"texto\", c(2, 3, 4)). Matriz (matrix): Es una estructura bidimensional que almacena datos del mismo tipo en filas y columnas. Por ejemplo, matriz &lt;- matrix(1:6, nrow = 2, ncol = 3). Arreglo (array): Similar a una matriz, pero puede tener más de dos dimensiones. Por ejemplo, mi_arreglo &lt;- array(1:12, dim = c(2, 3, 2)). DataFrame: Es similar a una matriz, pero puede contener diferentes tipos de datos en cada columna. Los data frames son muy utilizados para almacenar conjuntos de datos. Por ejemplo, mi_df &lt;- data.frame(nombre = c(\"Juan\", \"María\"), edad = c(25, 30)). NULL: Representa la ausencia de valor o datos faltantes. Por ejemplo, si una variable no tiene un valor asignado, se considera NULL. Infinito y NaN: R también tiene representaciones especiales para el infinito (Inf o -Inf) y para valores indefinidos (NaN, que significa “No es un número”). Estos son algunos de los tipos de datos más comunes en R. Es importante comprender cómo trabajar con cada tipo de dato, ya que es fundamental para el análisis y la manipulación de datos en R. 2.7.3 La Importancia de los Objetos en R Uno de los conceptos fundamentales en R es el uso de objetos para almacenar y manipular datos. Los objetos son contenedores que almacenan información, como números, texto, vectores, matrices, data frames y más. Comprender la importancia de los objetos es esencial para trabajar de manera efectiva en R. A continuación, se destacan algunas razones por las cuales los objetos son fundamentales: Organización de Datos: Los objetos permiten organizar los datos de manera estructurada. Por ejemplo, puedes almacenar datos en un vector, una matriz o un data frame, lo que facilita la gestión y la manipulación de la información. Reutilización: Los objetos se pueden reutilizar en múltiples operaciones y análisis. Puedes crear un objeto con datos y luego realizar diversas operaciones estadísticas o gráficas sin necesidad de volver a cargar los datos cada vez. Claridad y Documentación: El uso de objetos con nombres descriptivos mejora la claridad del código. Puedes asignar nombres significativos a los objetos, lo que facilita la comprensión del código y su documentación. Programación Modular: Los objetos permiten dividir un problema en partes más pequeñas y manejables. Puedes crear funciones que operen sobre objetos específicos, lo que promueve la programación modular y la reutilización de código. Interacción con Paquetes: Muchos paquetes y funciones en R trabajan con objetos específicos. Al comprender cómo funcionan estos objetos, puedes aprovechar al máximo la funcionalidad de los paquetes y realizar análisis avanzados. Visualización y Gráficos: Los objetos pueden contener datos que se utilizan para crear visualizaciones y gráficos. Los paquetes de gráficos en R, como ggplot2, se basan en objetos para generar gráficos personalizados. Análisis Estadístico: Los objetos son esenciales para realizar análisis estadísticos en R. Puedes aplicar pruebas, modelos y métodos estadísticos a los datos almacenados en objetos. En resumen, los objetos son la base de la programación en R y desempeñan un papel crucial en el análisis de datos y la creación de visualizaciones. Comprender cómo trabajar con objetos y cómo seleccionar el tipo adecuado de objeto para tus datos es esencial para aprovechar al máximo las capacidades de R. 2.7.4 DataFrames en R Un DataFrame en R es una estructura de datos bidimensional que se utiliza para almacenar y organizar datos de manera tabular. Cada columna de un DataFrame puede contener un tipo de dato diferente, como numérico, de caracteres, lógico, etc. Los DataFrames son muy utilizados para trabajar con conjuntos de datos y realizar análisis de datos en R. 2.7.5 Ejemplo de DataFrame Supongamos que queremos crear un DataFrame para almacenar información sobre estudiantes, incluyendo sus nombres, edades y calificaciones en dos materias: Matemáticas y Ciencias. Podemos crear un DataFrame de la siguiente manera: # Crear un DataFrame de ejemplo estudiantes &lt;- data.frame( Nombre = c(&quot;Juan&quot;, &quot;María&quot;, &quot;Carlos&quot;, &quot;Ana&quot;), Edad = c(25, 30, 22, 28), Matematicas = c(90, 85, 78, 92), Ciencias = c(88, 92, 76, 89) ) # Mostrar el DataFrame estudiantes #&gt; Nombre Edad Matematicas Ciencias #&gt; 1 Juan 25 90 88 #&gt; 2 María 30 85 92 #&gt; 3 Carlos 22 78 76 #&gt; 4 Ana 28 92 89 Cómo identificar los elementos de un dataframe estudiantes$Edad #&gt; [1] 25 30 22 28 estudiantes[,2] #&gt; [1] 25 30 22 28 estudiantes[1,] #&gt; Nombre Edad Matematicas Ciencias #&gt; 1 Juan 25 90 88 2.8 Un for en R En R, un bucle for se utiliza para repetir una serie de instrucciones un número específico de veces o para iterar sobre una secuencia de elementos, como vectores o listas. Aquí tienes un ejemplo simple de cómo se usa un bucle for para imprimir los números del 1 al 5. # Ejemplo de bucle for para imprimir números del 1 al 5 for (i in 1:5) { print(i) } #&gt; [1] 1 #&gt; [1] 2 #&gt; [1] 3 #&gt; [1] 4 #&gt; [1] 5 En este ejemplo, utilizaremos un bucle for para generar una serie de datos y almacenarlos en un DataFrame. Supongamos que queremos calcular y almacenar los primeros diez números pares en un DataFrame. Utilizaremos un bucle for para generar estos números y luego los almacenaremos en un DataFrame. # Crear un DataFrame vacío mi_df &lt;- data.frame(NumerosPares = numeric(0)) # Usar un bucle for para generar números pares y almacenarlos for (i in 1:10) { numero_par &lt;- 2 * i mi_df &lt;- rbind(mi_df,numero_par) } # Mostrar el DataFrame resultante mi_df #&gt; X2 #&gt; 1 2 #&gt; 2 4 #&gt; 3 6 #&gt; 4 8 #&gt; 5 10 #&gt; 6 12 #&gt; 7 14 #&gt; 8 16 #&gt; 9 18 #&gt; 10 20 2.9 Leer un Excel 2.9.1 Datos Empleados Datos Empleados RStudio permite cargar datos a través de menús (File &gt; Import Dataset). Por menús se pueden cargar datos CSV, EXCEL, SPSS, SAS y STATA. library(readxl) myxls &lt;- read_xlsx(&quot;Datos/Datos_de_empleados.xlsx&quot;) head(myxls) #&gt; # A tibble: 6 × 10 #&gt; id sexo fechnac educ catlab salario salini tiempemp #&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 1 h 19027 15 3 57000 27000 98 #&gt; 2 2 h 21328 16 1 40200 18750 98 #&gt; 3 3 m 10800 12 1 21450 12000 98 #&gt; 4 4 m 17272 8 1 21900 13200 98 #&gt; 5 5 h 20129 15 1 45000 21000 98 #&gt; 6 6 h 21419 15 1 32100 13500 98 #&gt; # ℹ 2 more variables: expprev &lt;dbl&gt;, minoría &lt;dbl&gt; myxls$fechnac &lt;- as.Date(as.numeric(myxls$fechnac)) #&gt; Warning in as.Date(as.numeric(myxls$fechnac)): NAs #&gt; introduced by coercion 2.9.2 Leer un csv Al igual que los archivos de excel, R puede leer multitud de formatos. En general para leer cada formato es necesario disponer del paquete que hace esa función. Los archivos con extensión csv son muy comunes para la transferencia de bases de datos. Por ejemplo read.csv() es una función que lee este formato CSV significa “comma separated data”. En realidad CSV es un caso particular de “tabular o text data” 2.10 Actividad 1: El objetivo de esta actividad es familiarizarse con los conceptos básicos de R y practicar algunas operaciones simples. Instalación de Paquetes: Instala el paquete stringr usando el siguiente comando: Carga el paquete stringr Explora la viñeta y investiga sobre las cosas que hace Usa los ejemplos que aparecen en la ayuda de la funció ‘str_sub()’ Creación de un DataFrame: Crea un DataFrame llamado datos con tres columnas: Nombre, Edad y Puntuación. Llena el DataFrame con al menos 5 filas de datos ficticios. Operaciones Básicas: Calcula la media de las edades en el DataFrame. Encuentra la persona con la puntuación más alta. Filtra las filas para mostrar solo las personas mayores de 25 años. Ordena el DataFrame por nombre en orden alfabético. Visualización de Datos (Opcional): Si te sientes cómodo, intenta visualizar alguna característica de tu DataFrame, como un gráfico de barras para las puntuaciones. 2.11 Actividad 2: El objetivo de esta actividad es practicar el uso del bucle for en R para generar tablas de multiplicar. Elige un número para el cual quieres generar la tabla de multiplicar. Puede ser cualquier número entero positivo. Utiliza un bucle for para generar la tabla de multiplicar del número seleccionado. La tabla debe incluir multiplicaciones del número del 1 al 10. Muestra los resultados en forma de una tabla que tenga dos columnas: una para el multiplicador y otra para el resultado de la multiplicación. 2.12 Actividad 3: El objetivo es descargar un fichero de Internet y procesarlo Descargar el excel de esta web https://econet.carm.es/web/crem/inicio/-/crem/sicrem/PU_padron/series/sec4_sec2.html Manejar el excel para que en la primera fila aparezca el nombre de la variable y en cada fila siguiente la población de cada municipio. Eliminar la fila de Total. Importar el fichero a R Calcular la suma de población en la CARM cada año usando la función sum() "],["estadística-con-r.html", "Capítulo 3 Estadística con R 3.1 Resumen estadístico 3.2 Gráficos con R 3.3 Test Hipótesis 3.4 Distribuciones bivariadas", " Capítulo 3 Estadística con R R es un lenguaje especializado en implementar técnicas estadísticas. Utilizaremos R para realizar un análisis estadístico básico. 3.1 Resumen estadístico El archivo “datos de empleados.sav” de SPSS es un conjunto de datos ampliamente utilizado en análisis estadísticos y gestión de recursos humanos. Este archivo contiene información detallada sobre los empleados de una organización, abarcando diversas variables como edad, género, departamento, puesto de trabajo, salario, años de servicio, nivel educativo y rendimiento laboral. Gracias a esta riqueza de datos, los analistas pueden explorar patrones y tendencias dentro de la fuerza laboral, lo que facilita la toma de decisiones informadas en áreas cruciales como la contratación, capacitación, evaluación del desempeño y retención de empleados. Por ejemplo, los datos pueden utilizarse para identificar diferencias salariales entre distintos géneros o departamentos, analizar la relación entre la antigüedad y el rendimiento laboral, o evaluar el impacto de la formación en el desarrollo profesional. Además, la estructura del archivo “datos de empleados.sav” permite la aplicación de diversas técnicas estadísticas y métodos de análisis, como regresión, análisis de varianza y pruebas de hipótesis, proporcionando un enfoque robusto para abordar cuestiones complejas en la gestión de recursos humanos. Datos Empleados Las funciones más elementales tienen una sintaxis muy intuitiva library(readxl) bbdd &lt;- read_xlsx(&quot;Datos/Datos_de_empleados.xlsx&quot;) # bbdd$fechnac &lt;- as.Date(as.numeric(bbdd$fechnac),format = &quot;%Y%m%d&quot;) knitr::kable(head(bbdd[,c(2,6:7)],6)) sexo salario salini h 57000 27000 h 40200 18750 m 21450 12000 m 21900 13200 h 45000 21000 h 32100 13500 Para calcular algunos indicadores básicos se utilizan comandos sencillos &gt; # Min, max,... &gt; min(bbdd$salario) #&gt; [1] 15750 &gt; max(bbdd$salario) #&gt; [1] 135000 &gt; which.max(bbdd$salario) #&gt; [1] 29 &gt; bbdd$salario[which.max(bbdd$salario)] #&gt; [1] 135000 &gt; sum(bbdd$salario) #&gt; [1] 16314875 3.1.1 Medidas de posición central Las medidas de tendencia central son estadísticas fundamentales que describen el punto medio o típico de un conjunto de datos. Entre las más utilizadas se encuentran la media aritmética, la mediana y la moda. La media aritmética, o promedio, se calcula sumando todos los valores y dividiéndolos por el número total de observaciones, proporcionando una medida del centro basada en todos los datos. La mediana, en cambio, es el valor que divide el conjunto de datos en dos mitades iguales, siendo especialmente útil cuando hay valores atípicos o distribuciones sesgadas, ya que no se ve afectada por extremos. La moda es el valor que aparece con mayor frecuencia en el conjunto de datos y puede ser útil para identificar el valor más común en una distribución. Cada una de estas medidas ofrece una perspectiva diferente sobre la centralidad de los datos y se elige según las características del conjunto de datos y el contexto del análisis &gt; # Media y Mediana de Salario &gt; mean(bbdd$salario) #&gt; [1] 34419.57 &gt; median(bbdd$salario) #&gt; [1] 28875 &gt; &gt; # Es fácil calcular la media geométrica &gt; x &lt;- bbdd$salario &gt; n &lt;- length(x) &gt; prod(x)^(1/n) #&gt; [1] Inf &gt; geom &lt;- exp(mean(log(x))) &gt; print(geom) #&gt; [1] 31470.09 &gt; &gt; # También la media armónica &gt; armo &lt;- 1/mean(1/x) &gt; print(armo) #&gt; [1] 29366.07 Algunos detalles sobre valores perdidos En R, los valores faltantes están representados por el símbolo NA (no disponible). Los valores imposibles (por ejemplo, la división por cero) están representados por el símbolo NaN (no es un número) &gt; x &lt;- c(1,2,NA,3) &gt; mean(x) # devuelve NA #&gt; [1] NA &gt; sum(x, na.rm=TRUE) # devuelve 2 #&gt; [1] 6 &gt; mean(x, na.rm=TRUE) # devuelve 2 #&gt; [1] 2 Igualmente se obtien en medidas de posición no central Las medidas de posición no central, como los cuartiles, deciles y percentiles, son herramientas estadísticas que dividen un conjunto de datos en partes iguales para analizar su distribución. Los cuartiles dividen los datos en cuatro partes: el primer cuartil (Q1) indica el 25% inferior, el segundo cuartil (Q2) es la mediana, y el tercer cuartil (Q3) representa el 75% inferior. Los deciles y percentiles proporcionan divisiones más finas, en diez y cien partes, respectivamente. quantile(bbdd$salario) 0% 25% 50% 75% 100% 15750.0 24000.0 28875.0 36937.5 135000.0 quantile(bbdd$salario,c(0,.15,.85)) 0% 15% 85% 15750.0 22050.0 50027.5 3.1.2 Medidas de dispersión absolutas Las medidas de dispersión absolutas cuantifican la variabilidad o dispersión de un conjunto de datos respecto a su centro. Entre las más comunes se encuentran el rango, la desviación media y la desviación estándar. El rango es la diferencia entre el valor máximo y el mínimo, proporcionando una medida simple de la extensión de los datos. La desviación media calcula el promedio de las diferencias absolutas entre cada dato y la media, ofreciendo una visión general de la dispersión. La desviación estándar mide la dispersión de los datos respecto a la media. &gt; # Medidas de dispersión absolutas &gt; range(bbdd$salario) #&gt; [1] 15750 135000 &gt; IQR(bbdd$salario) # Rango intercuartílico #&gt; [1] 12937.5 &gt; sd(bbdd$salario) # Desviación estándar #&gt; [1] 17075.66 &gt; var(bbdd$salario) # Varianza #&gt; [1] 291578214 3.1.3 Medidas de asimetría y curtosis Las medidas de asimetría y curtosis evalúan la forma y la distribución de un conjunto de datos. La asimetría indica si los datos están sesgados hacia un lado: una asimetría positiva sugiere una cola larga a la derecha, mientras que una negativa indica una cola larga a la izquierda. La curtosis mide la “agudeza” de la distribución: una curtosis alta (leptocúrtica) indica colas más pesadas y un pico más pronunciado, mientras que una baja (platicúrtica) señala colas más ligeras y un pico más plano. Estas medidas son cruciales para comprender la distribución de los datos más allá de la media y la dispersión if(!require(moments)) {install.packages(&quot;moments&quot;)} #&gt; Loading required package: moments library(moments) skewness(bbdd$salario) #nos da el valor de la asimetria de los datos de la variable x [1] 2.117877 kurtosis(bbdd$salario) #nos da el achatamiento de la distribucion de los datos de la variable x. [1] 8.30863 3.1.4 La función ‘summary()’ La función ‘summary()’ en R proporciona un resumen estadístico de un objeto, como un conjunto de datos o un modelo. Para data frames, incluye medidas como la media, mediana, mínimos, máximos y cuartiles, ofreciendo una visión rápida y completa de las características clave del conjunto de datos # Resumen de la variable summary(bbdd$salario) #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 15750 24000 28875 34420 36938 135000 # También para una data frame BBDD &lt;- as.data.frame(bbdd[,c(3,6:7)]) summary(BBDD) #&gt; fechnac salario salini #&gt; Length:474 Min. : 15750 Min. : 9000 #&gt; Class :character 1st Qu.: 24000 1st Qu.:12488 #&gt; Mode :character Median : 28875 Median :15000 #&gt; Mean : 34420 Mean :17016 #&gt; 3rd Qu.: 36938 3rd Qu.:17490 #&gt; Max. :135000 Max. :79980 # Un summary por categorías # Añadimos variable al data frame bbdd$MiVariable &lt;- floor(bbdd$tiempemp/30) by(bbdd$salario, bbdd$MiVariable, summary) #&gt; bbdd$MiVariable: 2 #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 15750 23438 27900 33087 35250 103500 #&gt; --------------------------------------------- #&gt; bbdd$MiVariable: 3 #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; 16200 25200 30825 38021 40875 135000 3.1.5 Actividad Importar datos del archivo “datos de empleados”: Calcular medidas de tendencia central (media y mediana) y de dispersión (rango, desviación estándar) para una variable numérica, por ejemplo, el salario. Calcular medidas de asimetría y curtosis para la misma variable. 3.2 Gráficos con R Las gráficas son la mejor forma de simplificar lo complejo. Un buen gráfico suele ser más accesible que una tabla. Sin embargo es muy importante tener claro qué gráfico queremos hacer. Las facilidades gráficas de R constituyen una de las componentes más importantes de este lenguaje. R incluye muchas y muy variadas funciones para hacer gráficas estadísticas estándar: desde gráficos muy simples a figuras de gran calidad para incluir en artículos y libros. Permite además construir otras nuevas a la medida del usuario (aunque a veces hacer cosas simples no es fácil). Permite exportar gráficas en distintos formatos: PDF, JPEG, GIF, etc. Para ver una demo de gráficos con colores: demo(graphics). Aquí únicamente veremos algunas de todas las posibilidades. Otra alternativa son los paquetes: ggplot y ggplot2. La función plot() La función plot() en R se utiliza para crear gráficos básicos. Es fundamental para visualizar datos, permitiendo la generación de gráficos de dispersión, líneas, barras, y más. Con diversos argumentos y opciones de personalización, plot() facilita el análisis visual y la interpretación de conjuntos de datos complejos. &gt; x &lt;- (0:100)/10 &gt; y &lt;- sin(x) &gt; plot(x, y, main=&quot;Función Seno&quot;) ** Algunas opciones de la función plot()** main: Cambia el título del gráfico sub: Cambia el subtítulo del gráfico type: Tipo de gráfico (puntos, líneas, etc.) xlab, ylab: Cambia las etiquetas de los ejes xlim, ylim: Cambia el rango de valores de los ejes lty: Cambia el tipo de línea; lwd: Cambia el grosor de línea col: Color con el que dibuja plot(x, y, main=&quot;Seno&quot;, type=&quot;l&quot;) plot(x, y, main=&quot;Función Seno&quot;, lty=2, col=&quot;red&quot;, type=&quot;l&quot;) plot(x, cos(x), main=&quot;Función Coseno&quot;, lty=3, col=&quot;blue&quot;, type=&quot;l&quot;,xlim=c(0, 2), ylab=&quot;cos(x)&quot;) Gráfico de sectores pie(c(3,5,8)) pie(c(3,5,8), labels=c(&quot;Uno&quot;,&quot;Dos&quot;,&quot;Tres&quot;),col=c(&quot;blue&quot;,&quot;red&quot;,&quot;green&quot;),main=&quot;Mi gráfico&quot;) La función boxplot realiza este clásico gráfico La función boxplot() en R se usa para crear gráficos de caja, que visualizan la distribución de un conjunto de datos a través de sus cuartiles. Muestra la mediana, los cuartiles y los posibles valores atípicos, permitiendo identificar la dispersión, simetría y anomalías en los datos # Con una sola variable boxplot(bbdd$salario,col=c(&#39;powderblue&#39;)) # Con las tres variables a la vez boxplot(bbdd$salario,bbdd$salini,col=c(&#39;powderblue&#39;,&#39;#FF6D0099&#39;)) # Los dos juntos par(mfrow=c(1,2)) boxplot(bbdd$salario,bbdd$salini,col=c(&#39;powderblue&#39;,&#39;#FF6D0099&#39;)) boxplot(log(bbdd$salario),log(bbdd$salini),col=c(&#39;powderblue&#39;,&#39;#FF6D0099&#39;)) Escribe colors() para una lista de colores 3.2.1 ggplot2 El paquete ggplot2 en R es una herramienta poderosa para la creación de gráficos y visualizaciones de datos. Basado en la gramática de gráficos de Hadley Wickham, ggplot2 permite construir visualizaciones complejas de manera flexible y coherente. Utiliza un sistema de capas, donde cada capa representa una parte del gráfico, como los datos, los estéticos, y los elementos geométricos. Los usuarios pueden personalizar gráficos con facilidad, añadiendo títulos, etiquetas, y temas. Además, ggplot2 facilita la visualización de relaciones entre variables, distribuciones, y patrones, convirtiéndose en una opción popular para el análisis exploratorio de datos y la comunicación de resultados library(ggplot2) Distr &lt;- as.factor(bbdd$sexo) qplot( x=Distr , y=salario , data=bbdd , geom=c(&quot;boxplot&quot;,&quot;jitter&quot;) , fill=Distr) + theme_bw() #&gt; Warning: `qplot()` was deprecated in ggplot2 3.4.0. #&gt; This warning is displayed once every 8 hours. #&gt; Call `lifecycle::last_lifecycle_warnings()` to see where #&gt; this warning was generated. Distr &lt;- as.factor(bbdd$catlab) qplot( x=Distr , y=salario , data=bbdd , geom=c(&quot;boxplot&quot;,&quot;jitter&quot;) , fill=Distr) + theme_bw() Con ggplot se pueden hacer boxplots muy bonitos [https://www.rstudio.com/wp-content/uploads/2015/03/ggplot2-cheatsheet.pdf] También violin-plots [https://www.data-to-viz.com/caveat/boxplot.html] Histograma hist(log(bbdd$salario), breaks=20, col = &quot;red&quot;) Se puede incluir la curva normal sobre el histograma g &lt;- log(bbdd$salario) m &lt;- mean(g) std&lt;-sqrt(var(g)) hist(g, density=10, breaks=20, prob=TRUE,col=&quot;blue&quot;, xlab=&quot;log(Renta)&quot;, main=&quot;Curva normal sobre histograma&quot;) curve(dnorm(x, mean=m, sd=std), col=&quot;red&quot;, lwd=4, add=TRUE) 3.2.2 Actividad Crear un histograma ggplot(empleados, aes(x = salario)) + geom_histogram(binwidth = 1000, fill = &quot;blue&quot;, color = &quot;black&quot;) + labs(title = &quot;Distribución del Salario de los Empleados&quot;, x = &quot;Salario&quot;, y = &quot;Frecuencia&quot;) + theme_minimal() Crear un gráfico de caja (boxplot) para comparar los salarios por departamento ggplot(empleados, aes(x = departamento, y = salario, fill = departamento)) + geom_boxplot() + labs(title = &quot;Distribución del Salario por Departamento&quot;, x = &quot;Departamento&quot;, y = &quot;Salario&quot;) + theme_minimal() 3.3 Test Hipótesis 3.3.1 Un simple t-test de igualdad de medias El contraste de medias basado en la t de Student es una técnica estadística para comparar las medias de dos grupos y determinar si hay una diferencia significativa entre ellas. Se utiliza cuando los datos siguen una distribución normal y el tamaño de la muestra es pequeño. La prueba t calcula el valor t, que se compara con un valor crítico de la distribución t para decidir si se rechaza la hipótesis nula de igualdad de medias. Existen dos tipos principales: la prueba t para muestras independientes, que compara dos grupos distintos, y la prueba t para muestras dependientes, que compara dos mediciones en el mismo grupo # Contrastar si la Renta media de 2015 es 40000 euros t.test(bbdd$salario,mu=40000) One Sample t-test data: bbdd$salario t = -7.1151, df = 473, p-value = 4.168e-12 alternative hypothesis: true mean is not equal to 40000 95 percent confidence interval: 32878.40 35960.73 sample estimates: mean of x 34419.57 t.test(bbdd$salario,mu=35000,alternative=&quot;greater&quot;) One Sample t-test data: bbdd$salario t = -0.74005, df = 473, p-value = 0.7702 alternative hypothesis: true mean is greater than 35000 95 percent confidence interval: 33126.96 Inf sample estimates: mean of x 34419.57 Puede usar la opción var.equal = TRUE para especificar varianzas iguales y una estimación de varianza agrupada. Puede usar la opción alternative = “less” o alternative = “greater” para especificar una prueba de una cola 3.3.2 Un simple t-test de igualdad de medias pareadas t.test(bbdd$salario,bbdd$salini) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: bbdd$salario and bbdd$salini #&gt; t = 20.152, df = 665.3, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 15707.74 19099.22 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 34419.57 17016.09 t.test(log(bbdd$salario),log(bbdd$salini)) #&gt; #&gt; Welch Two Sample t-test #&gt; #&gt; data: log(bbdd$salario) and log(bbdd$salini) #&gt; t = 28.163, df = 932.96, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true difference in means is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.6394884 0.7352878 #&gt; sample estimates: #&gt; mean of x mean of y #&gt; 10.356793 9.669405 Un poco mas complicado: ANOVA de un factor El ANOVA de un factor (Análisis de Varianza) es una técnica estadística utilizada para comparar las medias de tres o más grupos independientes y determinar si existen diferencias significativas entre ellas. Este análisis evalúa si la variabilidad entre las medias de los grupos es mayor que la variabilidad dentro de los grupos. El ANOVA de un factor calcula un estadístico F, que compara la variabilidad entre los grupos con la variabilidad dentro de los grupos. Si el valor F es suficientemente alto, se rechaza la hipótesis nula de igualdad de medias. Es útil para experimentos con un único factor categórico y múltiples niveles. group &lt;- as.factor(bbdd$catlab) levels(group) &lt;- c(&quot;asa&quot;,&quot;aaas&quot;,&quot;asddd&quot;) anova &lt;- aov(bbdd$salario ~ group, data = bbdd) summary(anova) #&gt; Df Sum Sq Mean Sq F value Pr(&gt;F) #&gt; group 2 8.944e+10 4.472e+10 434.5 &lt;2e-16 *** #&gt; Residuals 471 4.848e+10 1.029e+08 #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 TukeyHSD(anova) #&gt; Tukey multiple comparisons of means #&gt; 95% family-wise confidence level #&gt; #&gt; Fit: aov(formula = bbdd$salario ~ group, data = bbdd) #&gt; #&gt; $group #&gt; diff lwr upr p adj #&gt; aaas-asa 3100.349 -1657.805 7858.503 0.2768689 #&gt; asddd-asa 36139.258 33251.225 39027.291 0.0000000 #&gt; asddd-aaas 33038.909 27761.979 38315.839 0.0000000 3.3.3 Test de normalidad paramétricos Test Paramétricos: K-S y Shapiro Los test de normalidad Kolmogorov-Smirnov (KS) y Shapiro-Wilk son utilizados para evaluar si un conjunto de datos sigue una distribución normal. El test KS compara la distribución empírica de los datos con una distribución normal teórica, evaluando las diferencias entre las funciones de distribución acumulada observada y esperada. El test Shapiro-Wilk, en cambio, evalúa la normalidad ajustando una estadística basada en los valores ordenados de la muestra. Mientras que el test KS es más general y se aplica a cualquier distribución, el Shapiro-Wilk es más específico para la normalidad y suele ser más potente para muestras pequeñas. Ambos test ayudan a validar supuestos en análisis estadísticos. g &lt;- log(bbdd$salario) ks.test(g, pnorm, mean(g), sd(g)) #&gt; Warning in ks.test.default(g, pnorm, mean(g), sd(g)): ties #&gt; should not be present for the Kolmogorov-Smirnov test Asymptotic one-sample Kolmogorov-Smirnov test data: g D = 0.13563, p-value = 5.343e-08 alternative hypothesis: two-sided shapiro.test(g) Shapiro-Wilk normality test data: g W = 0.92568, p-value = 1.438e-14 Tenga en cuenta que, la prueba de normalidad es sensible al tamaño de la muestra. Las muestras pequeñas con mayor frecuencia pasan las pruebas de normalidad. Por lo tanto, es importante combinar la inspección visual y la prueba de significación para tomar la decisión correcta. 3.4 Distribuciones bivariadas 3.4.1 Tablas de contingencia Una tabla de correlación muestra las relaciones entre variables cuantitativas, indicando la fuerza y dirección de la asociación mediante coeficientes de correlación, como Pearson o Spearman. Permite identificar patrones de dependencia lineal o no lineal entre pares de variables. Por otro lado, una tabla de contingencia, también conocida como tabla de frecuencia cruzada, se utiliza para analizar la relación entre dos variables categóricas. Muestra la frecuencia de ocurrencia conjunta de las categorías de las variables, facilitando el análisis de asociaciones y la prueba de independencia mediante chi-cuadrado. Ambas tablas son herramientas fundamentales para la exploración y análisis de datos en estadística. La función table() calcula tablas de frecuencias a partir de factores. &gt; msa &lt;- mean(bbdd$salario) # Salario &gt; msi &lt;- mean(bbdd$salini) # SAl ini &gt; Ricos &lt;- bbdd$salario &gt; msa # ricos/pobres &gt; Ricosi &lt;- bbdd$salini &gt; msi # densos/no dendos &gt; # Distribuciones de frecuencia unidimensional &gt; table(Ricos) #&gt; Ricos #&gt; FALSE TRUE #&gt; 329 145 &gt; table(Ricosi) #&gt; Ricosi #&gt; FALSE TRUE #&gt; 345 129 &gt; # Distribuciones de frecuencia bidimensionales &gt; TaRiRi &lt;- table(Ricos,Ricosi) &gt; print(TaRiRi) #&gt; Ricosi #&gt; Ricos FALSE TRUE #&gt; FALSE 310 19 #&gt; TRUE 35 110 &gt; TaRiSx &lt;- table(Ricos,bbdd$sexo) &gt; print(TaRiSx) #&gt; #&gt; Ricos h m #&gt; FALSE 138 191 #&gt; TRUE 120 25 &gt; &gt; # Como siempre, hay gráficos informativos &gt; library(graphics) &gt; mosaicplot(TaRiRi,main = &quot;Ricos vs Ricosi&quot;) &gt; mosaicplot(TaRiSx,main = &quot;Ricos vs Sexo&quot;,color = TRUE) 3.4.2 Algo de test de independencia El test de independencia chi-cuadrado (\\(\\chi^2\\)) se utiliza para evaluar si dos variables categóricas están asociadas o son independientes entre sí. Se basa en la comparación entre las frecuencias observadas en una tabla de contingencia y las frecuencias esperadas si las variables fueran independientes. Calcula un estadístico \\(\\chi^2\\) que se compara con un valor crítico de la distribución chi-cuadrado, considerando el número de grados de libertad. Si el estadístico \\(\\chi^2\\) es mayor que el valor crítico, se rechaza la hipótesis nula de independencia, indicando una asociación significativa entre las variables. Este test es útil para analizar relaciones en datos categóricos. &gt; # Contrastes Chi2 de independencia &gt; chisq.test(TaRiRi) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity #&gt; correction #&gt; #&gt; data: TaRiRi #&gt; X-squared = 246.05, df = 1, p-value &lt; 2.2e-16 &gt; chisq.test(TaRiSx) #&gt; #&gt; Pearson&#39;s Chi-squared test with Yates&#39; continuity #&gt; correction #&gt; #&gt; data: TaRiSx #&gt; X-squared = 65.953, df = 1, p-value = 4.618e-16 3.4.3 Correlaciones Una matriz de correlaciones es una tabla que muestra las correlaciones entre múltiples variables cuantitativas. Cada celda indica el coeficiente de correlación entre un par de variables, facilitando la identificación de relaciones lineales y patrones de asociación en grandes conjuntos de datos. &gt; cor(bbdd$salario,bbdd$salini) #&gt; [1] 0.8801175 &gt; cor(bbdd$salario,bbdd$expprev) #&gt; [1] -0.09746693 &gt; # Un plot con correlaciones &gt; library(ggcorrplot) &gt; ggcorrplot(cor(bbdd[,6:9]),lab_size = 3,hc.order = TRUE,method = &quot;circle&quot;,lab = TRUE) &gt; # para saber si es significativamente distinta de cero &gt; cor.test(bbdd$salario,bbdd$salini) #&gt; #&gt; Pearson&#39;s product-moment correlation #&gt; #&gt; data: bbdd$salario and bbdd$salini #&gt; t = 40.276, df = 472, p-value &lt; 2.2e-16 #&gt; alternative hypothesis: true correlation is not equal to 0 #&gt; 95 percent confidence interval: #&gt; 0.8580696 0.8989267 #&gt; sample estimates: #&gt; cor #&gt; 0.8801175 &gt; plot(bbdd$salario,bbdd$salini, main=&quot;Salario vs Salario Inicial&quot;, col=&#39;red&#39;) "],["regresión-lineal-múltiple.html", "Capítulo 4 Regresión Lineal Múltiple 4.1 Estimación de los parámetros. Método de los mínimos cuadrados. 4.2 Descomposición de la Varianza y bondad del ajuste. 4.3 Ejemplo de regresión lineal múltiple con R", " Capítulo 4 Regresión Lineal Múltiple Esta sesión trata sobre la regresión lineal, que es un enfoque sencillo dentro de los algoritmos de aprendizaje supervisado y que se utiliza principalmente para predecir respuestas cuantitativas. Aunque puede parecer menos emocionante que otros métodos estadísticos más modernos, la regresión lineal sigue siendo una herramienta ampliamente utilizada y útil. Queremos destacar que es una base importante para comprender métodos más complejos, ya que muchos de estos métodos se pueden ver como extensiones de la regresión lineal. Por lo tanto es importante comprender la regresión lineal antes de abordar métodos de aprendizaje más avanzados. El enfoque principal de esta sesión es revisar las ideas clave detrás del modelo de regresión lineal y el método de mínimos cuadrados utilizado para ajustar este modelo. La regresión múltiple tiene como objetivo analizar un modelo que pretende explicar el comportamiento de una variable (endógena, explicada o dependiente), que se denota como \\(Y\\), utilizando la información proporcionada por los valores tomados por un conjunto de variables explicativas (exógenas o independientes), que se denotan por \\(X_1, X_2,\\dots, X_p\\). El modelo lineal (modelo econométrico) viene dado de la forma: \\[\\begin{equation}\\label{eq:modelo} Y=\\beta_0+\\beta_1 X_1+\\beta_2 X_2+\\dots+\\beta_p X_p +e \\end{equation}\\] donde \\(\\beta_0,\\beta_1,\\dots,\\beta_p\\) son parámetros desconocidos (o coeficientes) y \\(e\\) es un termino aleatorio de error, que es independiente de las variables explicativas \\(X_1, X_2,\\dots, X_p\\) y tiene media cero. Regresión lineal múltiple con una variable respuesta (\\(Y\\)) y dos variables predictoras (\\(X_1,X_2)\\) 4.1 Estimación de los parámetros. Método de los mínimos cuadrados. Supongamos que tenemos una muestra de tamaño \\(n\\) en la que hemos observado las variables \\[Y=\\begin{pmatrix}y_1\\\\y_2\\\\ \\vdots\\\\y_n \\end{pmatrix};\\quad X=(1, X_1, X_2,\\dots, X_p)=\\begin{pmatrix} 1&amp;x_{11}&amp;x_{12}&amp;\\dots&amp;x_{1p}\\\\ 1&amp;x_{21}&amp;x_{22}&amp;\\dots&amp;x_{2p}\\\\ &amp;&amp;&amp;\\vdots&amp;\\\\ 1&amp;x_{n1}&amp;x_{n2}&amp;\\dots&amp;x_{np} \\end{pmatrix}\\] Si denotamos por \\[\\beta=\\begin{pmatrix}\\beta_0\\\\\\beta_1\\\\\\vdots\\\\\\beta_p\\end{pmatrix}\\] el modelo econométrico se puede expresar en forma matricial como \\[Y=X\\beta+E\\] Teorema Si las columnas de \\(X\\) son linealmente independientes, entonces el estimador mínimo cuadrático de los coeficientes del modelo sería \\[\\widehat{\\beta}=(X^tX)^{-1}X^tY\\] donde \\(X^t\\) denota la matriz traspuesta de X. Demostración: Denotemos por \\(\\widehat{Y}=X\\widehat{\\beta}\\) tenemois que los terminos de error (o residuos) se pueden escribir como \\(e_i=y_i-\\widehat{y}_i=y_i-\\widehat{\\beta_0}+\\widehat{\\beta_1}x_{i1}-\\widehat{\\beta_2}x_{i2}-\\dots-\\widehat{\\beta_p}x_{ip}\\). El método de estimación mínimo cuadrático consiste en obtener el vector de coeficientes \\(\\widehat{\\beta}\\) que minimiza la suma de los errores al cuadrado. Teniendo en cuenta que al ser escalares \\(Y^tX\\widehat{\\beta}=\\widehat{\\beta}^tX^tY\\), se verifica que la suma de los errores al cuadrado se puede escribir como \\[\\begin{eqnarray} RSS&amp;=&amp;\\sum_{i=1}^ne_i^2=[Y-X\\widehat{\\beta}]^t[Y-X\\widehat{\\beta}]=\\\\ &amp;=&amp;Y^tY-Y^tX\\widehat{\\beta}-\\widehat{\\beta}^tX^tY+\\widehat{\\beta}^tX^tX\\widehat{\\beta}=\\\\ &amp;=&amp;Y^tY-2Y^tX\\widehat{\\beta}+\\widehat{\\beta}^tX^tX\\widehat{\\beta}. \\end{eqnarray}\\] Para minimizar \\(RSS\\) tenemos que resolver la ecuación \\[\\frac{\\partial RSS}{\\partial \\widehat{\\beta}}=-2Y^tX+2\\widehat{\\beta}^tX^tX=0\\] y por tanto como las columnas de \\(X\\) son linealmente independientes existe la inversa de la matriz \\(X^tX\\) obteniendo \\[\\widehat{\\beta}=(X^tX)^{-1}X^tY\\] tal y como queriamos demostrar.\\(\\square\\) Obsérvese que \\(\\widehat{\\beta}_i\\) midel el cambbio en \\(Y\\) por cada cambio unitario en \\(X_i\\) para todo \\(i=1, 2,\\dots, p\\). Además si comprobamos que los residuos son homocedásticos, independientes e identicamente distribuidos como una distribución \\(N(0,\\sigma^2)\\), tenemos que \\(Y\\) se distribuye como \\(N(X\\widehat{\\beta},\\sigma^2 I)\\). Un estimador de la varianza del error sería: \\[\\sigma^2=\\frac{1}{n-(p+1)}\\sum_{i=1}^ne_i^2=\\frac{1}{n-(p+1)}\\sum_{i=1}^n(y_i-\\widehat{y}_i)^2\\] Como \\(\\widehat{\\beta}=(X^tX)^{-1}X^tY\\) se verifica que su media es \\[\\mathbb{E}(\\widehat{\\beta})=(X^tX)^{-1}X^tE(Y)=(X^tX)^{-1}X^tX\\beta=\\beta.\\] Si queremos hacer inferencia para contrastar una hipotesis nula del estilo \\(H_0: \\, \\beta=0\\) tenemos que \\[Var(\\widehat{\\beta})=(X^tX)^{-1}X^tVar(Y)X(X^tX)^{-1}=(X^tX)^{-1}X^t\\sigma^2X(X^tX)^{-1}=\\sigma^2(X^tX)^{-1}\\] y como \\(\\widehat{\\beta}\\) es una combinmacion lineal de elementos de \\(Y\\) bajo \\(H_0\\) se verifica que \\[\\widehat{\\beta}\\sim N(0,\\sigma^2(X^tX)^{-1}).\\] Esto nos permite hacer inferencia sobre la significatividad de los parámetros estimados \\(\\widehat{\\beta}\\), contrastando si significativamente distintos de cero, así como calcular intervalos de confianza para los mismos. Los coeficientes estimados \\(\\widehat{\\beta}\\) nos proporcionan información sobre cuanto aporta cada variable independiente \\(X_1, X_2,\\dots, X_p\\) a la estimación de \\(Y\\). 4.2 Descomposición de la Varianza y bondad del ajuste. Basado en la ley del valor esperado total que dice \\(\\mathbb{E}(Y)=\\mathbb{E}(\\mathbb{E}(Y|X))\\) podemos demostrar el siguiente resultado. Proposición: Si \\(X\\) e \\(Y\\) son dos variables aleatorias definidas en el mismo espacio de probabilidad y suponemos que \\(Y\\) tiene varianza finita, entonces \\[Var(Y)=\\mathbb{E}(Var(Y|X))+Var(\\mathbb{E}(Y|X))\\] Demostración: Sabemos que \\(Var(Y)=\\mathbb{E}(Y^2)-\\mathbb{E}(Y)^2\\) y por tanto \\(\\mathbb{E}(Y^2)=Var(Y)+\\mathbb{E}(Y)^2\\). Luego aplicando la ley del valor esperado total a la expresion anterior tenemos que \\[\\mathbb{E}(Y^2)=\\mathbb{E}(Var(Y|X)+\\mathbb{E}(Y|X)^2)=\\mathbb{E}(Var(Y|X))+\\mathbb{E}(\\mathbb{E}(Y|X))^2.\\] Restando \\(\\mathbb{E}(Y)^2\\) en ambos lados de la igualdad anterior y applicando de nuevo ley del valor esperado total a \\(\\mathbb{E}(Y)^2=\\mathbb{E}(\\mathbb{E}(Y|X))^2\\) tenemos que \\[\\begin{eqnarray} Var(Y)&amp;=&amp;\\mathbb{E}(Y^2)-\\mathbb{E}(Y)^2=\\mathbb{E}(Var(Y|X))+\\mathbb{E}(\\mathbb{E}(Y|X))^2-\\mathbb{E}(Y)^2=\\\\ &amp;=&amp;\\mathbb{E}(Var(Y|X))+\\mathbb{E}(\\mathbb{E}(Y|X)^2)-\\mathbb{E}(\\mathbb{E}(Y|X))^2=\\mathbb{E}(Var(Y|X))+Var(\\mathbb{E}(Y|X)) \\end{eqnarray}\\] tal y como queriamos demostrar. \\(\\square\\) Corolario Dado el modelo de regresión lineal \\(Y=X\\beta+E\\) se verifica que \\(Var(Y)=Var(E)+Var(\\widehat{Y})\\) Demostración: Obsérvese que \\(Var(Y|X)=Var(E)=\\sigma^2\\) y \\(\\mathbb{E}(Y|X)=\\widehat{Y}\\). Por lo tanto como consecuencia de la proposición anterior tenemos que \\(Var(Y)=\\sigma^2+Var(\\widehat{Y})\\), es decir la varianza total de \\(Y\\) se descompone como la suma de la varianza explicada por \\(\\widehat{Y}\\) y la varianza de los errores. \\(\\square\\) 4.2.1 Coeficientes de determinación y correlación. Se define el coeficiente de determinación \\(R^2\\) como la proporcion de la varianza total que es recogida por la varianza de la variable ajustada, es decir \\[R^2=\\frac{Var(\\widehat{Y})}{Var(Y)}=1-\\frac{Var(E)}{Var(Y)}\\] De esta última expresión es inmediato ver que \\(0\\leq R^2\\leq 1\\). Observese que si \\(R^2=0\\) significa que \\(Var(E)=Var(Y)\\) y por la proposición anterior \\(Var(\\widehat{Y})=0\\), por tanto \\(\\widehat{Y}=E(Y)\\) y el modelo no recoge nada de la variabilidad total y como consecuencia el ajuste es malo. En el otro extremo, si \\(R^2=1\\) se sigue que \\(Var(\\widehat{Y})=Var(Y)\\) y \\(Var(E)=0\\) y por tanto el modelo recoge toda la variabilidad obteniendo \\(Y=\\widehat{Y}\\) y \\(E=0\\). De esta manera concluimos que cuanto más cercano esté \\(R^2\\) a 1 mejor será el ajuste del modelo. Proposición: Dado el modelo de regresión \\(Y=X\\beta+E\\) se verifica que \\[Cov(\\widehat{Y},E)=\\widehat{Y}^tE=0\\] Demostración: Como \\(E\\) tiene media cero, se sigue que \\(Cov(\\widehat{Y},E)=\\mathbb{E}(\\widehat{Y}^tE)=\\mathbb{E}(\\widehat{\\beta}^tX^t(Y-\\widehat{Y}))=\\\\=\\mathbb{E}(Y^tX(X^tX)^{-1}X^tY-Y^tX(X^tX)^{-1}X^tX(X^tX)^{-1}X^tY)=0 \\quad\\square\\) Definition Dadas dos variables estadísticas \\(U\\) y \\(V\\), se define el coefficiente de correlacion de Pearson de \\(U\\) y \\(V\\) como \\[Cor(U,V)=\\rho_{UV}=\\frac{Cov(U,V)}{\\sqrt{Var(U)Var(V)}}\\] Teorema:El coeficiente de determinacion \\(R^2\\) coincide con el coeficiente de correlacion dde Pearson de las variables \\(Y\\) e \\(\\widehat{Y}\\) al cuadrado: \\[R^2=Cor(\\widehat{Y},Y)^2\\] Demostración: Como \\(\\mathbb{E}(\\widehat{Y})=\\mathbb{E}(Y)\\) y \\(Cov(\\widehat{Y},E)=0\\) tenemos que \\[\\begin{eqnarray} Cor(\\widehat{Y},Y)&amp;=&amp;\\frac{(\\widehat{Y}-\\mathbb{E}(Y))^t(Y-\\mathbb{E}(Y))}{\\sqrt{Var(Y)Var(\\widehat{Y})}}=\\frac{(\\widehat{Y}-\\mathbb{E}(Y))^t(Y-\\widehat{Y}+\\widehat{Y}-\\mathbb{E}(Y))}{\\sqrt{Var(Y)Var(\\widehat{Y})}}=\\\\ &amp;=&amp;\\frac{(\\widehat{Y}-\\mathbb{E}(Y))^t(E+\\widehat{Y}-\\mathbb{E}(Y))}{\\sqrt{Var(Y)Var(\\widehat{Y})}}=\\frac{(\\widehat{Y}-\\mathbb{E}(Y))^t(\\widehat{Y}-\\mathbb{E}(Y))}{\\sqrt{Var(Y)Var(\\widehat{Y})}}=\\\\ &amp;=&amp;\\frac{\\sqrt{Var(\\widehat{Y})}}{\\sqrt{Var(Y)}} \\end{eqnarray}\\] y por tanto \\(Cor(\\widehat{Y},Y)=R^2\\) tal y como queríamos demostrar.\\(\\quad \\square\\) 4.2.2 Coeficientes de determinación semi-parcial y parcial Para conocer cuanto contribuye la variable \\(X_k\\) de manera única al modelo de regresión, podemos pensar en cuanto se modifica el coeficiente de determinación al excluir esta variable en la regresión lineal. De esta manera si denotamos por \\(R_{-k}^2\\) el coefficiente de determinación del modelo de regresión lineal omitiendo la variable \\(X_k\\) la cantidad \\[R^2-R_{-k}^2\\] es una manera de cuantificar cuanta información única sobre \\(Y\\) en \\(X_k\\) no está explicada por el resto de variables indeependientes. Esta cantidad es conocida como coeficiente de determinación semi-parcial. Se define el coefficiente de determinacion parcial como \\[\\frac{R^2-R^2_{-k}}{1-R^2_{-k}}\\] Teorema: Sea \\(U\\) el residuo de la regresión lineal de una variable independiente \\(X_k\\) sobre el resto de las variables independientes \\(X_i\\) con \\(i\\neq k\\). Denotemos por \\(\\widehat{Y}_{-k}\\) y \\(V\\) los valores ajustados de \\(Y\\) y los residuos cuando hacemos la resgesion de \\(Y\\) sobre todas las variables independientes excepto \\(X_k\\) respectivamente. Entonces el coeficiente de determinación semi-parcial y el coeficiente de determinación parcial se pueden calcular como: \\[R^2-R^2_{-k}=Cor(Y,U)^2 \\quad\\quad\\quad\\quad\\frac{R^2-R^2_{-k}}{1-R^2_{-k}}=Cor(U,V)^2\\] Demostración: La demostración la haremos utilizando algebra lineal. Para ello utilizaremos algunos conceptos básicos. Denotamos el producto escalar de dos vectores \\(S\\) y \\(W\\) por \\(\\langle S, W \\rangle=S^tW=\\sum_{i=1}^n s_iw_i\\) y su norma por \\(||S||=\\sqrt{\\langle S, S\\rangle}\\). Sabemos que si \\(S\\) y \\(W\\) son ortogonales entonves \\(\\langle S,W\\rangle=0\\). Sea la matriz \\(P_{-k}\\) la proyección ortogonal en el espacio generado por todas las variables independientes excepto \\(X_k\\). Es conocido que la matriz \\(P_{-k}\\) es simétrica e idempotente, es decir \\(P_{-k}^2=P_{-k}\\). Entonces se tiene que \\(P_{-k}Y=\\widehat{Y}_{-k}\\) y \\(U=X_k-\\widehat{X}_k=(I-P_{-k})X_k\\). Además \\(U\\) es ortogonal a todos los \\(X_i\\) con \\(i\\neq k\\) ya que \\(\\langle U,X_i\\rangle=U^tX_i=X_k^t(I-P_{-k})X_i=X_k^t(X_i-X_i)=0\\). Por lo tanto, \\(\\widehat{Y}\\) que es la proyeccion ortogonal de \\(Y\\) en el espacio generado por todas las variables predictoras se puede descomponer como la suma de la proyección ortogonal sobre el espacio generado por todas la variables excepto \\(X_k\\) y uno ortogonal a este, e.g. el generado por \\(U\\). Pero la proyeccion de \\(Y\\) en el espacio generado por \\(U\\) es el valor estimado de la recta de regrsion de \\(Y\\) sobre \\(U\\), es decir \\(\\frac{\\langle Y,U\\rangle}{||U||}U\\). Por tanto tenemos que \\[\\widehat{Y}=\\widehat{Y}_{-k}+\\frac{\\langle Y,U\\rangle}{||U||^2}U\\] y teniendo en cuenta que \\(Y_{-k}\\) y \\(U\\) son oryogonales calculando la norma al cuadrado en la expresion anterior \\[||Y||^2=||Y_{-k}||^2+\\frac{\\langle Y,U\\rangle^2}{||U||^2}.\\] Utilizando estas expresiones y que \\(\\mathbb{E}(Y)=\\mathbb{E}(\\widehat{Y})\\) tenemos que \\[\\begin{eqnarray} R^2&amp;=&amp;1-\\frac{Var(E)}{Var(Y)}=1-\\frac{Var(Y)-Var(\\widehat{Y})}{Var(Y)}=1-\\frac{||Y||^2-||\\widehat{Y}||^2}{||Y-\\mathbb{E}(Y)||^2}\\\\ &amp;=&amp;1-\\frac{||Y||^2-||\\widehat{Y}_{-k}||^2-\\frac{\\langle Y,U\\rangle^2}{||U||^2}}{||Y-\\mathbb{E}(Y)||^2}\\\\ &amp;=&amp;1-\\frac{||Y-\\widehat{Y}_{-k}||^2}{||Y-\\mathbb{E}(Y)||^2}+\\frac{\\frac{\\langle Y,U\\rangle^2}{||U||^2}}{||Y-\\mathbb{E}(Y)||^2}=R^2_{-k}+\\frac{\\langle Y,U\\rangle^2}{||Y-\\mathbb{E}(Y)||^2||U||^2}\\\\ &amp;=&amp;R^2_{-k}+ Cor(Y,U) \\end{eqnarray}\\] Luego el coeficiente de determinación semi-parcial queda \\(R^2-R^2_{-k}=Cor(Y,U)^2\\). Por otro lado como \\(1-R^2_{-k}=1-(1-\\frac{Var(V)}{Var(Y)})=\\frac{||V||^2}{||Y-\\mathbb{E}(Y)||^2}\\) y \\(\\widehat{Y}_{-k}\\) es ortogonal a \\(U\\), tenemos que el coeficiente de determinación parcial es \\[\\frac{R^2-R^2_{-k}}{1-R^2_{-k}}=\\frac{\\frac{\\langle Y,U\\rangle^2}{||Y-\\mathbb{E}(Y)||^2||U||^2}}{\\frac{||V||^2}{||Y-\\mathbb{E}(Y)||^2}}=\\frac{\\langle Y,U\\rangle^2}{||U||^2||V||^2}=\\\\ =\\frac{\\langle Y-\\widehat{Y}_{-k},U\\rangle^2}{||U||^2||V||^2}=\\frac{\\langle V,U\\rangle^2}{||U||^2||V||^2}=Cor(U,V)^2\\] tal y como queríamos demostrar.\\(\\,\\square\\) Como consecuencia el coeficiente de determinación semi-parcial tiene dos interpretaciones: 1.- La mejora en \\(R^2\\) que resulta de introducir la variable \\(X_k\\) en el modelo de regresión que ya incluía al resto de variables independientes. 2.- Es el coeficiente de determinación de la regresión lineal simple de \\(Y\\) sobre \\(U\\). Asimismo, el coeficiente de determinación parcial se puede interpretar como: 1.- La fracción de la máxima mejorta posible en \\(R^2\\) al que contribuye la variable \\(X_k\\). 2.- Es el coeficiente de determinación de la regresión simple de \\(V\\) sobre \\(U\\). 4.3 Ejemplo de regresión lineal múltiple con R Carguemos la base de datos de empleados Base de datos de empleados &gt; library(readxl) &gt; datos &lt;- read_xlsx(&quot;Datos/Datos_de_empleados.xlsx&quot;) Expliquemos con un modelo de regresión lineal el salario de los trabajadores como función del salario inicial, el nivel educativo del trabajador y los meses de experiencia previa. Para ello en R tenemos que indicar las variables a relacionar de la siguiente manera: &gt; formula&lt;- datos$salario~datos$salini+datos$educ+datos$expprev El modelo de regresión se estima utilizando el comando lm y un resumen del modelo aparece con el comando summary tal y como mostramos a continuación: &gt; modols&lt;-lm(formula) &gt; summary(modols) #&gt; #&gt; Call: #&gt; lm(formula = formula) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -28853 -4167 -1172 2724 48701 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) -3.662e+03 1.935e+03 -1.892 0.0591 . #&gt; datos$salini 1.749e+00 5.989e-02 29.198 &lt; 2e-16 *** #&gt; datos$educ 7.360e+02 1.687e+02 4.363 1.58e-05 *** #&gt; datos$expprev -1.673e+01 3.605e+00 -4.641 4.51e-06 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 7632 on 470 degrees of freedom #&gt; Multiple R-squared: 0.8015, Adjusted R-squared: 0.8002 #&gt; F-statistic: 632.6 on 3 and 470 DF, p-value: &lt; 2.2e-16 Obsérvese, que el salario inicial y el nivel educativo tienen un impacto positivo y significativo en el salario de la persona, mientras que los años de experiencia, a pesar de ser significativo en el modelo, tiene un impacto negativo en el salario del empleado. EL coeficiente de determinación del modelo es de \\(R^2=0.8015\\) y por tanto podemos decir que el modelo recoge aproximadamente el \\(80\\%\\) de la varianza de la variable dependiente lo que significa que es un ajuste bueno. Podemos obtener los valores predichos por el modelo y representarlos graficamente. &gt; Yfit=modols$fitted.values &gt; xx=seq(1,length(Yfit)) &gt; plot(xx,modols$fitted.values,pch=19,col=&#39;red&#39;,ylim=c(8500,136000),xlab=&quot;&quot;,ylab=&quot;Salario&quot;, xaxt=&quot;n&quot;) &gt; axis(1, at = seq(1,length(Yfit),20)) &gt; par(new=TRUE) &gt; plot(datos$salario,pch=19,col=&#39;blue&#39;,xlab=&quot;&quot;,ylab=&quot;Salario&quot;,axes=FALSE) &gt; par(new=FALSE) Podemos ahora calcular las correlaciones parciales de todas las variables intervinientes en el modelo. Por ejemplo calculemos el coeficiente de correlación parcial de la variable expprev en el modelo *modols. Para ello calculamos \\(R^2\\) y \\(R^2_{-expprev}\\) tal y como sigue &gt; restot=summary(modols) &gt; R2=restot$r.squared &gt; modsinexpprev=lm(datos$salario~datos$salini+datos$educ)# modelo sin la variable expprev &gt; resexpprev=summary(modsinexpprev) &gt; R2expprev=resexpprev$r.squared &gt; PartialrR2expprev=(R2-R2expprev)/(1-R2expprev) &gt; print(PartialrR2expprev) #&gt; [1] 0.04381444 Otra forma de calcularlo sería calculando \\(1-SR_{full}/SR_{expprev}\\) donde \\(SR_{full}\\) y \\(SR_{expprev}\\) son la suma de los cuadrados de los residuos del modelo con todas las variables \\(modols\\) y el modelo reducido sin la variable expprev, \\(modsibexpprev\\) respectuivamente. &gt; PartialrR2expprev2=1-sum(modols$residuals^2)/sum(modsinexpprev$residuals^2) &gt; print(PartialrR2expprev2) #&gt; [1] 0.04381444 Por tanto tenemos que la mejora máxima en el coeficiente de determinación, \\(R^2\\), que produce la incorporación de la variable expprev es del \\(4.38\\%\\). Ejercicio: 1.- Calcula e interpreta los coeficientes de correlación parcial del resto de variables del modelo modols. 2.- Investiga el efecto que puede tener el género del empleado en el salario. "],["análisis-de-componentes-principales.html", "Capítulo 5 Análisis de Componentes Principales 5.1 Introducción 5.2 Antes de empezar 5.3 El ACP 5.4 Test iniciales 5.5 Una aplicación con “Datos de Empleados” 5.6 Paquetes de R para el ACP 5.7 Proporción de la varianza explicada 5.8 El gráfico de sedimentación 5.9 Las componentes 5.10 Ejercicio: ACP como indicador sintético 5.11 Aplicación: Regresión y ACP", " Capítulo 5 Análisis de Componentes Principales 5.1 Introducción El análisis de componentes principales (ACP) es una técnica fundamental en estadística y análisis multivariado que se utiliza para simplificar y entender la estructura subyacente en conjuntos de datos complejos. Esta metodología tiene aplicaciones en diversas disciplinas, como la estadística, la ingeniería, la biología, la economía y la ciencia de datos. En esencia, el ACP busca transformar un conjunto de variables correlacionadas en un nuevo conjunto de variables no correlacionadas, conocidas como componentes principales. Estos componentes se ordenan en función de la cantidad de varianza que explican en los datos originales, lo que permite destacar las direcciones principales de variabilidad en el conjunto de datos. La idea central detrás del ACP es reducir la dimensionalidad del conjunto de datos, manteniendo la mayor cantidad posible de información. Al proyectar los datos en un espacio de menor dimensión definido por los componentes principales, se facilita la visualización y la interpretación de patrones y tendencias en los datos, lo que puede ser crucial para la toma de decisiones informada. A lo largo de este proceso, el ACP proporciona una herramienta valiosa para identificar patrones subyacentes, eliminar redundancias y resaltar las relaciones más importantes entre las variables, lo que contribuye significativamente a la simplificación y comprensión de conjuntos de datos complejos. 5.2 Antes de empezar Antes de aplicar el ACP, es importante considerar algunas condiciones y realizar ciertos pasos: Tipo de Variables: El ACP se utiliza comúnmente con variables cuantitativas. Las variables deben ser de escala numérica, ya que el método implica operaciones algebraicas y estadísticas que requieren números. Si tienes variables categóricas, es posible que necesites realizar alguna transformación o utilizar técnicas diferentes. Escalas de Medición: Las variables deben tener escalas de medición comparables. Si las unidades de medida son muy diferentes entre las variables, es recomendable estandarizar o normalizar las variables antes de aplicar el ACP para evitar que una variable con una escala más grande domine la variabilidad. Correlación entre Variables: El ACP asume que existe cierta correlación entre las variables originales. Si las variables están completamente incorrelacionadas, el análisis no aportará información significativa. Linealidad: El ACP asume linealidad entre las variables. Si la relación entre las variables es altamente no lineal, el ACP puede no capturar adecuadamente la estructura subyacente de los datos. 5.3 El ACP 5.3.1 Escalado de las variables El proceso de PCA identifica aquellas direcciones en las que la varianza es mayor. Como la varianza de una variable se mide en su misma escala elevada al cuadrado, si antes de calcular las componentes no se estandarizan todas las variables para que tengan media 0 y desviación estándar 1, aquellas variables cuya escala sea mayor dominarán al resto. De ahí que sea recomendable estandarizar siempre los datos. 5.3.2 Influencia de outliers Al trabajar con varianzas, el método PCA es altamente sensible a outliers, por lo que es altamente recomendable estudiar si los hay. La detección de valores atípicos con respecto a una determinada dimensión es algo relativamente sencillo de hacer mediante comprobaciones gráficas. Sin embargo, cuando se trata con múltiples dimensiones el proceso se complica. Por ejemplo, considérese un hombre que mide 2 metros y pesa 50 kg. Ninguno de los dos valores es atípico de forma individual, pero en conjunto se trataría de un caso muy excepcional. La distancia de Mahalanobis es una medida de distancia entre un punto y la media que se ajusta en función de la correlación entre dimensiones y que permite encontrar potenciales outliers en distribuciones multivariante. 5.3.3 Proporción de varianza explicada Una de las preguntas más frecuentes que surge tras realizar un PCA es: ¿Cuánta información presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión? o lo que es lo mismo ¿Cuanta información es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporción de varianza explicada por cada componente principal. 5.3.4 Número óptimo de componentes principales Por lo general, dada una matriz de datos de dimensiones n x p, el número de componentes principales que se pueden calcular es como máximo de n-1 o p (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de interés utilizar el número mínimo de componentes que resultan suficientes para explicar los datos. No existe una respuesta o método único que permita identificar cual es el número óptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporción de varianza explicada acumulada y seleccionar el número de componentes mínimo a partir del cual el incremento deja de ser sustancial. 5.4 Test iniciales 5.4.1 Pruebas esfericidad de Bartlett Se utiliza para probar la Hipótesis Nula que afirma que las variables no están correlacionadas en la población. Es decir, comprueba si la matriz de correlaciones es una matriz de identidad. Se puede dar como válidos aquellos resultados que nos presenten un valor elevado del test y cuya fiabilidad sea menor a 0.05. En este caso se rechaza la Hipótesis Nula y se continúa con el Análisis. Prueba de esfericidad de Bartlett: Si Sig. (p-valor) &lt; 0.05 aceptamos \\(H_0\\) (hipótesis nula) =&gt; se puede aplicar el análisis factorial. Si Sig. (p-valor) &gt; 0.05 rechazamos \\(H_0\\) =&gt; no se puede aplicar el análisis factorial. 5.4.2 Prueba del KMO El test KMO (Kaiser, Meyer y Olkin) relaciona los coeficientes de correlación, \\(r_{jh}\\), observados entre las variables \\(X_j\\) y \\(X_h\\), y \\(a_{jh}\\) son los coeficientes de correlación parcial entre las variables \\(X_j\\) y \\(X_h\\). Cuanto más cerca de 1 tenga el valor obtenido del test KMO, implica que la relación entres las variables es alta. Si KMO ≥ 0.9, el test es muy bueno; notable para KMO ≥ 0.8; mediano para KMO ≥ 0.7; bajo para KMO ≥ 0.6; y muy bajo para KMO &lt; 0.5. La prueba de evalúa la aplicabilidad del análisis factorial de las variables estudiadas. El modelo es significativo (aceptamos la hipótesis nula, H0) cuando se puede aplicar el análisis factorial 5.5 Una aplicación con “Datos de Empleados” El objetivo es aplicar el ACP a los datos “Datos de Empleados”. Debemos seleccionar solo las variables cuantitativas a las que vamos a añadir la edad del empleado. Lectura de datos y cálculo de matriz de correlaciones library(openxlsx) bbdd &lt;- read.xlsx(&quot;Datos/Datos_de_empleados.xlsx&quot;) cor(bbdd$salario,bbdd$salini) #&gt; [1] 0.8801175 cor(bbdd$salario,bbdd$expprev) #&gt; [1] -0.09746693 # Un plot con correlaciones library(ggcorrplot) #&gt; Loading required package: ggplot2 ggcorrplot(cor(bbdd[,6:9]),lab_size = 3,hc.order = TRUE,method = &quot;circle&quot;,lab = TRUE) Ejercicio: Incluir nueva variable: Edad del empleado Buscar información para calcular la edad de cada empleado e introducirla como una nueva variable Normalizamos datos El cálculo de los componentes principales depende de las unidades de medida empleadas en las variables. Es importante, antes de aplicar el PCA, estandarizar las variables para que tengan media 0 y desviación estándar 1. de &lt;- bbdd[,c(6:9,11)] de.n &lt;- scale(de) corr_matrix &lt;- cor(de.n) ggcorrplot(corr_matrix,lab = TRUE,digits=3) library(corrplot) #&gt; corrplot 0.92 loaded corrplot(cor(de.n)) Test de adecuación del ACP library(psych) cortest.bartlett(cor(de.n),n=474) #&gt; $chisq #&gt; [1] 1266.164 #&gt; #&gt; $p.value #&gt; [1] 7.662669e-266 #&gt; #&gt; $df #&gt; [1] 10 Con este test podemos ver la relación de coeficientes de correlación entre variables si el valor es cercano a 1, mayor será la relación entre variables, dado los valores, se puede decir que es apto para la realización del análisis de componentes. KMO(de.n) #&gt; Kaiser-Meyer-Olkin factor adequacy #&gt; Call: KMO(r = de.n) #&gt; Overall MSA = 0.48 #&gt; MSA for each item = #&gt; salario salini tiempemp expprev edad #&gt; 0.48 0.47 0.08 0.50 0.51 5.6 Paquetes de R para el ACP library(stats) prcomp() -&gt; Forma rápida de implementar PCA sobre una matriz de datos. princomp() library(FactoMineR) PCA() -&gt; PCA con resultados más detallados. Los valores ausentes se reemplazan por la media de cada columna. Pueden incluirse variables categóricas suplementarias. Estandariza automáticamente los datos. library(factoextra) fviz_pca_ind() -&gt; Representación de observaciones sobre componentes principales. fviz_pca_var() -&gt; Representación de variables sobre componentes principales. fviz_screeplot() -&gt; Representación (gráfico barras) de eigenvalores. fviz_contrib() -&gt; Representa la contribución de filas/columnas de los resultados de un pca. get_pca() -&gt; Extrae la información sobre las observaciones y variables de un análisis PCA. get_pca_var() -&gt; Extrae la información sobre las variables. get_pca_ind() -&gt; Extrae la información sobre las observaciones. 5.7 Proporción de la varianza explicada ¿Cuánta información presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión? ¿Cuánta información es capaz de capturar cada una de las componentes principales obtenidas? # https://rpubs.com/laurarojasmar/ACP pca &lt;- prcomp(de, scale = TRUE) print(pca) #&gt; Standard deviations (1, .., p=5): #&gt; [1] 1.3993033 1.3197040 1.0021060 0.4405821 0.3193784 #&gt; #&gt; Rotation (n x k) = (5 x 5): #&gt; PC1 PC2 PC3 PC4 #&gt; salario -0.60483585 -0.36493800 0.025204955 0.02748384 #&gt; salini -0.54214209 -0.45751707 -0.101152801 0.01888093 #&gt; tiempemp -0.01592624 -0.07314815 0.992391695 -0.04844080 #&gt; expprev 0.39509934 -0.58382909 -0.065339258 -0.70311587 #&gt; edad 0.42883379 -0.55793488 0.004724985 0.70863934 #&gt; PC5 #&gt; salario 0.70682615 #&gt; salini -0.69726014 #&gt; tiempemp -0.08489944 #&gt; expprev 0.06632468 #&gt; edad 0.05116844 de.pca &lt;- princomp(de.n) summary(princomp(de.n)) #&gt; Importance of components: #&gt; Comp.1 Comp.2 Comp.3 #&gt; Standard deviation 1.397826 1.3183112 1.0010484 #&gt; Proportion of Variance 0.391610 0.3483237 0.2008433 #&gt; Cumulative Proportion 0.391610 0.7399337 0.9407770 #&gt; Comp.4 Comp.5 #&gt; Standard deviation 0.44011714 0.31904129 #&gt; Proportion of Variance 0.03882252 0.02040051 #&gt; Cumulative Proportion 0.97959949 1.00000000 5.8 El gráfico de sedimentación El gráfico de sedimentación se obtiene al representar en ordenadas las raíces características y en abscisas los números de las componentes principales correspondientes a cada raíz característica en orden decreciente.Uniendo todos los puntos se obtiene una Figura que, en general, se parece al perfil de una montaña con una fuerte pendiente hasta llegar a la base, formada por una meseta con una ligera inclinación.En este símil establecido de la montaña, en la meseta es donde se acumulan los guijarros caídos desde la cumbre, es decir, donde se sedimentan. Este es el motivo por lo que al gráfico se le conoce con el nombre de gráfico de sedimentación, su denominación en ingléses scree plot. De acuerdo con el criterio gráfico se retienen todas aquellas componentes previas a la zona de sedimentación. scree(de.n) library(psych) library(FactoMineR) library(factoextra) de.pca &lt;- PCA(de.n, graph = FALSE) fviz_eig(de.pca, addlabels=T) 5.9 Las componentes biplot(x = pca, scale = 0, cex = 0.6, col = c(&quot;blue4&quot;, &quot;brown3&quot;)) de.pca &lt;- princomp(de.n) a &lt;- de.pca$scores bbdd$cp1 &lt;- a[,1] bbdd$cp2 &lt;- a[,2] plot(bbdd$cp1,bbdd$cp2,col=as.factor(bbdd$sexo)) plot(bbdd$cp1,bbdd$cp2,col=as.factor(bbdd$catlab)) 5.10 Ejercicio: ACP como indicador sintético El ACP puede utilizarse para crear un indicador sintético al considerar los primeros componentes principales como resúmenes o representaciones condensadas del conjunto de datos original. Si bien cada componente principal es una combinación lineal de las variables originales, los primeros componentes suelen capturar las características más importantes o patrones de variabilidad presentes en los datos. 5.10.1 Los datos El siguiente conjunto de datos corresponde a calificaciones de 20 estudiantes en 5 materias Ciencias Natuales (CNa), Matemáticas (Mat), Francés (Fra), Latín (Lat) y Literatura (Lit) # https://bookdown.org/victor_morales/TecnicasML/an%C3%A1lisis-de-componentes-principales.html CNa &lt;- c(7,5,5,6,7,4,5,5,6,6,6,5,6,8,6,4,6,6,6,7) Mat &lt;- c(7,5,6,8,6,4,5,6,5,5,7,5,6,7,7,3,4,6,5,7) Fra &lt;- c(5,6,5,5,6,6,5,5,7,6,5,4,6,8,5,4,7,7,4,6) Lat &lt;- c(5,6,7,6,7,7,5,5,6,6,6,5,6,8,6,4,8,7,4,7) Lit &lt;- c(6,5,5,6,6,6,6,5,6,6,5,4,5,8,6,4,7,7,4,6) Notas &lt;- cbind(CNa,Mat,Fra,Lat,Lit) Notas #&gt; CNa Mat Fra Lat Lit #&gt; [1,] 7 7 5 5 6 #&gt; [2,] 5 5 6 6 5 #&gt; [3,] 5 6 5 7 5 #&gt; [4,] 6 8 5 6 6 #&gt; [5,] 7 6 6 7 6 #&gt; [6,] 4 4 6 7 6 #&gt; [7,] 5 5 5 5 6 #&gt; [8,] 5 6 5 5 5 #&gt; [9,] 6 5 7 6 6 #&gt; [10,] 6 5 6 6 6 #&gt; [11,] 6 7 5 6 5 #&gt; [12,] 5 5 4 5 4 #&gt; [13,] 6 6 6 6 5 #&gt; [14,] 8 7 8 8 8 #&gt; [15,] 6 7 5 6 6 #&gt; [16,] 4 3 4 4 4 #&gt; [17,] 6 4 7 8 7 #&gt; [18,] 6 6 7 7 7 #&gt; [19,] 6 5 4 4 4 #&gt; [20,] 7 7 6 7 6 Obtener un sumario de la información Obtener la matriz de correlaciones Obtener test de adecuación Obtener CPs Interpretarlas 5.11 Aplicación: Regresión y ACP En esta sección realizaremos un ejercicio en el que se combina la Regresión Lineal Múltiple (RLM) con el ACP. El objetivo es reducir los problemas de multicolinealidad en la RLM al incluir variables altamente correlacionadas. El ACP se utiliza principalmente para abordar problemas de multicolinealidad o reducción de dimensionalidad en conjuntos de datos, lo que puede tener implicaciones en modelos de regresión múltiple. Sin embargo, es importante tener en cuenta que al usar el ACP de esta manera, la interpretación de los coeficientes en términos de las variables originales puede volverse más complicada. 5.11.1 Lectura y sumario de los datos Utilizaremos la base de datos de venta de vehículos. El objetivo es predecir el precio en base a sus características. Un sumario de los datos se muestra a continuación. library(openxlsx) CarSales &lt;- read.xlsx(&quot;Datos/CarSAles.xlsx&quot;) summary(CarSales) #&gt; fabricante modelo ventas #&gt; Length:157 Length:157 Min. : 0.11 #&gt; Class :character Class :character 1st Qu.: 14.11 #&gt; Mode :character Mode :character Median : 29.45 #&gt; Mean : 53.00 #&gt; 3rd Qu.: 67.96 #&gt; Max. :540.56 #&gt; #&gt; reventa tipo precio #&gt; Min. : 5.16 Min. :0.0000 Min. : 9.235 #&gt; 1st Qu.:11.26 1st Qu.:0.0000 1st Qu.:18.017 #&gt; Median :14.18 Median :0.0000 Median :22.799 #&gt; Mean :18.07 Mean :0.2611 Mean :27.391 #&gt; 3rd Qu.:19.88 3rd Qu.:1.0000 3rd Qu.:31.948 #&gt; Max. :67.55 Max. :1.0000 Max. :85.500 #&gt; NA&#39;s :36 NA&#39;s :2 #&gt; motor_s caballos BaseNeumatico #&gt; Min. :1.000 Min. : 55.0 Min. : 92.6 #&gt; 1st Qu.:2.300 1st Qu.:149.5 1st Qu.:103.0 #&gt; Median :3.000 Median :177.5 Median :107.0 #&gt; Mean :3.061 Mean :185.9 Mean :107.5 #&gt; 3rd Qu.:3.575 3rd Qu.:215.0 3rd Qu.:112.2 #&gt; Max. :8.000 Max. :450.0 Max. :138.7 #&gt; NA&#39;s :1 NA&#39;s :1 NA&#39;s :1 #&gt; anchura longitud peso_revestimiento #&gt; Min. :62.60 Min. :149.4 Min. :1.895 #&gt; 1st Qu.:68.40 1st Qu.:177.6 1st Qu.:2.971 #&gt; Median :70.55 Median :187.9 Median :3.342 #&gt; Mean :71.15 Mean :187.3 Mean :3.378 #&gt; 3rd Qu.:73.42 3rd Qu.:196.1 3rd Qu.:3.800 #&gt; Max. :79.90 Max. :224.5 Max. :5.572 #&gt; NA&#39;s :1 NA&#39;s :1 NA&#39;s :2 #&gt; tapón_combustible kpl #&gt; Min. :10.30 Min. :15.00 #&gt; 1st Qu.:15.80 1st Qu.:21.00 #&gt; Median :17.20 Median :24.00 #&gt; Mean :17.95 Mean :23.84 #&gt; 3rd Qu.:19.57 3rd Qu.:26.00 #&gt; Max. :32.00 Max. :45.00 #&gt; NA&#39;s :1 NA&#39;s :3 5.11.2 Eliminación de datos perdidos Por simplicidad en el ejercicio se eliminan algunos datos en los que hay datos faltantes. En general estos valores podrian ser sustituidos por valores medios. CarSales &lt;- CarSales[!is.na(CarSales$peso_revestimiento),] CarSales &lt;- CarSales[!is.na(CarSales$precio),] CarSales &lt;- CarSales[!is.na(CarSales$kpl),] 5.11.3 Un modelo de regresión básico formula &lt;- precio ~ motor_s + caballos + BaseNeumatico + anchura + longitud + peso_revestimiento + tapón_combustible + kpl summary(lm(formula , data=CarSales)) #&gt; #&gt; Call: #&gt; lm(formula = formula, data = CarSales) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -14.494 -3.106 -0.469 2.547 32.108 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 23.04275 17.32973 1.330 0.18575 #&gt; motor_s -3.84947 1.24859 -3.083 0.00246 ** #&gt; caballos 0.26273 0.01825 14.397 &lt; 2e-16 *** #&gt; BaseNeumatico -0.06376 0.15059 -0.423 0.67265 #&gt; anchura -0.42902 0.27540 -1.558 0.12149 #&gt; longitud -0.23427 0.08328 -2.813 0.00560 ** #&gt; peso_revestimiento 9.01935 2.15367 4.188 4.9e-05 *** #&gt; tapón_combustible 0.31214 0.30997 1.007 0.31563 #&gt; kpl 0.52783 0.25548 2.066 0.04063 * #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 6.738 on 143 degrees of freedom #&gt; Multiple R-squared: 0.7932, Adjusted R-squared: 0.7816 #&gt; F-statistic: 68.57 on 8 and 143 DF, p-value: &lt; 2.2e-16 Se eliminan algunas variables no significativas. Pueden ser importantes para ontener un modelo explicativo pero no para un modelo predictivo. # Eliminamos variables no informativas formula &lt;- precio ~ motor_s + caballos + anchura + longitud + peso_revestimiento + kpl summary(lm(formula , data=CarSales)) #&gt; #&gt; Call: #&gt; lm(formula = formula, data = CarSales) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -13.810 -3.132 -0.354 2.399 32.529 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 23.95706 17.06820 1.404 0.16257 #&gt; motor_s -3.82675 1.24211 -3.081 0.00247 ** #&gt; caballos 0.26213 0.01778 14.747 &lt; 2e-16 *** #&gt; anchura -0.42581 0.27043 -1.575 0.11753 #&gt; longitud -0.25383 0.06039 -4.203 4.58e-05 *** #&gt; peso_revestimiento 9.98519 1.85702 5.377 2.96e-07 *** #&gt; kpl 0.44621 0.23777 1.877 0.06258 . #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 6.715 on 145 degrees of freedom #&gt; Multiple R-squared: 0.7917, Adjusted R-squared: 0.7831 #&gt; F-statistic: 91.85 on 6 and 145 DF, p-value: &lt; 2.2e-16 Finalmente se seleccionan algunas variables explicativas Xs Xs &lt;- CarSales[,c(&quot;motor_s&quot;,&quot;caballos&quot;,&quot;anchura&quot;,&quot;longitud&quot;,&quot;peso_revestimiento&quot;)] 5.11.4 Evaluamos matriz de correlaciones La multicolinealidad en un modelo de regresión múltiple implica la existencia de altas correlaciones entre las variables independientes, generando inestabilidad numérica y elevando la varianza de los estimadores de los coeficientes. Esto dificulta la interpretación precisa de los efectos individuales de las variables predictoras, así como la evaluación confiable de la importancia relativa de cada predictor en la predicción del resultado. library(ggcorrplot) ggcorrplot(cor(Xs),lab_size = 5,lab = TRUE) 5.11.5 Test de adecuación del ACP Test de Bartlett contrasta la hipótesis de que el determinante es distinto de la unidad (matriz identidad) Xs.n &lt;- scale(Xs) library(psych) cortest.bartlett(cor(Xs.n)) #&gt; $chisq #&gt; [1] 356.0616 #&gt; #&gt; $p.value #&gt; [1] 2.059602e-70 #&gt; #&gt; $df #&gt; [1] 10 Indicador KMO sugiere buena adecuación del ACP KMO(Xs.n) #&gt; Kaiser-Meyer-Olkin factor adequacy #&gt; Call: KMO(r = Xs.n) #&gt; Overall MSA = 0.81 #&gt; MSA for each item = #&gt; motor_s caballos anchura #&gt; 0.75 0.74 0.85 #&gt; longitud peso_revestimiento #&gt; 0.84 0.88 5.11.6 Cálculo de las componentes la librería ‘stats’ incluye la función ‘princomp()’ que realiza el ACP. Xs.pca &lt;- princomp(Xs.n) summary(Xs.pca) #&gt; Importance of components: #&gt; Comp.1 Comp.2 Comp.3 #&gt; Standard deviation 1.8864719 0.8581591 0.54305278 #&gt; Proportion of Variance 0.7164689 0.1482628 0.05937187 #&gt; Cumulative Proportion 0.7164689 0.8647317 0.92410355 #&gt; Comp.4 Comp.5 #&gt; Standard deviation 0.50375651 0.3510200 #&gt; Proportion of Variance 0.05109024 0.0248062 #&gt; Cumulative Proportion 0.97519380 1.0000000 Una o como máximo 2 CP serían necesarias para resumir la información. Con dos CPs se está recogiendo el 86% de la información. 5.11.7 Interpretación de las CPs Xs.pca$loadings #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; motor_s 0.482 0.345 0.804 #&gt; caballos 0.421 0.618 0.386 -0.540 #&gt; anchura 0.457 -0.317 -0.175 -0.803 -0.120 #&gt; longitud 0.403 -0.628 0.563 0.356 #&gt; peso_revestimiento 0.468 -0.708 0.478 -0.218 #&gt; #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; SS loadings 1.0 1.0 1.0 1.0 1.0 #&gt; Proportion Var 0.2 0.2 0.2 0.2 0.2 #&gt; Cumulative Var 0.2 0.4 0.6 0.8 1.0 La CP1 es un indicador global. Todas las variables puntúan de forma positiva. La CP2 discrimina coches potentes (xon muchos CVs y cc) frente a coches pequeños. biplot(Xs.pca) summary(lm(CarSales$precio ~ Xs.pca$scores[,1:2])) #&gt; #&gt; Call: #&gt; lm(formula = CarSales$precio ~ Xs.pca$scores[, 1:2]) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -22.567 -4.051 -0.997 2.878 33.487 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value #&gt; (Intercept) 27.3318 0.6601 41.40 #&gt; Xs.pca$scores[, 1:2]Comp.1 4.4839 0.3499 12.81 #&gt; Xs.pca$scores[, 1:2]Comp.2 9.7531 0.7692 12.68 #&gt; Pr(&gt;|t|) #&gt; (Intercept) &lt;2e-16 *** #&gt; Xs.pca$scores[, 1:2]Comp.1 &lt;2e-16 *** #&gt; Xs.pca$scores[, 1:2]Comp.2 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: #&gt; 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 8.138 on 149 degrees of freedom #&gt; Multiple R-squared: 0.6856, Adjusted R-squared: 0.6814 #&gt; F-statistic: 162.5 on 2 and 149 DF, p-value: &lt; 2.2e-16 "],["análisis-cluster.html", "Capítulo 6 Análisis Cluster 6.1 Medidas de distancia 6.2 Análisis cluster con el algoritmo K-medias. 6.3 Un ejemplo de análisis clúster con R 6.4 Comentarios finales sobre el algoritmo k-medias 6.5 Análisis clúster jerárquico", " Capítulo 6 Análisis Cluster El análisis clúster es un conjunto amplio de técnicas para encontrar subgrupos de observaciones dentro de un conjunto de datos. Cuando agrupamos observaciones, queremos que las observaciones en el mismo grupo sean similares y que las observaciones en diferentes grupos sean diferentes. Dado que no hay una variable de respuesta, este es un método no supervisado, lo que implica que busca encontrar relaciones entre las \\(n\\) observaciones sin ser entrenado por una variable de respuesta. El agrupamiento nos permite identificar qué observaciones son similares y potencialmente categorizarlas como tales. El agrupamiento K-medias (K-means) es el método de agrupamiento más simple y comúnmente utilizado para dividir un conjunto de datos en un conjunto de k grupos o clusters. Ejemplo de tres cluster en un conjunto de datos Para realizar un análisis de clúster en R, por lo general, los datos deben estar preparados de la siguiente manera: 1.- Las filas representan observaciones (individuos) y las columnas representan variables. 2.- Cualquier caso con valor omitido en los datos debe ser eliminado o estimado. 3.- Los datos deben estar estandarizados para hacer que las variables sean comparables. Recuerde que la estandarización consiste en transformar las variables de manera que tengan una media cero y una desviación estándar de uno. Estas son las tres librerias de R que necesitaremos library(tidyverse) # Manipulación de datos library(cluster) # Algoritmos de clustering library(factoextra) # Algoritmos de clustering y visualización 6.1 Medidas de distancia La clasificación de observaciones en grupos requiere algunos métodos para calcular la distancia o la (dis)similitud entre cada par de observaciones. El resultado de este cálculo se conoce como una matriz de disimilitud o distancia. Existen muchos métodos para calcular esta distancia. La elección de las medidas de distancia es un paso crítico en el agrupamiento lod datos y afectará la forma de los grupos o clusters. Los métodos clásicos para las medidas de distancia son las distancias Euclidea y Manhattan, aunque también existen otras distancias basadas en correlaciones: Distancia Euclidea \\[d_E(x,y)=\\sqrt{\\sum\\limits_{i=1}^n(x_i-y_i)^2}\\] Distancia Manhattan \\[d_M(x,y)=\\sum\\limits_{i=1}^n|x_i-y_i|\\] Distancia Correlación de Pearson \\[d_P(x,y)=1-\\frac{\\sum\\limits_{i=1}^n(x_i-\\bar{x})(y_i-\\bar{y})}{\\sqrt{\\sum\\limits_{i=1}^n(x_i-\\bar{x})^2\\sum\\limits_{i=1}^n(y_i-\\bar{y})^2}}\\] Distancia Correlación de Spearman \\[d_S(x,y)=1-\\frac{\\sum\\limits_{i=1}^n(\\tilde{x}_i-\\bar{\\tilde{x}})(\\tilde{y}_i-\\bar{\\tilde{y}})}{\\sqrt{\\sum\\limits_{i=1}^n(\\tilde{x}_i-\\bar{\\tilde{x}})^2\\sum\\limits_{i=1}^n(\\tilde{y}_i-\\bar{\\tilde{y}})^2}}\\] donde hemos denotado por \\(\\tilde{x}_i=rank(x_i)\\) el rango de la observación \\(x_i\\). Para el caso en el que los datos a clusterizar sean secuencias de caracteres (tipo categóricos), tambien se puede calcular la Distancia de Hamming que se calcula contando el número de posiciones en las que los caracteres correspondientes son diferentes. Se utiliza principalmente en secuencias binarias. Por ejemplo, dadas las secuencias \\(&quot;1010101&quot;\\) y \\(&quot;1001001&quot;\\) la distancia de Hamming sería 3, ya que hay tres posiciones donde son diferentes. 6.2 Análisis cluster con el algoritmo K-medias. El análsis clúster K-medias es el algoritmo de aprendizaje automático no supervisado más comúnmente utilizado para dividir un conjunto de datos en un conjunto de \\(k\\) grupos (es decir, \\(k\\) clústeres), donde \\(k\\) representa el número de grupos predefinido por el analista. La clasificacion de los individuos se hace de tal manera que dentro del mismo clúster son tan similares como sea posible, mientras que los individuos de diferentes clústeres son tan diferentes como sea posible. Cada clúster está representado por su centro (o centroide), que corresponde a la media de los puntos asignados al clúster. La idea básica detrás de la del algoritmo k-medias consiste en definir grupos de manera que la variación total intra-cluster (conocida como variación total dentro del grupo) se minimice. Existen varios algoritmos de k-nedias disponibles. El algoritmo estándar es el algoritmo de Hartigan-Wong (Hartigan y Wong 1979), que define la variación total dentro del grupo como la suma de las distancias euclidianas al cuadrado entre los elementos y el centroide correspondiente: \\[W(C_k)=\\sum\\limits_{x_i\\in C_k}(x_i-\\mu_k)^2\\] donde \\(x_i\\) es el dato perteneciente al cluster \\(C_k\\) y \\(\\mu_k\\) es el valor medio de los puntos asignados al cluster \\(C_k\\). Cada punto \\(x_i\\) se asocia al clúster que verifica que la suma de los cuadrados de las distancias de cada observación al centro de su cluster asignado (valor medio \\(\\mu_k\\)) sea mínimo. Se define la variación total dentro de clúster como \\[VTDC=\\sum_{i=1}^KW(C_k)=\\sum_{i=1}^K\\sum\\limits_{x_i\\in C_k}(x_i-\\mu_k)^2\\] \\(VTDC\\) mide la compacidad (es decir, la calidad) del agrupamiento y queremos que sea lo más pequeña posible. 6.2.1 K-medias El primer paso al utilizar el algoritmo k-medias es indicar el número de grupos/clústeres \\(k\\) que se generarán en la solución final. El algoritmo comienza seleccionando aleatoriamente \\(k\\) individuos del conjunto de datos para que sirvan como los centros iniciales de los clústeres. Los individuos seleccionados también se conocen como medias o centroides del clúster. A continuación, a cada uno de los objetos restantes se le asigna su centroide más cercano, donde “más cercano” se define usando la distancia utilizada entre el objeto y la media del grupo. Este paso se llama “paso de asignación de clúster”. Después del paso de asignación, el algoritmo calcula el nuevo valor medio de cada clúster. El término “actualización del centroide del clúster” se utiliza para designar este paso. Ahora que los centros han sido recalculados, se vuelve a verificar cada observación para ver si podría estar más cerca de un grupo diferente. Todos los objetos se vuelven a asignar utilizando las medias actualizadas del grupo. Los pasos de asignación de grupo y actualización del centroide se repiten de forma iterativa hasta que las asignaciones de grupo dejan de cambiar (es decir, hasta que se alcanza la convergencia). Es decir, los grupos formados en la iteración actual son los mismos que los obtenidos en la iteración anterior. El algoritmo \\(K\\)-medias se puede resumir de la siguiente manera: Especificar el número de clúster \\(K\\) que se crearán (por el investigador). Seleccionar aleatoriamente k individuos del conjunto de datos como los centros o medias iniciales del clúster. Asignar cada observación a su centroide más cercano, basado en una distancia entre el individuo y el centroide. Para cada uno de los \\(k\\) clústeres, actualizar el centroide del clúster calculando los nuevos valores medios de todos los puntos de datos en el clúster. El centroide de un \\(i\\)-ésimo clúster es un vector de longitud \\(p\\) que contiene las medias de todas las variables para las observaciones en el \\(i\\)-ésimo clúster y \\(p\\) es el número de variables. Minimizar de forma iterativa la suma total de cuadrados dentro del grupo \\(VTDC\\). Es decir, iterar los pasos 3 y 4 hasta que las asignaciones de clúster dejen de cambiar o se alcance el número máximo de iteraciones. Por defecto, el software R utiliza 10 como valor predeterminado para el número máximo de iteraciones. 6.3 Un ejemplo de análisis clúster con R En esta sección seleccionamos las librerias a utilizar y explicamos las funciones básicas para realizar un análsis cluster por k-medias 6.3.1 Funciones básicas de R para usar K-medias Para realizar un análisis cluster con R necesitamos cargar algunas librerias esenciales. &gt; library(dplyr) &gt; library(magrittr) &gt; library(plyr) &gt; library(ggplot2) &gt; library(plotly) &gt; library(tidyverse) # data manipulation &gt; library(cluster) # clustering algorithms &gt; # install.packages(&quot;factoextra&quot;) &gt; library(factoextra) # clustering algorithms &amp; visualization El fichero de datos que vamos a trabajar es CarSales.xlsx que contine datos de estimaciones de ventas, precios de catálogo y especificaciones físicas hipotéticas de varias marcas y modelos de vehículos. &gt; library(readxl) # read xlsx files &gt; data &lt;- read_xlsx(&quot;Datos/CarSales.xlsx&quot;) Seleccionamos aquellas variables del fichero de datos que son numéricas para realizar el análsis cluster en funcion de ellas guardandolas en el objeto de R del tipo data frame llamado datos. Seguidamente tipifico los datos (acción que consiste en quitar la media y dividir por la desciación tipica) para normalizar las variables a que tengan media 0 y desviación típica 1 y elimino los NAN de la base de datos. &gt; datos=as.data.frame(data[,c(3,4,6:14)])# selecciono aquellos elementos utilizados para la clusterización &gt; df=as.data.frame(scale(na.omit(datos)))# tipifico los datos, elimino los Nan y lo guardo en el data frame df Dentro de R, es sencillo calcular y visualizar la matriz de distancias utilizando las funciones get_dist y fviz_dist del paquete R factoextra. get_dist: para calcular una matriz de distancias entre las filas de una matriz de datos. La distancia predeterminada calculada es la Euclidiana; sin embargo, get_dist también admite otras distancias como las descritas anteriormente, entre otras. fviz_dist: para visualizar una matriz de distancias. &gt; distance &lt;- get_dist(df) &gt; fviz_dist(distance, gradient = list(low = &quot;#00AFBB&quot;, mid = &quot;white&quot;, high = &quot;#FC4E07&quot;)) En R, podemos emplear la función kmeans para llevar a cabo el análisis de k-medias. En el ejemplo que vamos a realizar, se organizarán los datos en dos grupos centers = 2. Además, la función kmeans cuenta con una opción llamada nstart que realiza intentos con múltiples configuraciones iniciales y registra la mejor de ellas. Por ejemplo, al incluir nstart = 25 se generarán 25 configuraciones iniciales. Este método se sugiere con frecuencia. &gt; k2 &lt;- kmeans(df, centers = 2, nstart = 25) Para mostrar la estructura interna del objeto k2 generado al realizar el algoritmo k-medias en el ejemplo anterior podemos utilizar la funcion str &gt; str(k2) #&gt; List of 9 #&gt; $ cluster : Named int [1:117] 2 1 1 2 1 1 2 1 1 1 ... #&gt; ..- attr(*, &quot;names&quot;)= chr [1:117] &quot;1&quot; &quot;2&quot; &quot;4&quot; &quot;5&quot; ... #&gt; $ centers : num [1:2, 1:11] -0.0426 0.0434 0.4457 -0.4533 0.5622 ... #&gt; ..- attr(*, &quot;dimnames&quot;)=List of 2 #&gt; .. ..$ : chr [1:2] &quot;1&quot; &quot;2&quot; #&gt; .. ..$ : chr [1:11] &quot;ventas&quot; &quot;reventa&quot; &quot;precio&quot; &quot;motor_s&quot; ... #&gt; $ totss : num 1276 #&gt; $ withinss : num [1:2] 558 261 #&gt; $ tot.withinss: num 819 #&gt; $ betweenss : num 457 #&gt; $ size : int [1:2] 59 58 #&gt; $ iter : int 1 #&gt; $ ifault : int 0 #&gt; - attr(*, &quot;class&quot;)= chr &quot;kmeans&quot; La función kmeans produce como resultado una lista con información diversa, siendo los elementos más relevantes: 1.- cluster: Un vector de números enteros (del 1 al k), que indica el clúster al que se asigna cada punto. 2.- centers: Una matriz que contiene los centros de los clústeres. 3.- totss: La suma total de los cuadrados. 4.- withinss: Un vector que contiene las sumas de cuadrados dentro de cada clúster, con un componente por clúster. 5.- tot.withinss: La suma total de cuadrados dentro de todos los clústeres, es decir, la suma de withinss. 6.- betweenss: La suma de cuadrados entre clústeres, calculada como totss - tot.withinss. 7.- size: El número de puntos en cada clúster. Al imprimir los resultados, observaremos que nuestras agrupaciones generaron clústeres con tamaños de 61 y 56, respectivamente. &gt; k2 #&gt; K-means clustering with 2 clusters of sizes 59, 58 #&gt; #&gt; Cluster means: #&gt; ventas reventa precio motor_s caballos #&gt; 1 -0.04264890 0.4456548 0.5621683 0.6911197 0.6746937 #&gt; 2 0.04338422 -0.4533385 -0.5718609 -0.7030355 -0.6863263 #&gt; BaseNeumatico anchura longitud peso_revestimiento #&gt; 1 0.5217784 0.6563562 0.5944363 0.7700351 #&gt; 2 -0.5307746 -0.6676727 -0.6046852 -0.7833115 #&gt; tapón_combustible kpl #&gt; 1 0.6509841 -0.5620632 #&gt; 2 -0.6622080 0.5717540 #&gt; #&gt; Clustering vector: #&gt; 1 2 4 5 6 7 9 10 11 12 13 14 15 17 18 #&gt; 2 1 1 2 1 1 2 1 1 1 1 1 1 1 1 #&gt; 20 21 22 23 24 25 26 27 29 30 31 32 33 36 37 #&gt; 2 2 2 1 1 1 2 2 2 2 1 2 1 2 2 #&gt; 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 #&gt; 2 1 1 1 1 1 1 2 2 2 2 1 1 1 1 #&gt; 56 57 58 59 60 61 62 63 64 65 66 68 69 70 71 #&gt; 2 1 2 2 2 1 1 2 2 2 1 2 2 1 1 #&gt; 72 74 77 78 80 81 82 83 84 85 86 87 88 89 90 #&gt; 1 1 1 1 2 2 2 1 2 1 2 2 2 2 1 #&gt; 91 92 93 94 95 96 102 103 104 105 106 109 112 113 114 #&gt; 1 1 2 1 1 1 2 2 1 1 1 2 1 1 1 #&gt; 115 116 117 119 120 121 122 123 125 126 127 130 131 132 137 #&gt; 2 2 1 2 2 1 1 1 2 1 1 2 2 2 2 #&gt; 138 139 140 141 143 144 145 146 147 148 149 150 #&gt; 2 1 2 2 2 2 1 2 2 2 2 2 #&gt; #&gt; Within cluster sum of squares by cluster: #&gt; [1] 557.6675 261.4993 #&gt; (between_SS / total_SS = 35.8 %) #&gt; #&gt; Available components: #&gt; #&gt; [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; #&gt; [4] &quot;withinss&quot; &quot;tot.withinss&quot; &quot;betweenss&quot; #&gt; [7] &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; También es posible visualizar los resultados utilizando la función fviz_cluster. Esto proporciona una representación visual atractiva de los clústeres. En el caso de que haya más de dos dimensiones (variables), fviz_cluster llevará a cabo un análisis de componentes principales (PCA) y representará los puntos de datos en función de los dos primeros componentes principales que explican la mayor parte de la variabilidad. &gt; fviz_cluster(k2, data = df) Alternativamente, puedes utilizar gráficos de dispersión clásicos por pares para ilustrar los clústeres en comparación con las variables originales. &gt; data_sin_NAN=na.omit(data) &gt; modelo=data_sin_NAN$modelo &gt; &gt; df %&gt;% + as_tibble() %&gt;% + mutate(cluster = k2$cluster, + Modelo =modelo) %&gt;% + ggplot(aes(precio, caballos, color = factor(cluster), label = modelo)) + + geom_text() También podemos visualizar las variables y sus medias para cada cluster y así poder visualizar el peso de cada variable en el cluster y cuando ayuda en la clasificación. &gt; df2=as.data.frame(scale(na.omit(datos)))#vuelvo a cargar los datos &gt; df2$clus=as.factor(k2$cluster)#defino una nueva columna de pertenencia a clúster &gt; df2$clus=factor(df2$clus)#lo defino como un factor &gt; data_long=gather(df2,caracteristica,valor,ventas:kpl,factor_key=TRUE)#apilo los datos &gt; &gt; ggplot(data_long,aes(as.factor(x=caracteristica),y=valor, group=clus,colour=clus))+ + stat_summary(fun=mean,geom=&quot;pointrange&quot;,size=1)+ + stat_summary(geom=&quot;line&quot;)+ + geom_point(aes(shape=clus)) #&gt; No summary function supplied, defaulting to `mean_se()` #&gt; Warning: Removed 22 rows containing missing values #&gt; (`geom_segment()`). 6.3.2 Selección del número de clústeres Dado que el número de clústeres (\\(k\\)) debe establecerse antes de iniciar el algoritmo, a menudo es ventajoso utilizar varios valores diferentes de \\(k\\) y examinar las diferencias en los resultados. Podemos ejecutar el mismo proceso para 3, 4 y 5 clústeres, y los resultados se muestran en la figura: &gt; k3 &lt;- kmeans(df, centers = 3, nstart = 25) &gt; k4 &lt;- kmeans(df, centers = 4, nstart = 25) &gt; k5 &lt;- kmeans(df, centers = 5, nstart = 25) &gt; &gt; # Gráficos para comparar &gt; p1 &lt;- fviz_cluster(k2, geom = &quot;point&quot;, data = df) + ggtitle(&quot;k = 2&quot;) &gt; p2 &lt;- fviz_cluster(k3, geom = &quot;point&quot;, data = df) + ggtitle(&quot;k = 3&quot;) &gt; p3 &lt;- fviz_cluster(k4, geom = &quot;point&quot;, data = df) + ggtitle(&quot;k = 4&quot;) &gt; p4 &lt;- fviz_cluster(k5, geom = &quot;point&quot;, data = df) + ggtitle(&quot;k = 5&quot;) &gt; &gt; library(gridExtra) #&gt; #&gt; Attaching package: &#39;gridExtra&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; combine &gt; grid.arrange(p1, p2, p3, p4, nrow = 2) Estos gráficos nos permiten ver las diferentes configuraciones para diferente número de clústeres y nos dan una intuición sobre el número óptimo de clústeres. Para facilitar al analista la determinación de este numero óptimo, a continuación se explican los tres métodos más comunes para determinarlo, Elbow, Silhouette y el estadístico Gap 6.3.2.1 Método Elbow Recordad que la idea que utiliza el algoritmo k-medias para identificar los clústeres es minimizar la variación total dentro de clúster \\[VTDC=\\sum_{i=1}^KW(C_k)=\\sum_{i=1}^K\\sum\\limits_{x_i\\in C_k}(x_i-\\mu_k)^2\\] La suma \\(VTDC\\), mide cuanto de compacto es el agrupamiento, y buscamos minimizar esta medida. Por tanto, podemos seguir el siguiente procedimiento para determinar el número óptimo de clústeres. 1.- Calcula el algoritmo k-medias para diferentes valores de k. Por ejemplo, variando k desde 1 hasta 10 clústeres. 2.- Calcular, para cada valor de k, la variación total dentro de los clústeres \\(VTDC\\). 3.- Representar gráficamente la curva de valores de \\(VTDC\\) en función del número de clústeres k. 4.-El punto en el gráfico donde se observa un quiebre (codo) se considera comúnmente como un indicador del número apropiado de clústeres. Podemos llevar a cabo este procedimiento en R con el siguiente código. Los resultados sugieren que 3 es el número óptimo de clústeres, ya que se observa el codo. &gt; nk=10 &gt; pares=matrix(0,nk,2)# definimos la matriz donde guardar los pares (nº cluster, VTDC) para valores de k=1:nk &gt; for (k in 1:nk){ + pares[k,]=c(k,kmeans(df, k, nstart = 10 )$tot.withinss)# calculamos (nº cluster, VTDC) + } &gt; plot(pares[,1], pares[,2], + type=&quot;b&quot;, pch = 19, frame = FALSE, + xlab=&quot;Número de clústeres K&quot;, + ylab=&quot;Variación total dentro del clúster VTDC&quot;) El método Elbow tambien se puede realizar con la función de R fviz_nbclust de la siguiente forma: &gt; set.seed(123) &gt; fviz_nbclust(df, kmeans, method = &quot;wss&quot;) 6.3.2.2 Método Silhouette El método Silhouette evalúa la calidad de un agrupamiento, es decir, determina como de bien cada individuo se encuentra dentro del clúster. Pasamos ahora a definir la silueta de un individuo \\(i\\) que pertenece al clúster \\(C_I\\) como \\(s(i)=0\\) si \\(|C_I|=1\\) y \\[s(i)=\\frac{b(i)-a(i)}{\\max\\{a(i),b(i)\\}}\\mbox{ si }\\quad |C_I|&gt;1\\] donde \\[a(i)=\\frac{1}{|C_I|-1}\\sum\\limits_{j\\in C_I, j\\neq i}d(i,j)\\] es la distancia media del individuo \\(i\\) a todos los individuos de su cluster y \\[b(i)=\\min\\limits_{k\\neq I}\\frac{1}{|C_k|}\\sum\\limits_{j\\in C_k}d(i,j)\\] la distancia media más pequeña entre las distancias medias del individuo \\(i\\) a todos los individuos de un clúster \\(C_k\\). Es fácil deducir que \\(-1\\leq s(i)\\leq1\\). Un valor pequeño de \\(a(i)\\) significa que el individuo \\(i\\) es muy cercano a los individuos de su clúster, mientras que un valos grande de \\(b(i)\\) significa que el individuo \\(i\\) está muy distante de los indivíduos de su cluster vecino. Por todo ello, un valos de \\(s(i)\\) cercano a 1 significa que el individuo \\(i\\) está en el cluster apropiado, si es cercano a -1 significa que sería más apropiado que el individuo \\(i\\) perteneciera a su clúster vecino y un valor \\(s(i)\\)=0 significa que podría pertenecer a dos clústeres diferentes. Un valor de silueta promedio alto indica un buen agrupamiento. El método del coeficiente de silueta calcula la silueta promedio de las observaciones para diferentes valores de k. El número óptimo de clústeres, k, es aquel que maximiza la silueta promedio a lo largo de un rango de posibles valores para k. En este contexto se define el coeficiente Silhouette como \\[SC=\\max_{k}\\bar{s}(k)\\] donde \\(\\bar{s}(k)\\) denota la media de \\(s(i)\\) para todos los \\(i\\in C_k\\). Para llevar a cabo este enfoque, podemos utilizar la función silhouette del paquete cluster para calcular el ancho promedio de Silhouette. El siguiente código implementa este método para 2-\\(n_k\\) clústeres. &gt; nk=10 &gt; Spares=matrix(0,nk-1,2)# definimos la matriz donde guardar los pares (nº cluster, VTDC) para valores de k=1:nk &gt; cont=0 &gt; for (k in 2:nk){ + cont=cont+1 + resultado&lt;-kmeans(df, k, nstart = 10) + ss&lt;-silhouette(resultado$cluster,dist(df)) + Spares[cont,]=c(k,mean(ss[,3]))# calculamos (nº cluster, VTDC) + } &gt; plot(Spares[,1], Spares[,2], + type=&quot;b&quot;, pch = 19, frame = FALSE, + xlab=&quot;Número de clústeres K&quot;, + ylab=&quot;Silueta promedio&quot;) Los resultados muestran que 3 clústeres maximizan los valores medio de silueta, y que 5 clústeres son el segundo número óptimo. 6.3.3 Visualización del clúster óptimo Con los resultados obtenidos en las secciones anteriores indican que \\(k=3\\) es el número optimo de clústeres, realizaremos el algoritmo k-medias para k=3 y lo representaremos con las función fviz_cluster. &gt; ClusterOpt &lt;- kmeans(df, centers = 3, nstart = 25) &gt; fviz_cluster(ClusterOpt, data = df) 6.4 Comentarios finales sobre el algoritmo k-medias El algoritmo k-medias es un algoritmo muy sencillo y rápido que puede manejar grandes conjuntos de datos eficientemente. Sin embargo, presenta algunas debilidades. Una posible limitación de k-medias es que requiere que especifiquemos previamente el número de clústeres. La agrupación jerárquica es una alternativa que no exige comprometerse con una cantidad específica de clústeres. Además, la agrupación jerárquica tiene la ventaja de generar una representación visual atractiva de las observaciones en forma de un dendrograma. Otra desventaja del algoritmo k-medias es su sensibilidad a valores atípicos, lo que puede resultar en diferentes resultados al cambiar el orden de los datos. 6.5 Análisis clúster jerárquico El análisis clúster jerárquico es un enfoque alternativo al algoritmo k-medias para identificar grupos en el conjunto de datos. No requiere que especifiquemos previamente el número de clústeres a generar, como lo exige el enfoque k-medias. Además, el análisis jerárquico tiene una ventaja adicional sobre k-medias que resulta en una atractiva representación gráfica basada en árbol de las observaciones, llamada dendrograma. Requeriremos el siguiente paquete adicional library(dendextend) # Para comparar dos dendrogramas El análisis clúster jerárquico se clasifica en dos tipos principales: aglomerativo y divisivo. Aglomerativo: También conocido como AGNES (Agglomerative Nesting). En este enfoque, el proceso comienza desde la parte inferior, donde inicialmente cada individuo se considera un clúster individual (hoja). En cada paso, se combinan los dos clústeres más similares en uno más grande (nodo). Este procedimiento se repite hasta que todos los puntos forman parte de un único clúster grande (raíz), y el resultado se visualiza como un dendrograma. Divisivo: También conocido como DIANA (Divise Analysis). Aquí, el proceso se desarrolla de arriba hacia abajo. Comienza desde la raíz, donde todos los objetos están en un solo clúster. En cada iteración, el clúster más heterogéneo se divide en dos. Este proceso se repite hasta que cada objeto tiene su propio clúster. Es relevante destacar que la agrupación aglomerativa es eficiente para identificar clústeres pequeños, mientras que la agrupación divisiva es más efectiva para identificar clústeres grandes. No obstante, una pregunta más significativa es: ¿Cómo medimos la diferencia entre dos conjuntos de observaciones?. Se han desarrollado diversos métodos de aglomeración de clústeres, también conocidos como métodos de enlace, para abordar esta interrogante. Entre los tipos más comunes de estos métodos se encuentran: Maximum or complete linkage clustering: Calcula todas las diferencias entre los elementos del clúster 1 y los elementos del clúster 2, y toma el valor más grande (es decir, el máximo) de estas diferencias como la distancia entre los dos clústeres. Esto tiende a generar clústeres más compactos. Minimum or single linkage clustering: Calcula todas las diferencias entre los elementos del clúster 1 y los elementos del clúster 2, y toma la más pequeña de estas diferencias como criterio de enlace. Esto tiende a producir clústeres largos y “sueltos”. Mean or average linkage clustering: Calcula todas las diferencias entre los elementos del clúster 1 y los elementos del clúster 2, y toma el promedio de estas diferencias como la distancia entre los dos clústeres. Centroid linkage clustering: Calcula la diferencia entre el centroide del clúster 1 (un vector promedio de longitud p variables) y el centroide del clúster 2. Ward’s minimum variance method: Minimiza la varianza total dentro del clúster. En cada paso, se fusionan los pares de clústeres con la mínima distancia entre clústeres. 6.5.1 Análsis clúster jerárquico con R Existen varias funciones de R para realizar análsis clúster jerárquico, pero las más comunes son hclust y agnes para clúster jerárquico aglomerativo y diana para clúster jerárquico divisivo. 6.5.1.1 Análisis clúster jerárquico aglomerativo Podemos realizar el análisis jerárquico aglomerativo con la función hclust. En primer lugar calculamos ma matriz de disimilaridad con dist y lo introducimos en hclust y especificamos el método de aglomeración que queremos utilizar (i.e. “complete”, “average”, “single”, “ward.D”). Una vez realizado esto podemos realizar el dendrograma. &gt; # Matriz de disimilaridad &gt; d &lt;- dist(df, method = &quot;euclidean&quot;) &gt; &gt; # Clúster jerárquico usando Complete Linkage &gt; hc1 &lt;- hclust(d, method = &quot;complete&quot; ) &gt; &gt; # Graficamos el dendrograma &gt; plot(hc1, cex = 0.6, hang = -1) Alternativamente, podemos utilizar la función agnes. Estas funciones se comportan de manera muy similar, sin embargo, con la función agnes, también puedes obtener el coeficiente aglomerativo, que mide la cantidad de estructura de agrupamiento encontrada (valores más cercanos a 1 sugieren una estructura de agrupamiento fuerte). &gt; # Calculado con agnes &gt; hc2 &lt;- agnes(df, method = &quot;complete&quot;) &gt; &gt; # Coeficiente aglomerativo &gt; hc2$ac #&gt; [1] 0.9061342 Esto nos permite determinar que método de agrupación jerarquico obtiene estructuras de agrupación más fuertes. A continuacion calculamos el coeficiente aglomerativo para 4 métodos diferentes obteniendo que el método Ward es el mejor. &gt; m &lt;- c( &quot;average&quot;, &quot;single&quot;, &quot;complete&quot;, &quot;ward&quot;) &gt; results=matrix(0,2,4) &gt; for (i in 1:4){ + results[,i]=c(m[i],agnes(df, method = m[i])$ac) + } &gt; print(results) #&gt; [,1] [,2] #&gt; [1,] &quot;average&quot; &quot;single&quot; #&gt; [2,] &quot;0.843969754548982&quot; &quot;0.723129823717284&quot; #&gt; [,3] [,4] #&gt; [1,] &quot;complete&quot; &quot;ward&quot; #&gt; [2,] &quot;0.90613423703583&quot; &quot;0.949958399929762&quot; De forma analoga a lo realizado anteriormente podemo visualizar el dendrograma. &gt; # Clúster jerárquico usando Complete Linkage &gt; hc3 &lt;- hclust(d, method = &quot;ward.D&quot; ) &gt; &gt; # Graficamos el dendrograma &gt; plot(hc3, cex = 0.6, hang = -1, main=&quot;Dendrograma agnes&quot;) 6.5.1.2 Análisis clúster jerárquico divisivo La funcion de R diana proporcionada en el paquete cluster nos permite realizar el análisis cluster jerarquico divisivo, el cual funciona similar que agnes perono hay método que proporcionar. &gt; # Calculamos el análisis cluster jerarquico divisivo &gt; hc4 &lt;- diana(df) &gt; # El coeficiente de división; Cantidad de estructura de agrupamiento encontrada &gt; hc4$dc #&gt; [1] 0.89846 &gt; # plot dendrogram &gt; pltree(hc4, cex = 0.6, hang = -1, main = &quot;Dendrograma de diana&quot;) 6.5.2 Trabajando con dendrogramas En el dendrograma que se muestra arriba, cada hoja representa una observación. Al ascender por el árbol, las observaciones similares se combinan en ramas, las cuales a su vez se fusionan a una mayor altura. La altura de la fusión, indicada en el eje vertical, refleja la (des)similitud entre dos observaciones. A mayor altura de fusión, menor similitud entre las observaciones. Es importante señalar que las conclusiones sobre la proximidad de dos observaciones se derivan únicamente de la altura donde las ramas que contienen esas dos observaciones se fusionan inicialmente. No podemos usar la proximidad a lo largo del eje horizontal como criterio de similitud. El valor de la altura del corte en el dendrograma determina el número de clústeres obtenidos y desempeña un papel similar a la k en la agrupación k-medias. Para identificar subgrupos (clústeres), podemos realizar un corte en el dendrograma utilizando la función cutree. &gt; # Método de Ward &gt; hc5 &lt;- hclust(d, method = &quot;ward.D2&quot; ) &gt; &gt; # Cortamos el árbol en 3 grupos &gt; sub_grp &lt;- cutree(hc5, k = 3) &gt; &gt; # Numero de miembros en cada grupo &gt; table(sub_grp) #&gt; sub_grp #&gt; 1 2 3 #&gt; 23 62 32 Podemos añadir a la base de datos la pertenencia al cluster de cada individuo de la siguiente forma &gt; BaseDatos=df &gt; BaseDatos %&gt;% mutate(cluster = sub_grp) %&gt;% head #&gt; ventas reventa precio motor_s caballos #&gt; 1 -0.5621358 -0.1440282 -0.3158715 -1.1834292 -0.7045706 #&gt; 2 -0.2628377 0.1588420 0.1717713 0.1433723 0.7461447 #&gt; 4 -0.6731286 1.0075678 1.1329225 0.4276869 0.4901361 #&gt; 5 -0.5157989 0.3639148 -0.1398961 -1.1834292 -0.5338982 #&gt; 6 -0.5373420 0.4759294 0.5640058 -0.2357138 0.3194637 #&gt; 7 -0.7691598 1.8067488 2.5463801 1.0910877 2.1968600 #&gt; BaseNeumatico anchura longitud peso_revestimiento #&gt; 1 -0.76099980 -1.10186337 -1.1059950 -1.1471500 #&gt; 2 0.09608047 -0.25204126 0.3741573 0.3231015 #&gt; 4 0.90347493 0.05956018 0.6413068 0.8807254 #&gt; 5 -0.58709946 -0.84691674 -0.7016607 -0.5459879 #&gt; 6 0.17060919 1.39094816 0.3091750 0.3967816 #&gt; 7 0.70473168 0.79607268 0.7568308 0.9678018 #&gt; tapón_combustible kpl cluster #&gt; 1 -1.21562468 0.8810010 1 #&gt; 2 -0.16149767 0.1998747 2 #&gt; 4 0.04932774 -0.4812516 2 #&gt; 5 -0.37232307 0.6539589 2 #&gt; 6 0.18109362 -0.4812516 3 #&gt; 7 1.55145874 -0.7082937 3 También es posible dibujar el dendrograma con un rectángulo marcando cada cluster. El argumento border se usa para especificar el color de cada rectangulo &gt; require(graphics) &gt; plot(hc5, cex = 0.6) &gt; rect.hclust(hc5, k = 3, border = 2:4) Podemos usar la función fviz_cluster del paquete factoextra para visualizar el resultado. &gt; fviz_cluster(list(data = df, cluster = sub_grp)) Para usar cutree con agnes y diana haremos lo siguiente: &gt; # Corta el árbol agnes() en 3 grupos &gt; hc_a &lt;- agnes(df, method = &quot;ward&quot;) &gt; cutree(as.hclust(hc_a), k = 3) #&gt; 1 2 4 5 6 7 9 10 11 12 13 14 15 17 18 #&gt; 1 2 2 2 3 3 2 2 2 2 3 2 3 3 2 #&gt; 20 21 22 23 24 25 26 27 29 30 31 32 33 36 37 #&gt; 1 2 2 2 2 3 1 1 2 2 2 2 3 1 2 #&gt; 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 #&gt; 2 3 3 3 3 3 3 1 2 2 2 3 2 3 3 #&gt; 56 57 58 59 60 61 62 63 64 65 66 68 69 70 71 #&gt; 2 3 1 2 2 2 3 1 1 2 2 2 2 2 2 #&gt; 72 74 77 78 80 81 82 83 84 85 86 87 88 89 90 #&gt; 2 3 3 3 1 2 2 2 2 2 2 2 2 2 3 #&gt; 91 92 93 94 95 96 102 103 104 105 106 109 112 113 114 #&gt; 2 3 2 2 3 3 1 2 2 3 2 2 3 2 3 #&gt; 115 116 117 119 120 121 122 123 125 126 127 130 131 132 137 #&gt; 1 2 3 2 2 2 2 2 2 3 3 1 1 1 1 #&gt; 138 139 140 141 143 144 145 146 147 148 149 150 #&gt; 2 2 1 1 1 2 3 1 1 2 1 1 &gt; &gt; # Corta el árbol diana() en 3 grupos &gt; hc_d &lt;- diana(df) &gt; cutree(as.hclust(hc_d), k = 3) #&gt; 1 2 4 5 6 7 9 10 11 12 13 14 15 17 18 #&gt; 1 1 2 1 1 2 1 1 1 1 2 1 2 2 1 #&gt; 20 21 22 23 24 25 26 27 29 30 31 32 33 36 37 #&gt; 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 #&gt; 38 40 41 42 43 44 46 47 48 49 50 52 53 54 55 #&gt; 1 2 3 2 2 1 1 1 1 1 1 2 1 2 2 #&gt; 56 57 58 59 60 61 62 63 64 65 66 68 69 70 71 #&gt; 1 3 1 1 1 1 2 1 1 1 1 1 1 1 1 #&gt; 72 74 77 78 80 81 82 83 84 85 86 87 88 89 90 #&gt; 1 2 2 2 1 1 1 1 1 2 1 1 1 1 2 #&gt; 91 92 93 94 95 96 102 103 104 105 106 109 112 113 114 #&gt; 1 1 1 2 2 2 1 1 1 1 1 1 2 1 2 #&gt; 115 116 117 119 120 121 122 123 125 126 127 130 131 132 137 #&gt; 1 1 1 1 1 1 1 1 1 2 2 1 1 1 1 #&gt; 138 139 140 141 143 144 145 146 147 148 149 150 #&gt; 1 1 1 1 1 1 2 1 1 1 1 1 Finalmente podemos comparar dos dendrogramas. Aqui comparamos el cluster jerárquico con complete linkage versus el método de Ward. La función tanglegram dibuja dos dendrogramas a cada lado con las etiquetas conectadas por lineas. &gt; # Calcula la matriz de distancia &gt; res.dist &lt;- dist(df, method = &quot;euclidean&quot;) &gt; &gt; # Calculamos 2 cluster jerárquicos &gt; hc1 &lt;- hclust(res.dist, method = &quot;complete&quot;) &gt; hc2 &lt;- hclust(res.dist, method = &quot;ward.D2&quot;) &gt; &gt; # Creamos dos dendrogramas &gt; dend1 &lt;- as.dendrogram (hc1) &gt; dend2 &lt;- as.dendrogram (hc2) &gt; &gt; tanglegram(dend1, dend2) La salida muestra la union mediante líneas de los mismos nodos en ambos árboles. La calidad de la alineación de los dos árboles se puede medir utilizando la función “entanglement” (enredo). El entanglement es una medida entre 1 (entrelazamiento completo) y 0 (sin entrelazamiento). Un coeficiente de entrelazamiento más bajo corresponde a una buena alineación. La salida de tanglegram se puede personalizar utilizando muchas otras opciones de la siguiente manera: &gt; if(!require(dendextend)) {install.packages(&quot;dendextend&quot;)} &gt; library(dendextend) &gt; dend_list &lt;- dendlist(dend1, dend2) &gt; tanglegram(dend1, dend2, + highlight_distinct_edges = FALSE, # Turn-off dashed lines + common_subtrees_color_lines = FALSE, # Turn-off line colors + common_subtrees_color_branches = TRUE, # Color common branches + main = paste(&quot;Entrelazamiento =&quot;, round(entanglement(dend_list), 2)) + ) "],["métodos-basados-en-árboles.html", "Capítulo 7 Métodos basados en árboles 7.1 Introducción 7.2 Origen de los Árboles de Decisión 7.3 Los Árboles de Decisión en las técnicas de Machine Learning 7.4 Árboles de regresión 7.5 Árboles de clasificación 7.6 Ventajas y desventajas de los Árboles de Decisión 7.7 Paquetes de R 7.8 Ejemplo Árbol de Clasificación 7.9 Ejemplo Árbol de Regresión 7.10 Webs 7.11 Referencias", " Capítulo 7 Métodos basados en árboles 7.1 Introducción Los árboles de decisión son modelos predictivos formados por reglas binarias (si/no) con las que se consigue repartir las observaciones en función de sus atributos y predecir así el valor de la variable respuesta. Los métodos basados en árboles se han convertido en uno de los referentes dentro del ámbito predictivo debido a los buenos resultados que generan en problemas muy diversos. Los árboles de decisión son uno de los métodos más simples y fáciles de interpretar para realizar predicciones en problemas de clasificación y de regresión. Estos métodos fueron desarrollados a partir de los años 70 del siglo XX como una alternativa versátil a los métodos clásicos de la estadística, fuertemente basados en las hipótesis de linealidad y de normalidad, y enseguida se convierten en una técnica básica del aprendizaje automático. Aunque su calidad predictiva es mediocre (especialmente en el caso de regresión), constituyen la base de otros métodos altamente competitivos (métodos de ensamblado en paralelo o en serie) en los que se combinan múltiples árboles para mejorar la predicción, pagando el precio, eso sí, de hacer más difícil la interpretación del modelo resultante. Los métodos basados en árboles son simples y útiles para la interpretación. Sin embargo, por lo general no son competitivos con los mejores enfoques de aprendizaje supervisado en términos de precisión de predicción. Por lo tanto, también presentamos métodos alternativos que implican la producción de múltiples árboles que luego se combinan para producir una sola predicción de consenso. Veremos que la combinación de una gran cantidad de árboles a menudo puede resultar en mejoras importantes en la precisión de la predicción. Mientras que la Regresión tiene una doble función (interpretación/preducción) los árboles deshechan de forma autómatica todas aquellas variables que no son últiples para la predicción 7.2 Origen de los Árboles de Decisión Es difícil datar el orígen de una metodología. El primer artículo que desarrolla un enfoque de “árbol de decisión” data de 1959 y un investigador británico, William Belson, en un artículo titulado Matching and Prediction on the Principle of Biological Classification,1 cuyo resumen describe su enfoque como una forma de emparejar muestras de población y desarrollar criterios para hacerlo. El uso de árboles de decisión tuvo su origen en las ciencias sociales con los trabajos de Sonquist y Morgan el año 1964 y Morgan y Messenger el año 1979, ambos realizados en la Universidad de Michigan. El programa para la “Detección de Interacciones Automáticas”, creada el año 1971 por los investigadores Sonquist, Baker y Morgan, fue uno de los primeros métodos de ajuste de los datos basados en árboles de clasificación. En estadística, el año 1980, Kass introdujo un algoritmo recursivo de clasificación no binario, llamado “Detección de Interacciones Automáticas Chi-cuadrado”. Hacia el año 1984, los investigadores Breiman, Friedman, Olshen y Stone, introdujeron un nuevo algoritmo para la construcción de árboles y los aplicaron a problemas de regresión y clasificación. El método es conocido como “Árboles de clasificación y regresión”. Casi al mismo tiempo el proceso de inducción mediante árboles de decisión comenzó a ser usado por la comunidad de “Aprendizaje automático”. Los creadores de la metodología del árbol de clasificación con aplicación al aprendizaje automático, también llamada metodología CART, fueron Leo Breiman, Jerome Friedman, Richard Olshen y Charles Stone. Su aplicación en el ámbito de la Estadística se inició en 1984.2 7.3 Los Árboles de Decisión en las técnicas de Machine Learning Los algoritmos de aprendizaje automático pueden clasificarse en dos grupos: Supervisados. No supervisados. reforzar el aprendizaje. Aprendizaje supervisado: El aprendizaje supervisado es un tipo de paradigma de aprendizaje automático en el que un modelo es entrenado utilizando un conjunto de datos etiquetado. En este enfoque, el algoritmo recibe un conjunto de ejemplos de entrada junto con sus correspondientes salidas deseadas (etiquetas) durante la fase de entrenamiento. El objetivo del modelo es aprender una función que mapee las entradas a las salidas de manera que pueda hacer predicciones precisas sobre nuevos datos no etiquetados. En términos más simples, en el aprendizaje supervisado, el modelo aprende a partir de ejemplos conocidos y luego se le proporcionan nuevos ejemplos para hacer predicciones. La “supervisión” se refiere a la idea de que el proceso de entrenamiento del modelo está supervisado por las etiquetas proporcionadas en el conjunto de datos de entrenamiento. Este enfoque se utiliza en una variedad de tareas, como clasificación y regresión. En la clasificación, el modelo se entrena para asignar las entradas a categorías predefinidas, mientras que en la regresión, el objetivo es predecir valores numéricos continuos. El aprendizaje supervisado es fundamental en la construcción de muchos sistemas de inteligencia artificial y toma su nombre del hecho de que el modelo es “supervisado” durante el proceso de aprendizaje con la ayuda de las etiquetas. Un árbol de decisión es un algoritmo supervisado de aprendizaje automático porque para que el algoritmo aprenda es necesaria una variable dependiente y nuestra meta es obtener una ‘función’ que permita predecir, a partir de variables independientes, el valor de la variable objetivo para casos desconocidos. Aprendizaje no supervisado: Los algoritmos de aprendizaje no supervisado trabajan de forma muy similar a los supervisados, con la diferencia de que éstos sólo ajustan su modelo predictivo tomando en cuenta los datos de entrada, sin importar los de salida. Es decir, a diferencia del supervisado, los datos de entrada no están clasificados ni etiquetados, y no son necesarias estas características para entrenar el modelo. Dentro de este tipo de algoritmos, el agrupamiento o clustering en inglés, es el más utilizado, ya que particiona los datos en grupos que posean características similares entre sí La Figura \\(\\ref{fig:algoritmos}\\) muestra esquemáticamente los principales algoritmos de Machine Learning. Algoritmos de Machine Learning según su método de aprendizaje 7.4 Árboles de regresión Los árboles de regresión se aplican cuando la variable respuesta es continua. En términos generales, en el entrenamiento de un árbol de regresión, las observaciones se van distribuyendo por bifurcaciones (nodos) generando la estructura del árbol hasta alcanzar un nodo terminal. Cuando se quiere predecir una nueva observación, se recorre el árbol acorde al valor de sus predicciones hasta alcanzar uno de los nodos terminales. La predicción del árbol es la media/mediana/moda de la variable respuesta de las observaciones de entrenamiento que están en ese mismo nodo terminal. 7.4.1 Un ejemplo de juguete Usaremos el mismo ejemplo que aparece en James Gareth et al.3 (pag. 304) para ilustrar cómo funcionan los árboles. En este ejemplo, objetivo es predecir una variable cuantitativa, el salario de los jugadores de béisbol, según los Años (cantidad de años que ha jugado en las ligas principales) y la cantidad de Hits/aciertos del año anterior. El siguiente código muestra un ejemplo básico de obtención de un árbol. Nótese que antes de realizar el análisis se eliminarán las observaciones a las que les faltan valores de Salario. También se transforma logarítmicamente el Salario para que su distribución tenga una forma de campana más típica (Recuerde que el salario se mide en miles de dólares). if(!require(ISLR)) {install.packages(&quot;ISLR&quot;)} #&gt; Loading required package: ISLR library(ISLR) # provide the collection of data-sets used in the book &#39;An Introduction to Statistical Learning with Applications in R&#39; if(!require(tree)) {install.packages(&quot;tree&quot;)} #&gt; Loading required package: tree library(tree) # Classification and regression trees data(Hitters) Hitters &lt;- Hitters[!is.na(Hitters$Salary),] formula &lt;- log(Salary) ~ Years + Hits myfirsttree &lt;- tree(formula, data = Hitters, control=tree.control(300, mincut = 60)) plot(x = myfirsttree, type = &quot;proportional&quot;) text(x = myfirsttree, splits = TRUE, pretty = 0, cex = 1, col = &quot;firebrick&quot;) Figure 7.1: Árbol de regresión La Figura \\(\\ref{fig:myfirsttree}\\) muestra un árbol de regresión ajustado a estos datos. Nótese que tiene forma de árbol (ver Sección \\(\\ref{sec:terminologia}\\)). Consiste en una serie de reglas de división binaria, comenzando en la parte superior del árbol. La división superior asigna observaciones que tienen menos de 4,5 años (Years &lt; 4.5) a la rama izquierda. El salario previsto para estos jugadores viene dado por el valor de respuesta media para los jugadores en el conjunto de datos con (Años&lt;4.5). Para tales jugadores, el salario logarítmico medio es 5,107, por lo que hacemos una predicción de 165174$, para estos jugadores. Los jugadores con (\\(Years \\geq 4.5\\)) se asignan a la rama derecha, y luego ese grupo se subdivide aún más por ‘Hits’. En general, el árbol estratifica o segmenta a los jugadores en tres regiones del espacio predictor: jugadores que han jugado durante cuatro años y medio o menos, jugadores que han jugado durante cuatro años y medio o más y que hicieron menos de 118 aciertos el año pasado y jugadores que han jugado durante cuatro años y medio o más y que hizo al menos 118 hits el año pasado. Estas tres regiones se pueden escribir como: \\(R_1\\) = { X | Años &lt; 4.5 }, \\(R_2\\)={X | Años&gt;=4.5, Hits &lt; 117.5 } y \\(R_3\\) = {X | años &gt;= 4.5, Hits &gt;= 117.5 }. La Figura \\(\\ref{fig:particion0}\\) ilustra las regiones en función de Años y Hits/Aciertos. Los salarios pronosticados para estos tres grupos son 1000 × exp(5,107) = 165 174\\(, 1000 × exp(5,999) = 402 834\\) y 1,000 × exp(6.740)=845,346$ respectivamente. plot(Hitters$Years, Hitters$Hits, col=&quot;orange&quot;, pch=16 ,xlab=&quot;Years&quot;, ylab=&quot;Hits&quot;) partition.tree(myfirsttree, cex = 2, add = TRUE) Figure 7.2: Partición del espacio con predicciones Podríamos interpretar el árbol de regresión que se muestra en la Figura \\(\\ref{fig:myfirsttree}\\) de la siguiente manera: los años son el factor más importante para determinar el salario, y los jugadores con menos experiencia ganan salarios más bajos que los jugadores más experimentados. Dado que un jugador tiene menos experiencia, la cantidad de golpes que hizo en el año anterior parece jugar un papel pequeño en su salario. Pero entre los jugadores que han estado en las ligas mayores durante cinco años o más, la cantidad de Hits/aciertos hechos el año anterior sí afecta el salario, y los jugadores que hicieron más Hits/aciertos el año pasado tienden a tener salarios más altos. El árbol de regresión que se muestra en la Figura \\(\\ref{fig:myfirsttree}\\) es probablemente una simplificación excesiva de la verdadera relación entre Aciertos, Años y Salario. Sin embargo, tiene ventajas sobre otros tipos de modelos de regresión. 7.4.2 Terminología Un árbol de decisión en Machine Learning es una estructura de árbol similar a un diagrama de flujo donde un nodo interno representa una característica (o atributo), la rama representa una regla de decisión y cada nodo hoja representa el resultado. El nodo superior en un árbol de decisión en Machine Learning se conoce como el nodo raíz. Aprende a particionar en función del valor del atributo. Divide el árbol de una manera recursiva llamada partición recursiva. Cada nodo en el árbol actúa como un caso de prueba para algún atributo, y cada borde que desciende de ese nodo corresponde a una de las posibles respuestas al caso de prueba. Este proceso es recursivo y se repite para cada subárbol enraizado en los nuevos nodos. La Figura \\(\\ref{fig:arbol}\\) muestra gráficamente la estructura de árbol. Ramas y nodos de un árbol de decisión Nodo raíz (nodo de decisión superior ): Representa a toda la población o muestra y esto se divide en dos o más conjuntos homogéneos. División: Es un proceso de división de un nodo en dos o más subnodos. Nodo de decisión: Cuando un subnodo se divide en subnodos adicionales, se llama nodo de decisión. Nodo de hoja / terminal: Los nodos sin hijos (sin división adicional) se llaman Hoja o nodo terminal. Poda: Cuando reducimos el tamaño de los árboles de decisión eliminando nodos (opuesto a la división), el proceso se llama poda. Rama / Subárbol: Una subsección del árbol de decisión se denomina rama o subárbol. Nodo padre e hijo: Un nodo, que se divide en subnodos se denomina nodo principal de subnodos, mientras que los subnodos son hijos de un nodo principal. 7.4.3 Como se crea un árbol En esta sección se presenta el proceso de construcción de un árbol de decisión. Básicamente, los árboles se construyen en dos dos pasos: Paso 1: Se divide el espacio de predicciones, es decir, el conjunto de valores posibles para \\(X_1, X_2,..., X_p\\), en J regiones distintas y no solapadas, \\(R_1, R_2,..., R_J\\). Paso 2: Para cada observación que cae en la región \\(R_j\\), se hace la misma predicción, que simplemente es la media (o la mediana o la moda) de los valores de respuesta para las observaciones de entrenamiento en \\(R_j\\). ¿Cómo se construyen las regiones \\(R_1, R_2,..., R_J\\)? En teoría, podrían tener cualquier forma. Sin embargo, por simplicidad se divide el espacio de predicciones en rectángulos o cajas (Figura \\(\\ref{fig:cajas}\\)). Esta división también ayuda en la interpretación del modelo predictivo resultante. Hay también posibilidad de realizar divisiones oblicuas4 o flexibles.5 División binaria recursiva En el caso de los árboles de regresión, el criterio empleado con más frecuencia para identificar las divisiones es el observar la suma de los residuos al cuadrado (Residual Sum of Squares, \\(RSS\\)) que es una medida de la discrepancia entre los datos reales y los predichos por el modelo. El objetivo es encontrar las J regiones \\((R_1,…, R_j)\\) que minimizan el \\(RSS\\) total. Un \\(RSS\\) bajo indica un buen ajuste del modelo a los datos. Por tanto, el objetivo es encontrar cajas \\(R_1, R_2,..., R_J\\)? que minimicen el \\(RSS\\), dado por: \\[ RSS=\\sum_{j=1}^J{\\sum_{i \\in R_j} (y_i-{\\hat y}_{R_j})^2} \\] donde \\({\\hat y}_{R_j}\\) es el valor predicho (para las observaciones de entrenamiento) dentro de la \\(R_j\\) caja. Desafortunadamente, es computacionalmente inviable considerar cada posible partición del espacio de características en cajas J. Por esta razón, se adopta un enfoque de arriba-hacia-abajo (top-down) que se conoce como división binaria recursiva. El enfoque es de arriba-hacia-abajo porque comienza en la parte superior del árbol (en cuyo punto todas las observaciones pertenecen a una sola región) y luego divide sucesivamente el espacio predictor; cada división se indica a través de dos nuevas ramas más abajo en el árbol. Para realizar la división binaria recursiva, primero seleccionamos el predictor \\(X_j\\) y el punto de corte \\(s\\) tal que dividiendo el espacio del predictor en las regiones \\(\\{X|X_j &lt; s\\}\\) y \\(\\{X|X_j \\geq s\\}\\) conduce a la mayor reducción posible en \\(RSS\\). (La notación \\(\\{X|X_j &lt; s\\}\\) significa la región del predictor espacio en el que \\(X_j\\) toma un valor menor que \\(s\\)). Es decir, consideramos todos los predictores \\(X_1,..., X_p\\) y todos los valores posibles del punto de corte s para cada uno de los predictores, y luego elegimos el predictor y el punto de corte tales que el árbol resultante tiene el \\(RSS\\) más bajo. Con mayor detalle, para cualquier j y s, definimos el par de semiplanos \\[ R_1(j,s)=\\{X|X_j &lt; s\\} \\ \\ y \\ \\ R_2(j,s)=\\{X|X_j \\geq s\\} \\] y se buscan los valores de j y s que minimizan la ecuación \\[ RSS={\\sum_{i \\in R_1} (y_i-{\\hat y}_{R_1})^2} + {\\sum_{i \\in R_2} (y_i-{\\hat y}_{R_2})^2} \\] 7.4.4 Evitar el overfitting El proceso de construcción de árboles tiende a reducir rápidamente el error \\(SSR\\), de tal forma que el modelo se ajusta muy bien a las observaciones empleadas como entrenamiento. Como consecuencia, se genera un sobreajuste (overfitting) que reduce su capacidad predictiva al aplicarlo a nuevos datos. La razón de este comportamiento radica en la facilidad con la que los árboles se ramifican adquiriendo estructuras complejas. De hecho, si no se limitan las divisiones, todo árbol termina ajustándose perfectamente a las observaciones de entrenamiento creando un nodo terminal por observación. Existen dos estrategias para prevenir el problema de overfitting de los árboles: - limitar el tamaño del árbol (parada temprana o early stopping) y - el proceso de podado ( pruning). La Figura \\(\\ref{fig:sobreajuste}\\) muestra el comportamiento del overffiting. Sobre-ajuste 7.4.4.1 Controlar el tamaño del árbol (parada temprana) El tamaño final que adquiere un árbol puede controlarse mediante reglas de parada que detengan la división de los nodos dependiendo de si se cumplen o no determinadas condiciones. El nombre de estas condiciones puede variar dependiendo del software o librería empleada, pero suelen estar presentes en todos ellos. Observaciones mínimas para división: define el número mínimo de observaciones que debe tener un nodo para poder ser dividido. Cuanto mayor el valor, menos flexible es el modelo. Observaciones mínimas de nodo terminal: define el número mínimo de observaciones que deben tener los nodos terminales. Su efecto es muy similar al de observaciones mínimas para división. Profundidad máxima del árbol: define la profundidad máxima del árbol, entendiendo por profundidad máxima el número de divisiones de la rama más larga (en sentido descendente) del árbol. Número máximo de nodos terminales: define el número máximo de nodos terminales que puede tener el árbol. Una vez alcanzado el límite, se detienen las divisiones. Su efecto es similar al de controlar la profundidad máxima del árbol. Reducción mínima de error: define la reducción mínima de error que tiene que conseguir una división para que se lleve a cabo. A todos estos parámetros se les conoce como hiperparámetros porque no se aprenden durante el entrenamiento del modelo. Su valor tiene que ser especificado por el usuario en base a su conocimiento del problema y mediante el uso de validación cruzada. 7.4.4.2 Post-pruning El proceso descrito anteriormente puede producir buenas predicciones en el conjunto de entrenamiento, pero es probable que se produzca un sobreajuste los datos, lo que conducirá a un rendimiento deficiente del conjunto de prueba. Esto se debe a que el árbol resultante puede ser demasiado complejo. Un árbol más pequeño con menos divisiones (menos regiones \\(R_1,... , R_J\\)) podría generar una aceptable predicción y una mejor interpretación a costa de un pequeño sesgo. Una posible alternativa al proceso descrito anteriormente es construir el árbol solo mientras la disminución en el \\(RSS\\) debido a cada división exceda algún umbral (threshold). Esta estrategia dará como resultado árboles más pequeños, pero es demasiado miope, ya que una división aparentemente sin valor en un nodo al principio del árbol podría ser seguida por una muy buena división en otro nodo posterior que conduzca a una gran reducción en \\(RSS\\). Una estrategia mejor sería hacer crecer un árbol \\(T_0\\) muy grande y luego podarlo para obtener un subárbol. La cuestión es: ¿Cómo determinamos la mejor manera de podar el árbol? Intuitivamente, nuestro objetivo es seleccionar un subárbol que conduzca a la tasa de error más baja. Dado un subárbol, se podría por ejemplo estimar su error utilizando un proceso de validación cruzada. Sin embargo, aplicar este procedimiento para cada posible subárbol sería demasiado engorroso, ya que es posible identificar una cantidad extremadamente grande de subárboles. En su lugar, es necesario encontrar una forma de seleccionar un pequeño conjunto de subárboles para su consideración/evaluación. La reducción de la complejidad de los costos, también conocida como reducción del eslabón más débil, brinda una manera de hacer precisamente esto. En lugar de considerar todos los subárboles posibles, consideramos una secuencia de árboles indexados por un parámetro de ajuste no negativo \\(\\alpha\\). En este caso, se busca el sub-árbol T que minimiza la ecuación (Cost complexity pruning) \\[ \\sum_{j=1}^{|T|}{\\sum_{i \\in R_j} (y_i-{\\hat y}_{R_j})^2} + \\alpha|T| \\] es lo más pequeño posible. Aquí |T| indica el número de nodos terminales del árbol T. El primer término de la ecuación se corresponde con el sumatorio total de los residuos cuadrados RSS. Por definición, cuantos más nodos terminales tenga el modelo menor será esta parte de la ecuación. El segundo término es la restricción, que penaliza al modelo en función del número de nodos terminales (a mayor número, mayor penalización). El grado de penalización se determina mediante el parámetro \\(\\alpha\\) que es la medida de costo-complejidad (cost complexity). Cuando \\(\\alpha\\)=0, la penalización es nula y el árbol resultante es equivalente al árbol original. A medida que se incrementa \\(\\alpha\\) la penalización es mayor y, como consecuencia, los árboles resultantes son de menor tamaño. El valor óptimo de \\(\\alpha\\) puede identificarse mediante cross validation. El parámetro \\(\\alpha\\) controla un equilibrio entre la complejidad del subárbol y su ajuste a los datos de entrenamiento. Cuando \\(\\alpha\\)= 0, entonces el subárbol T simplemente será igual a \\(T_0\\), porque entonces solo mide el error de entrenamiento. Sin embargo, a medida que aumenta \\(\\alpha\\), hay que pagar un precio por tener un árbol con muchos nodos terminales, por lo que la cantidad tenderá a minimizarse para un subárbol más pequeño. Resulta que a medida que se aumentea el valor de \\(\\alpha\\) desde cero, las ramas se eliminan del árbol de forma anidada y predecible, por lo que es fácil obtener la secuencia completa de subárboles en función de \\(\\alpha\\). Es posible seleccionar un valor de \\(\\alpha\\) usando un conjunto de validación o usando validación cruzada. 7.4.5 Validación cruzada La validación cruzada o cross-validation es una técnica utilizada para evaluar los resultados de un análisis estadístico cuando el conjunto de datos se ha segmentado en una muestra de entrenamiento y otra de prueba, la validación cruzada (Figura ) comprueba si los resultados del análisis son independientes de la partición. Aunque la validación cruzada es una técnica diseñada para modelos de regresión y predicción, su uso se ha extendido a muchos otros ejercicios de Machine Learning. Esquema de validación cruzada https://commons.wikimedia.org/w/index.php?curid=17617674 La manera más sencilla de realizar la validación cruzada es una vez segmentado el conjunto de datos en la muestra de entrenamiento y test, consiste en resolver el modelo con los datos de entrenamiento, y probar el modelo estimado en la muestra de test, la simple comparación del resultado obtenido en dicha muestra con las observaciones reales nos permite validar el modelo en términos de error (proceso hold-out). Una aplicación alternativa consiste en repetir el proceso anterior, seleccionando aleatoriamente distintos conjuntos de datos de entrenamiento, y calcular los estadísticos de validación a partir de la media de los valores en cada una de las repeticiones. A este método se le denomina Validación cruzada aleatoria (Figura \\(\\ref{fig:esquema}\\)). Los inconvenientes es que hay algunas muestras que quedan sin evaluar y otras que se evalúan más de una vez, es decir, los subconjuntos de prueba y entrenamiento se pueden solapar. Esquema de validación cruzada https://commons.wikimedia.org/w/index.php?curid=17616794 7.4.6 Algoritmo para crear un árbol de regresión con pruning 1. Se emplea recursive binary splitting para crear un árbol grande y complejo (\\(T_0\\)) empleando los datos de training y reduciendo al máximo posible las condiciones de parada. Normalmente se emplea como única condición de parada el número mínimo de observaciones por nodo terminal. 2. Se aplica el cost complexity pruning al árbol \\(T_0\\) para obtener el mejor sub-árbol en función de \\(\\alpha\\). Es decir, se obtiene el mejor sub-árbol para un rango de valores de \\(\\alpha\\). 3. Identificación del valor óptimo de \\(\\alpha\\) mediante k-cross-validation. Se divide la muestra de entrenamiento en K grupos. Para \\(k=1,...,K\\): 4. Repetir pasos 1 y 2 empleando todas las observaciones excepto las del grupo i-ésimo. 5. Evaluar el \\(SSR\\) para el rango de valores de \\(\\alpha\\) empleando el grupo i-ésimo. 6. Obtener el promedio de los K \\(SSR\\) calculados para cada valor \\(\\alpha\\). Seleccionar el sub-árbol del paso 2 que se corresponde con el valor \\(\\alpha\\) que ha conseguido el menor cross-validation mean squared error en el paso 3. En el caso de los árboles de clasificación (ver más adelante), en lugar de emplear la suma de residuos cuadrados \\(SSR\\) como criterio de selección, se emplea una medida alternativa (ver árboles de clasificación). 7.5 Árboles de clasificación Los árboles de clasificación son el subtipo de árboles de decisión que se aplica cuando la variable respuesta es categórica. 7.5.1 Cómo se crea un árbol de clasificación Para construir un árbol de clasificación se emplea el mismo método descrito en los árboles de regresión. Sin embargo, como la variable respuesta en cualitativa, no es posible emplear el \\(RSS\\) como criterio de selección de las divisiones óptimas. Existen varias alternativas, todas ellas con el objetivo de encontrar nodos lo más puros/homogéneos posible. Las más empleadas son: Classification Error Rate: Se define como la proporción de observaciones que no pertenecen a la clase más común en el nodo. \\[ E_m= 1- max (\\hat p_{mk}) \\] donde \\(\\hat p_{mk}\\) representa la proporción de observaciones del nodo m que pertenecen a la clase k. A pesar de la sencillez de esta medida, no es suficientemente sensible para crear buenos árboles, por lo que, en la práctica, suelen emplearse otras medidas. Gini Index: Es una medida de la varianza total en el conjunto de las K clases del nodo m. Se considera una medida de pureza del nodo. \\[ G = \\sum_{k=1}^K \\hat p_{mk}(1-\\hat p_{mk}) \\] Cuando \\(\\hat p_{mk}\\) es cercano a 0 o a 1 (el nodo contiene mayoritariamente observaciones de una clase), el término \\(\\hat p_{mk}(1-\\hat p_{mk})\\) es muy pequeño. Como consecuencia, cuanto mayor sea la pureza del nodo, menor el valor del índice Gini G. Cross Entropy: La entropía es otra forma de cuantificar el desorden de un sistema. En el caso de los nodos, el desorden se corresponde con la impureza. Si un nodo es puro, contiene únicamente observaciones de una clase, su entropía es cero. Por el contrario, si la frecuencia de cada clase es la misma, el valor de la entropía alcanza el valor máximo de 1. \\[ D = \\sum_{k=1}^K \\hat p_{mk}log(\\hat p_{mk}) \\] Chi-Square: Esta aproximación consiste en identificar si existe una diferencia significativa entre los nodos hijos y el nodo parental, es decir, si hay evidencias de que la división consigue una mejora. Para ello, se aplica un test estadístico chi-square goodness of fit empleando como distribución esperada \\(H_0\\) la frecuencia de cada clase en el nodo parental. Cuanto mayor el estadístico \\(\\chi^2\\), mayor la evidencia estadística de que existe una diferencia. \\[ \\chi^2=\\sum_k{(observado_k-esperado_k) \\over esperado_k} \\] Los árboles generados con este criterio de división reciben el nombre de CHAID (Chi-square Automatic Interaction Detector). Independientemente de la medida empleada como criterio de selección de las divisiones, el proceso siempre es el mismo. 7.5.2 Predicción del árbol Tras la creación de un árbol, las observaciones de entrenamiento quedan agrupadas en los nodos terminales. Para predecir una nueva observación, se recorre el árbol en función del valor de sus predictores hasta llegar a uno de los nodos terminales. En el caso de clasificación, suele emplearse la moda de la variable respuesta como valor de predicción, es decir, la clase más frecuente del nodo. Además, puede acompañarse con el porcentaje de cada clase en el nodo terminal, lo que aporta información sobre la confianza de la predicción. 7.6 Ventajas y desventajas de los Árboles de Decisión 7.6.1 Ventajas (algunas) Fácil de entender Util en exploración de datos:identificar importancia de variables a partir de cientos de variables. Menos limpieza de datos: outliers y valores faltantes no influencian el modelo (A un cierto grado) El tipo de datos no es una restricción Es un método no paramétrico 7.6.2 Desventajas (algunas) Sobreajuste Pérdida de información al categorizar variables continuas Inestabilidad: un pequeño cambio en los datos puede modificar ampliamente la estructura del árbol. Por lo tanto la interpretación no es tan directa como parece. Un árbol de decisión puede llegar a ser demasiado complejo con facilidad, perdiendo su utilidad. 7.7 Paquetes de R Algunas librerías que permiten crear y representar árboles de regresión y clasificación son: tree,6 rpart7 o party:8 rpart: Este es el primer paquete de R para árboles de clasificación y árboles de regresión. Tiene funciones de poda y gráficos muy generales. Se complementa con otro paquete rpart.plot9 party: El núcleo del paquete es la función ctree(), una implementación de árboles de inferencia condicional que integran modelos de regresión estructurados en árbol en una teoría bien definida de procedimientos de inferencia condicional. Esta clase no paramétrica de árboles de regresión es aplicable a todo tipo de problemas de regresión, incluidas variables de respuesta nominales, ordinales, numéricas, censuradas y multivariadas y escalas de medición arbitrarias de las covariables. Una de las principales diferencias que tiene con otros métodos de partición es que no hace prunning debido a su sustento estadístico. Es decir, el árbol sigue creciendo hasta que no quede ninguna variable que posea una relación significativa con el target. tree: La función para crear un árbol de regresión es tree, a continuación la estructura de la función. randomForest: dispone de los principales algoritmos para crear modelos Random Forest. Destaca por su fácil uso, pero no por su rapidez. ranger: algoritmos para crear modelos random forest. Es similar a randomForest pero mucho más rápido. Además, incorpora extremely randomized trees y quantile regression forests. gbm: dispone de los principales algoritmos de boosting. Este paquete ya no está mantenido, aunque es útil para explicar los conceptos, no se recomienda su uso en producción. XGBoost: esta librería permite acceder al algoritmo XGboost (Extra Gradient boosting). Una adaptación de gradient boosting que destaca por su eficiencia y rapidez. H2O: implementaciones muy optimizadas de los principales algoritmos de machine learning, entre ellos random forest, gradient boosting y XGBoost. Para conocer más detalles sobre cómo utilizar esta librería consultar Machine Learning con H2O y R. C50: implementación de los algoritmos C5.0 para árboles de clasificación. Existen otros paquetes que el lector puede consultar en la sección Recursive Partitioning de CRAN Task View: Machine Learning &amp; Statistical Learning. 7.8 Ejemplo Árbol de Clasificación Un conjunto de datos clásico en Machine Learning son los datos del Titanic. Este set de datos contiene información sobre los pasajeros del RMS Titanic, el transatlántico británico que se hundió en abril de 1912 durante su viaje inaugural desde Southampton a Nueva York. Entre la información almacenada se encuentran la edad, género, características socio-económicas de los pasajeros y si sobrevivieron o no al naufragio. Aunque pueda resultar un clásico poco original, estos datos tienen una serie de características que los hacen idóneos para ser utilizados como ejemplo introductorio. Se trata de un problema y de unos datos cuyas variables pueden entenderse de forma sencilla. Es intuitivo comprender el impacto que puede tener la edad, el sexo, la localización del camarote en la supervivencia de los pasajeros. Aunque no lo parezca, comprender a fondo el problema que se pretende modelar es lo más importante para lograr buenos resultados. En este ejemplo utilizaremos la librería rpart y rpart.plot para obtener los gráficos de árboles. En primer lugar obtendremos un simple árbol sin ningún tipo de restricciones library(rpart) library(rpart.plot) data(&quot;ptitanic&quot;) arbol_1 &lt;- rpart(survived ~., data = ptitanic, method = &#39;class&#39;) Por defecto, rpart() usa la medida de Gini para la división de los nodos. Los resultados que ofrece el algoritmo son: arbol_1 #&gt; n= 1309 #&gt; #&gt; node), split, n, loss, yval, (yprob) #&gt; * denotes terminal node #&gt; #&gt; 1) root 1309 500 died (0.6180290 0.3819710) #&gt; 2) sex=male 843 161 died (0.8090154 0.1909846) #&gt; 4) age&gt;=9.5 796 136 died (0.8291457 0.1708543) * #&gt; 5) age&lt; 9.5 47 22 survived (0.4680851 0.5319149) #&gt; 10) sibsp&gt;=2.5 20 1 died (0.9500000 0.0500000) * #&gt; 11) sibsp&lt; 2.5 27 3 survived (0.1111111 0.8888889) * #&gt; 3) sex=female 466 127 survived (0.2725322 0.7274678) #&gt; 6) pclass=3rd 216 106 died (0.5092593 0.4907407) #&gt; 12) sibsp&gt;=2.5 21 3 died (0.8571429 0.1428571) * #&gt; 13) sibsp&lt; 2.5 195 92 survived (0.4717949 0.5282051) #&gt; 26) age&gt;=16.5 162 79 died (0.5123457 0.4876543) #&gt; 52) parch&gt;=3.5 9 1 died (0.8888889 0.1111111) * #&gt; 53) parch&lt; 3.5 153 75 survived (0.4901961 0.5098039) #&gt; 106) age&gt;=27.5 44 17 died (0.6136364 0.3863636) * #&gt; 107) age&lt; 27.5 109 48 survived (0.4403670 0.5596330) #&gt; 214) age&lt; 21.5 28 11 died (0.6071429 0.3928571) * #&gt; 215) age&gt;=21.5 81 31 survived (0.3827160 0.6172840) * #&gt; 27) age&lt; 16.5 33 9 survived (0.2727273 0.7272727) * #&gt; 7) pclass=1st,2nd 250 17 survived (0.0680000 0.9320000) * La librería rpart.plot muestra los resultados gráficamente lo que puede ayudar a comprender el funcionamiento del algoritmo. En cada nodo se indica como se clasifican los individuos En segundo lugar se muestra el error de clasificarlos como se indica en la primera fila En la tercera fila el % elementos en ese nodo rpart.plot(arbol_1) Figure 7.3: Árbol base. Por ejemplo, el la primera rama de la izquierda se seleccionan “Hombres” que supone un 64% de la muestra. Todos los hombres (n=843) se clasifican como “Muertos”. La realidad es que de los 843 hombres, se salvaron 161 y por tanto el error = 161/843 = 0.19 Para mejorar este resultado, esta muestra de hombres puede subdividirse dependiendo de la edad para mejorar la predicción. Si la edad es mayor o igual a 9,5 años se clasifican como “Muertos” y si la edad es menor de 9.5, se clasifican como Sobrevivientes” Conforme el arbol va “creciendo” en “ramas” cada vez se comente menores errores. La capacidad predictiva puede ser muy alta si el árbol se deja crecer sin control. # Predicción con sobrajuste # ============================================================================== prediccion_0 &lt;- predict(arbol_1, type = &quot;class&quot;) table_mat &lt;- table(ptitanic$survived, prediccion_0) table_mat #&gt; prediccion_0 #&gt; died survived #&gt; died 749 60 #&gt; survived 169 331 accuracy_Test &lt;- sum(diag(table_mat)) / sum(table_mat) print(paste(&#39;Accuracy for test&#39;, round(accuracy_Test,3))) #&gt; [1] &quot;Accuracy for test 0.825&quot; El árbol sin podado clasifica correctamente el 83% de las observaciones 7.8.1 Pre-podado La Figura \\(\\ref{fig:arbol1}\\) muestra un árbol sin restricciones. Se puede hacer un prepodado controlando el número mínimo de observaciones en un nodo terminal: # Control de prepodado # ============================================================================== control &lt;- list(minbucket = 50) arbol_2 &lt;- rpart(survived ~., data = ptitanic, method = &#39;class&#39;, control = control) library(rpart.plot) rpart.plot(arbol_2, digits = 3) Figure 7.4: Árbol base 2. Otra alternativa sería controlar tanto el número mínimo de observaciones en un nodo (“minbucket=50”) como el número mínimo de observaciones en un nodo para poder ser dividido (“minsplit = 250” si en un nodo hay 250 observaciones o menos, entoncen éste no se dividirá y el arbol no crecerá más por esa rama). # Control de prepodado # ============================================================================== control &lt;- list(minbucket = 50, minsplit = 250) arbol_3 &lt;- rpart(survived ~., data = ptitanic, method = &#39;class&#39;, control = control) library(rpart.plot) rpart.plot(arbol_3, digits = 3) Figure 7.5: Árbol base 3. 7.8.2 Entrenamiento y test En esta subsección se dividirá la muestra en submuestras de entrenamiento y test y se evaluará la capacidad predictiva del árbol # División de los datos en train y test # ============================================================================== set.seed(123) train &lt;- sample(1:nrow(ptitanic), size = .7*nrow(ptitanic)) ptitanic_train &lt;- ptitanic[train,] ptitanic_test &lt;- ptitanic[-train,] El siguiente código obtiene un árbol con la muestra de entrenamiento. Los resultados se muestran en la Figura \\(\\ref{fig:arbol4}\\). control &lt;- list(minbucket = 50, minsplit = 50) arbol_4 &lt;- rpart(survived ~., data = ptitanic_train, method = &#39;class&#39;, control = control) rpart.plot(arbol_4, digits = 3) Figure 7.6: Árbol base 4. La función predict() obtiene las predicciones para la muestra de test y la tabla muestra los resultados de la predicción prediccion_1 &lt;- predict(arbol_4, newdata = ptitanic_test, type = &#39;class&#39;) table_mat &lt;- table(ptitanic_test$survived, prediccion_1) table_mat #&gt; prediccion_1 #&gt; died survived #&gt; died 210 20 #&gt; survived 68 95 La accuracy de este árbol se obtiene con el siguiente código: accuracy_Test &lt;- sum(diag(table_mat)) / sum(table_mat) print(paste(&#39;Accuracy for test&#39;, round(accuracy_Test,3))) #&gt; [1] &quot;Accuracy for test 0.776&quot; Lo ideal sería aplicar un proceso de validación cruzada para ajustar correctamente y encontrar así la mejor combinación de hiperparámetros. 7.9 Ejemplo Árbol de Regresión 7.9.1 Datos El set de datos Boston disponible en el paquete MASS contiene precios de viviendas de la ciudad de Boston, así como información socioeconómica del barrio en el que se encuentran. Se pretende ajustar un modelo de regresión que permita predecir el precio medio de una vivienda (medv) en función de las variables disponibles. if (!require(&quot;MASS&quot;)) install.packages(&quot;MASS&quot;) #&gt; Loading required package: MASS library(MASS) data(&quot;Boston&quot;, package = &quot;MASS&quot;) 7.9.2 Ajuste del modelo con la librería ‘tree’ La función tree() del paquete tree permite ajustar árboles de decisión. La elección entre árbol de regresión o árbol de clasificación se hace automáticamente dependiendo de si la variable respuesta es de tipo numeric o factor. Es importante tener en cuenta que solo estos dos tipos de vectores están permitidos, si se pasa uno de tipo character se devuelve un error. A continuación, se ajusta un árbol de regresión empleando como variable respuesta medv y como predictores todas las variables disponibles. Como en todo estudio de regresión, no solo es importante ajustar el modelo, sino también cuantificar su capacidad para predecir nuevas observaciones. Para poder hacer la posterior evaluación, se dividen los datos en dos grupos, uno de entrenamiento y otro de test. La función tree() crece el árbol hasta que encuentra una condición de stop. Por defecto, estas condiciones son: mincut: Número mínimo de observaciones que debe de tener al menos uno de los nodos hijos para que se produzca la división. Si al dividir el nodo, uno de los subnodos tiene menos de mincut observaciones, entonces el nodo no se divide. minsize: Número mínimo de observaciones que debe de tener un nodo para que pueda dividirse. # División de los datos en train y test equilibrando por cuartiles # ============================================================================== library(rsample) set.seed(123) split &lt;- initial_split(Boston, prop = 0.7, strata = medv) Boston_train &lt;- training(split) Boston_test &lt;- testing(split) 7.9.2.1 Prepodado El siguiente chunk muestra un código para obtener un modelo con pre-podado en función del número mínimo de observaciones # Creación y entrenamiento del modelo # ============================================================================== library(tree) set.seed(123) control &lt;- tree.control(dim(Boston_train)[1], mincut = 20, minsize = 50) arbol_regresion &lt;- tree::tree(formula = medv ~ ., data = Boston_train, control = control) summary(arbol_regresion) #&gt; #&gt; Regression tree: #&gt; tree::tree(formula = medv ~ ., data = Boston_train, control = control) #&gt; Variables actually used in tree construction: #&gt; [1] &quot;rm&quot; &quot;lstat&quot; &quot;nox&quot; &quot;crim&quot; #&gt; Number of terminal nodes: 7 #&gt; Residual mean deviance: 19.77 = 6821 / 345 #&gt; Distribution of residuals: #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -22.1800 -2.1320 0.1656 0.0000 2.0050 27.7700 La función summary() muestra que, el árbol entrenado, tiene un total de 7 nodos terminales y que se han empleado como predictores las variables “rm”, “lstat”, “nox” y “crim”. En el contexto de árboles de regresión, el término Residual mean deviance es la suma de cuadrados residuales dividida entre (número de observaciones - número de nodos terminales). Cuanto menor es la ‘deviance’, mejor es el ajuste del árbol a las observaciones de entrenamiento. El paquete tree no posee una buena capacidad para representar los árboles, pero una vez creado el árbol, se puede representar mediante la combinación de las funciones plot() y text(). La función plot() dibuja la estructura del árbol, las ramas y los nodos. Mediante su argumento type se puede especificar si se quiere que todas las ramas tengan el mismo tamaño (type = “uniform”) o que su longitud sea proporcional a la reducción de impureza (heterogeneidad) de los nodos terminales (type = “proportional”). Esta segunda opción permite identificar visualmente el impacto de cada división en el modelo. La función text() añade la descripción de cada nodo interno y el valor de cada nodo terminal. # Estructura del árbol con prepodado # ============================================================================== plot(x = arbol_regresion, type = &quot;proportional&quot;) text(x = arbol_regresion, splits = TRUE, pretty = 0, cex = 0.8, col = &quot;firebrick&quot;) 7.9.3 Podado del árbol (pruning) Con la finalidad de reducir la varianza del modelo y así mejorar la capacidad predictiva, se somete al árbol a un proceso de podado (pruning). El proceso de podado intenta encontrar el árbol más sencillo (menor tamaño) que consigue los mejores resultados de predicción. Para podar un árbol es necesario indicar el grado de penalización por complejidad \\(\\alpha\\). Cuanto mayor sea este valor, más agresivo es el podado y menor el tamaño del árbol resultante. Dado que no hay forma de conocer de antemano el valor óptimo de \\(\\alpha\\), se recurre a validación cruzada para identificarlo. La función cv.tree() emplea validación cruzada (cross validation) para identificar el valor óptimo de penalización. Por defecto, esta función emplea la ‘deviance’ para guiar el proceso de pruning. El podado se realiza sobre un árbol sin apenas requisitos de prepodado como muestra el siguiente código: # Pruning (const complexity pruning) por validación cruzada # ============================================================================== # El árbol se crece al máximo posible para luego aplicar el pruning control &lt;- tree.control(dim(Boston_train)[1], mincut = 1, minsize = 2, mindev = 0) arbol_regresion_base&lt;- tree(formula = medv ~ ., data = Boston_train, control = control) summary(arbol_regresion_base) #&gt; #&gt; Regression tree: #&gt; tree(formula = medv ~ ., data = Boston_train, control = control) #&gt; Variables actually used in tree construction: #&gt; [1] &quot;rm&quot; &quot;lstat&quot; &quot;dis&quot; &quot;indus&quot; &quot;black&quot; #&gt; [6] &quot;age&quot; &quot;crim&quot; &quot;ptratio&quot; &quot;tax&quot; &quot;rad&quot; #&gt; [11] &quot;zn&quot; &quot;nox&quot; #&gt; Number of terminal nodes: 281 #&gt; Residual mean deviance: 0.008568 = 0.6083 / 71 #&gt; Distribution of residuals: #&gt; Min. 1st Qu. Median Mean 3rd Qu. Max. #&gt; -0.1000 0.0000 0.0000 0.0000 0.0000 0.1333 El árbol sin restricciones de prepodado tiene 281 nodos y el valor de ‘Residual mean deviance’ ha disminuido mucho, pero obviamente hay sobreajuste. Para resucir el sobreajuste de este modelo se aplica un proceso de validación cruzada como se indicó el la Figura . En este caso se toman 5 folks. # Búsqueda por validación cruzada # ============================================================================== set.seed(123) cv_arbol &lt;- cv.tree(arbol_regresion_base, K = 5) El objeto devuelto por cv.tree() contiene: size: el tamaño (número de nodos terminales) de cada árbol. dev: la estimación de cross-validation test error para cada tamaño de árbol. k: El rango de valores de penalización \\(\\alpha\\) evaluados. method: El criterio empleado para seleccionar el mejor árbol. # Tamaño óptimo encontrado # ============================================================================== size_optimo &lt;- rev(cv_arbol$size)[which.min(rev(cv_arbol$dev))] paste(&quot;Tamaño óptimo encontrado:&quot;, size_optimo) #&gt; [1] &quot;Tamaño óptimo encontrado: 11&quot; library(gridExtra) library(ggplot2) resultados_cv &lt;- data.frame(n_nodos = cv_arbol$size, deviance = cv_arbol$dev, alpha = cv_arbol$k) p1 &lt;- ggplot(data = resultados_cv, aes(x = n_nodos, y = deviance)) + geom_line() + ylim(8000,12000)+ xlim(0,20)+ geom_point() + geom_vline(xintercept = size_optimo, color = &quot;red&quot;) + labs(title = &quot;Error vs tamaño del árbol&quot;) + theme_bw() p2 &lt;- ggplot(data = resultados_cv, aes(x = alpha, y = deviance)) + geom_line() + # ylim(8290,8300)+ # xlim(0,1)+ geom_point() + labs(title = &quot;Error vs penalización alpha&quot;) + theme_bw() grid.arrange(p1, p2, ncol = 2, nrow = 1) Figure 7.7: Partición del espacio con predicciones Una vez identificado el valor óptimo de \\(\\alpha\\), con la función prune.tree() se aplica el podado final. En este caso se impone la condición de que obtenga el árbol con el valor óptimo de ramas encontrado el el proceso de validación cruzada con cvtree(). # Estructura del árbol creado final # ============================================================================== arbol_final &lt;- prune.tree(tree = arbol_regresion_base, best = size_optimo) plot(x = arbol_final, type = &quot;uniform&quot;) text(x = arbol_final, splits = TRUE, pretty = 0, cex = 0.8, col = &quot;firebrick&quot;) 7.9.4 Predicción y evaluación del modelo Por último, se evalúa la capacidad predictiva de los tres árboles empleando el conjunto de test. # Error de test del modelo base con pre-podado y sin post-podado # ============================================================================== predicciones &lt;- predict(arbol_regresion, newdata = Boston_test) test_rmse &lt;- sqrt(mean((predicciones - Boston_test$medv)^2)) paste(&quot;Error de test (RMSE) del árbol solo CON pre-podado:&quot;, round(test_rmse,4)) #&gt; [1] &quot;Error de test (RMSE) del árbol solo CON pre-podado: 4.6083&quot; # Error de test del modelo base sin pre-podado y sin post-podado # ============================================================================== predicciones &lt;- predict(arbol_regresion_base, newdata = Boston_test) test_rmse &lt;- sqrt(mean((predicciones - Boston_test$medv)^2)) paste(&quot;Error de test (RMSE) del árbol SIN pre-podado y SIN post-podado:&quot;, round(test_rmse,4)) #&gt; [1] &quot;Error de test (RMSE) del árbol SIN pre-podado y SIN post-podado: 4.9402&quot; # Error de test del modelo final # ============================================================================== predicciones &lt;- predict(arbol_final, newdata = Boston_test) test_rmse &lt;- sqrt(mean((predicciones - Boston_test$medv)^2)) paste(&quot;Error de test (RMSE) del árbol sin pre-podado y CON post-podado:&quot;, round(test_rmse,4)) #&gt; [1] &quot;Error de test (RMSE) del árbol sin pre-podado y CON post-podado: 4.6198&quot; En este caso el modelo los modelos con pre-podado y post-podado tiene un comportamiento muy similar en términos de RMSE y ambas opciones mejoran el RMSE del modelo sin podar. 7.10 Webs https://rpubs.com/fdvm/clase_arboles_uba_dm https://rpubs.com/mpfoley73/529130 7.11 Referencias "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
