<!DOCTYPE html>
<html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>Capítulo 4 Regresión Lineal Múltiple | Herramientas… MBA</title>
<meta name="author" content="Fernando López   Manuel Reuz">
<meta name="description" content="Esta sesión trata sobre la regresión lineal, que es un enfoque sencillo dentro de los algoritmos de aprendizaje supervisado y que se utiliza principalmente para predecir respuestas cuantitativas....">
<meta name="generator" content="bookdown 0.40 with bs4_book()">
<meta property="og:title" content="Capítulo 4 Regresión Lineal Múltiple | Herramientas… MBA">
<meta property="og:type" content="book">
<meta property="og:description" content="Esta sesión trata sobre la regresión lineal, que es un enfoque sencillo dentro de los algoritmos de aprendizaje supervisado y que se utiliza principalmente para predecir respuestas cuantitativas....">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Capítulo 4 Regresión Lineal Múltiple | Herramientas… MBA">
<meta name="twitter:description" content="Esta sesión trata sobre la regresión lineal, que es un enfoque sencillo dentro de los algoritmos de aprendizaje supervisado y que se utiliza principalmente para predecir respuestas cuantitativas....">
<!-- JS --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://kit.fontawesome.com/6ecbd6c532.js" crossorigin="anonymous"></script><script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="libs/bootstrap-4.6.0/bootstrap.min.css" rel="stylesheet">
<script src="libs/bootstrap-4.6.0/bootstrap.bundle.min.js"></script><script src="libs/bs3compat-0.4.2/transition.js"></script><script src="libs/bs3compat-0.4.2/tabs.js"></script><script src="libs/bs3compat-0.4.2/bs3compat.js"></script><link href="libs/bs4_book-1.0.0/bs4_book.css" rel="stylesheet">
<script src="libs/bs4_book-1.0.0/bs4_book.js"></script><script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script><script src="libs/plotly-binding-4.10.1/plotly.js"></script><script src="libs/typedarray-0.1/typedarray.min.js"></script><link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet">
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script><link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet">
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script><script>
        $(function() {
            $("#toc h2").html("En este tema");
        });
    </script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- CSS --><style type="text/css">
    
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  </style>
<style type="text/css">
    /* Used with Pandoc 2.11+ new --citeproc when CSL is used */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
        }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
<link rel="stylesheet" href="style.css">
</head>
<body data-spy="scroll" data-target="#toc">

<div class="container-fluid">
<div class="row">
  <header class="col-sm-12 col-lg-3 sidebar sidebar-book"><a class="sr-only sr-only-focusable" href="#content">Skip to main content</a>

    <div class="d-flex align-items-start justify-content-between">
      <h1>
        <a href="index.html" title="">Herramientas… MBA</a>
      </h1>
      <button class="btn btn-outline-primary d-lg-none ml-2 mt-1" type="button" data-toggle="collapse" data-target="#main-nav" aria-expanded="true" aria-controls="main-nav"><i class="fas fa-bars"></i><span class="sr-only">Show table of contents</span></button>
    </div>

    <div id="main-nav" class="collapse-lg">
      <form role="search">
        <input id="search" class="form-control" type="search" placeholder="Search" aria-label="Search">
</form>

      <nav aria-label="Table of contents"><h2>Table of contents</h2>
        <ul class="book-toc list-unstyled">
<li><a class="" href="index.html"><span class="header-section-number">1</span> Presentación</a></li>
<li><a class="" href="introducci%C3%B3n-a-r.html"><span class="header-section-number">2</span> Introducción a R</a></li>
<li><a class="" href="estad%C3%ADstica-con-r.html"><span class="header-section-number">3</span> Estadística con R</a></li>
<li><a class="active" href="regresi%C3%B3n-lineal-m%C3%BAltiple.html"><span class="header-section-number">4</span> Regresión Lineal Múltiple</a></li>
<li><a class="" href="an%C3%A1lisis-de-componentes-principales.html"><span class="header-section-number">5</span> Análisis de Componentes Principales</a></li>
<li><a class="" href="an%C3%A1lisis-cluster.html"><span class="header-section-number">6</span> Análisis Cluster</a></li>
<li><a class="" href="m%C3%A9todos-basados-en-%C3%A1rboles.html"><span class="header-section-number">7</span> Métodos basados en árboles</a></li>
<li><a class="" href="m%C3%A9todos-chingones.html"><span class="header-section-number">8</span> Métodos chingones</a></li>
</ul>

        <div class="book-extra">
          <p><a id="book-repo" href="https://github.com/rstudio/bookdown-demo">View book source <i class="fab fa-github"></i></a></p>
        </div>
      </nav>
</div>
  </header><main class="col-sm-12 col-md-9 col-lg-7" id="content"><div id="regresión-lineal-múltiple" class="section level1" number="4">
<h1>
<span class="header-section-number">Capítulo 4</span> Regresión Lineal Múltiple<a class="anchor" aria-label="anchor" href="#regresi%C3%B3n-lineal-m%C3%BAltiple"><i class="fas fa-link"></i></a>
</h1>
<p>Esta sesión trata sobre la regresión lineal, que es un enfoque sencillo dentro de los algoritmos de aprendizaje supervisado y que se utiliza principalmente para predecir respuestas cuantitativas. Aunque puede parecer menos emocionante que otros métodos estadísticos más modernos, la regresión lineal sigue siendo una herramienta ampliamente utilizada y útil. Queremos destacar que es una base importante para comprender métodos más complejos, ya que muchos de estos métodos se pueden ver como extensiones de la regresión lineal. Por lo tanto es importante comprender la regresión lineal antes de abordar métodos de aprendizaje más avanzados. El enfoque principal de esta sesión es revisar las ideas clave detrás del modelo de regresión lineal y el método de mínimos cuadrados utilizado para ajustar este modelo.</p>
<p>La regresión múltiple tiene como objetivo analizar un modelo que pretende explicar el comportamiento de una variable (endógena, explicada o dependiente), que se denota como <span class="math inline">\(Y\)</span>, utilizando la información proporcionada por los valores tomados por un conjunto de variables explicativas (exógenas o independientes), que se denotan por <span class="math inline">\(X_1, X_2,\dots, X_p\)</span>. El modelo lineal (modelo econométrico) viene dado de la forma:</p>
<span class="math display">\[\begin{equation}\label{eq:modelo}
Y=\beta_0+\beta_1 X_1+\beta_2 X_2+\dots+\beta_p X_p +e
\end{equation}\]</span>
<p>donde <span class="math inline">\(\beta_0,\beta_1,\dots,\beta_p\)</span> son parámetros desconocidos (o coeficientes) y <span class="math inline">\(e\)</span> es un termino aleatorio de error, que es independiente de las variables explicativas <span class="math inline">\(X_1, X_2,\dots, X_p\)</span> y tiene media cero.</p>
<div class="float">
<img src="Figures/MLR.JPG" style="width:100.0%" alt="Regresión lineal múltiple con una variable respuesta (Y) y dos variables predictoras (X_1,X_2)"><div class="figcaption">Regresión lineal múltiple con una variable respuesta (<span class="math inline">\(Y\)</span>) y dos variables predictoras (<span class="math inline">\(X_1,X_2)\)</span>
</div>
</div>
<div id="estimación-de-los-parámetros.-método-de-los-mínimos-cuadrados." class="section level2" number="4.1">
<h2>
<span class="header-section-number">4.1</span> Estimación de los parámetros. Método de los mínimos cuadrados.<a class="anchor" aria-label="anchor" href="#estimaci%C3%B3n-de-los-par%C3%A1metros.-m%C3%A9todo-de-los-m%C3%ADnimos-cuadrados."><i class="fas fa-link"></i></a>
</h2>
<p>Supongamos que tenemos una muestra de tamaño <span class="math inline">\(n\)</span> en la que hemos observado las variables
<span class="math display">\[Y=\begin{pmatrix}y_1\\y_2\\ \vdots\\y_n \end{pmatrix};\quad X=(1, X_1, X_2,\dots, X_p)=\begin{pmatrix}
1&amp;x_{11}&amp;x_{12}&amp;\dots&amp;x_{1p}\\
1&amp;x_{21}&amp;x_{22}&amp;\dots&amp;x_{2p}\\
&amp;&amp;&amp;\vdots&amp;\\
1&amp;x_{n1}&amp;x_{n2}&amp;\dots&amp;x_{np}
\end{pmatrix}\]</span></p>
<p>Si denotamos por <span class="math display">\[\beta=\begin{pmatrix}\beta_0\\\beta_1\\\vdots\\\beta_p\end{pmatrix}\]</span>
el modelo econométrico se puede expresar en forma matricial como
<span class="math display">\[Y=X\beta+E\]</span>
<strong>Teorema</strong></p>
<p>Si las columnas de <span class="math inline">\(X\)</span> son linealmente independientes, entonces el estimador mínimo cuadrático de los coeficientes del modelo sería
<span class="math display">\[\widehat{\beta}=(X^tX)^{-1}X^tY\]</span>
donde <span class="math inline">\(X^t\)</span> denota la matriz traspuesta de X.</p>
<p><strong>Demostración: </strong>
Denotemos por <span class="math inline">\(\widehat{Y}=X\widehat{\beta}\)</span> tenemois que los terminos de error (o residuos) se pueden escribir como <span class="math inline">\(e_i=y_i-\widehat{y}_i=y_i-\widehat{\beta_0}+\widehat{\beta_1}x_{i1}-\widehat{\beta_2}x_{i2}-\dots-\widehat{\beta_p}x_{ip}\)</span>. El método de estimación mínimo cuadrático consiste en obtener el vector de coeficientes <span class="math inline">\(\widehat{\beta}\)</span> que minimiza la suma de los errores al cuadrado. Teniendo en cuenta que al ser escalares <span class="math inline">\(Y^tX\widehat{\beta}=\widehat{\beta}^tX^tY\)</span>, se verifica que la suma de los errores al cuadrado se puede escribir como</p>
<span class="math display">\[\begin{eqnarray}
RSS&amp;=&amp;\sum_{i=1}^ne_i^2=[Y-X\widehat{\beta}]^t[Y-X\widehat{\beta}]=\\
&amp;=&amp;Y^tY-Y^tX\widehat{\beta}-\widehat{\beta}^tX^tY+\widehat{\beta}^tX^tX\widehat{\beta}=\\
&amp;=&amp;Y^tY-2Y^tX\widehat{\beta}+\widehat{\beta}^tX^tX\widehat{\beta}.
\end{eqnarray}\]</span>
<p>Para minimizar <span class="math inline">\(RSS\)</span> tenemos que resolver la ecuación
<span class="math display">\[\frac{\partial RSS}{\partial \widehat{\beta}}=-2Y^tX+2\widehat{\beta}^tX^tX=0\]</span>
y por tanto como las columnas de <span class="math inline">\(X\)</span> son linealmente independientes existe la inversa de la matriz <span class="math inline">\(X^tX\)</span> obteniendo <span class="math display">\[\widehat{\beta}=(X^tX)^{-1}X^tY\]</span> tal y como queriamos demostrar.<span class="math inline">\(\square\)</span></p>
<p>Obsérvese que <span class="math inline">\(\widehat{\beta}_i\)</span> midel el cambbio en <span class="math inline">\(Y\)</span> por cada cambio unitario en <span class="math inline">\(X_i\)</span> para todo <span class="math inline">\(i=1, 2,\dots, p\)</span>.
Además si comprobamos que los residuos son homocedásticos, independientes e identicamente distribuidos como una distribución <span class="math inline">\(N(0,\sigma^2)\)</span>, tenemos que <span class="math inline">\(Y\)</span> se distribuye como <span class="math inline">\(N(X\widehat{\beta},\sigma^2 I)\)</span>. Un estimador de la varianza del error sería:
<span class="math display">\[\sigma^2=\frac{1}{n-(p+1)}\sum_{i=1}^ne_i^2=\frac{1}{n-(p+1)}\sum_{i=1}^n(y_i-\widehat{y}_i)^2\]</span></p>
<p>Como <span class="math inline">\(\widehat{\beta}=(X^tX)^{-1}X^tY\)</span> se verifica que su media es <span class="math display">\[\mathbb{E}(\widehat{\beta})=(X^tX)^{-1}X^tE(Y)=(X^tX)^{-1}X^tX\beta=\beta.\]</span> Si queremos hacer inferencia para contrastar una hipotesis nula del estilo <span class="math inline">\(H_0: \, \beta=0\)</span> tenemos que <span class="math display">\[Var(\widehat{\beta})=(X^tX)^{-1}X^tVar(Y)X(X^tX)^{-1}=(X^tX)^{-1}X^t\sigma^2X(X^tX)^{-1}=\sigma^2(X^tX)^{-1}\]</span> y como <span class="math inline">\(\widehat{\beta}\)</span> es una combinmacion lineal de elementos de <span class="math inline">\(Y\)</span> bajo <span class="math inline">\(H_0\)</span> se verifica que <span class="math display">\[\widehat{\beta}\sim N(0,\sigma^2(X^tX)^{-1}).\]</span> Esto nos permite hacer inferencia sobre la significatividad de los parámetros estimados <span class="math inline">\(\widehat{\beta}\)</span>, contrastando si significativamente distintos de cero, así como calcular intervalos de confianza para los mismos.</p>
<p>Los coeficientes estimados <span class="math inline">\(\widehat{\beta}\)</span> nos proporcionan información sobre cuanto aporta cada variable independiente <span class="math inline">\(X_1, X_2,\dots, X_p\)</span> a la estimación de <span class="math inline">\(Y\)</span>.</p>
</div>
<div id="descomposición-de-la-varianza-y-bondad-del-ajuste." class="section level2" number="4.2">
<h2>
<span class="header-section-number">4.2</span> Descomposición de la Varianza y bondad del ajuste.<a class="anchor" aria-label="anchor" href="#descomposici%C3%B3n-de-la-varianza-y-bondad-del-ajuste."><i class="fas fa-link"></i></a>
</h2>
<p>Basado en la ley del valor esperado total que dice <span class="math inline">\(\mathbb{E}(Y)=\mathbb{E}(\mathbb{E}(Y|X))\)</span> podemos demostrar el siguiente resultado.</p>
<p><strong>Proposición:</strong>
Si <span class="math inline">\(X\)</span> e <span class="math inline">\(Y\)</span> son dos variables aleatorias definidas en el mismo espacio de probabilidad y suponemos que <span class="math inline">\(Y\)</span> tiene varianza finita, entonces
<span class="math display">\[Var(Y)=\mathbb{E}(Var(Y|X))+Var(\mathbb{E}(Y|X))\]</span></p>
<p><strong>Demostración:</strong>
Sabemos que <span class="math inline">\(Var(Y)=\mathbb{E}(Y^2)-\mathbb{E}(Y)^2\)</span> y por tanto <span class="math inline">\(\mathbb{E}(Y^2)=Var(Y)+\mathbb{E}(Y)^2\)</span>. Luego aplicando la ley del valor esperado total a la expresion anterior tenemos que
<span class="math display">\[\mathbb{E}(Y^2)=\mathbb{E}(Var(Y|X)+\mathbb{E}(Y|X)^2)=\mathbb{E}(Var(Y|X))+\mathbb{E}(\mathbb{E}(Y|X))^2.\]</span> Restando <span class="math inline">\(\mathbb{E}(Y)^2\)</span> en ambos lados de la igualdad anterior y applicando de nuevo ley del valor esperado total a <span class="math inline">\(\mathbb{E}(Y)^2=\mathbb{E}(\mathbb{E}(Y|X))^2\)</span> tenemos que</p>
<span class="math display">\[\begin{eqnarray}
Var(Y)&amp;=&amp;\mathbb{E}(Y^2)-\mathbb{E}(Y)^2=\mathbb{E}(Var(Y|X))+\mathbb{E}(\mathbb{E}(Y|X))^2-\mathbb{E}(Y)^2=\\
&amp;=&amp;\mathbb{E}(Var(Y|X))+\mathbb{E}(\mathbb{E}(Y|X)^2)-\mathbb{E}(\mathbb{E}(Y|X))^2=\mathbb{E}(Var(Y|X))+Var(\mathbb{E}(Y|X))
\end{eqnarray}\]</span>
<p>tal y como queriamos demostrar. <span class="math inline">\(\square\)</span></p>
<p><strong>Corolario</strong>
Dado el modelo de regresión lineal <span class="math inline">\(Y=X\beta+E\)</span> se verifica que <span class="math inline">\(Var(Y)=Var(E)+Var(\widehat{Y})\)</span></p>
<p><strong>Demostración:</strong>
Obsérvese que <span class="math inline">\(Var(Y|X)=Var(E)=\sigma^2\)</span> y <span class="math inline">\(\mathbb{E}(Y|X)=\widehat{Y}\)</span>. Por lo tanto como consecuencia de la proposición anterior tenemos que <span class="math inline">\(Var(Y)=\sigma^2+Var(\widehat{Y})\)</span>, es decir la varianza total de <span class="math inline">\(Y\)</span> se descompone como la suma de la varianza explicada por <span class="math inline">\(\widehat{Y}\)</span> y la varianza de los errores. <span class="math inline">\(\square\)</span></p>
<div id="coeficientes-de-determinación-y-correlación." class="section level3" number="4.2.1">
<h3>
<span class="header-section-number">4.2.1</span> Coeficientes de determinación y correlación.<a class="anchor" aria-label="anchor" href="#coeficientes-de-determinaci%C3%B3n-y-correlaci%C3%B3n."><i class="fas fa-link"></i></a>
</h3>
<p>Se define el coeficiente de determinación <span class="math inline">\(R^2\)</span> como la proporcion de la varianza total que es recogida por la varianza de la variable ajustada, es decir <span class="math display">\[R^2=\frac{Var(\widehat{Y})}{Var(Y)}=1-\frac{Var(E)}{Var(Y)}\]</span>
De esta última expresión es inmediato ver que <span class="math inline">\(0\leq R^2\leq 1\)</span>. Observese que si <span class="math inline">\(R^2=0\)</span> significa que <span class="math inline">\(Var(E)=Var(Y)\)</span> y por la proposición anterior <span class="math inline">\(Var(\widehat{Y})=0\)</span>, por tanto <span class="math inline">\(\widehat{Y}=E(Y)\)</span> y el modelo no recoge nada de la variabilidad total y como consecuencia el ajuste es malo. En el otro extremo, si <span class="math inline">\(R^2=1\)</span> se sigue que <span class="math inline">\(Var(\widehat{Y})=Var(Y)\)</span> y <span class="math inline">\(Var(E)=0\)</span> y por tanto el modelo recoge toda la variabilidad obteniendo <span class="math inline">\(Y=\widehat{Y}\)</span> y <span class="math inline">\(E=0\)</span>.
De esta manera concluimos que cuanto más cercano esté <span class="math inline">\(R^2\)</span> a 1 mejor será el ajuste del modelo.</p>
<p><strong>Proposición:</strong> Dado el modelo de regresión <span class="math inline">\(Y=X\beta+E\)</span> se verifica que <span class="math display">\[Cov(\widehat{Y},E)=\widehat{Y}^tE=0\]</span></p>
<p><strong>Demostración:</strong> Como <span class="math inline">\(E\)</span> tiene media cero, se sigue que <span class="math inline">\(Cov(\widehat{Y},E)=\mathbb{E}(\widehat{Y}^tE)=\mathbb{E}(\widehat{\beta}^tX^t(Y-\widehat{Y}))=\\=\mathbb{E}(Y^tX(X^tX)^{-1}X^tY-Y^tX(X^tX)^{-1}X^tX(X^tX)^{-1}X^tY)=0 \quad\square\)</span></p>
<p><strong>Definition</strong> Dadas dos variables estadísticas <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span>, se define el <strong>coefficiente de correlacion de Pearson</strong> de <span class="math inline">\(U\)</span> y <span class="math inline">\(V\)</span> como
<span class="math display">\[Cor(U,V)=\rho_{UV}=\frac{Cov(U,V)}{\sqrt{Var(U)Var(V)}}\]</span></p>
<p><strong>Teorema:</strong>El coeficiente de determinacion <span class="math inline">\(R^2\)</span> coincide con el coeficiente de correlacion dde Pearson de las variables <span class="math inline">\(Y\)</span> e <span class="math inline">\(\widehat{Y}\)</span> al cuadrado:
<span class="math display">\[R^2=Cor(\widehat{Y},Y)^2\]</span></p>
<p><strong>Demostración:</strong> Como <span class="math inline">\(\mathbb{E}(\widehat{Y})=\mathbb{E}(Y)\)</span> y <span class="math inline">\(Cov(\widehat{Y},E)=0\)</span> tenemos que</p>
<span class="math display">\[\begin{eqnarray}
Cor(\widehat{Y},Y)&amp;=&amp;\frac{(\widehat{Y}-\mathbb{E}(Y))^t(Y-\mathbb{E}(Y))}{\sqrt{Var(Y)Var(\widehat{Y})}}=\frac{(\widehat{Y}-\mathbb{E}(Y))^t(Y-\widehat{Y}+\widehat{Y}-\mathbb{E}(Y))}{\sqrt{Var(Y)Var(\widehat{Y})}}=\\
&amp;=&amp;\frac{(\widehat{Y}-\mathbb{E}(Y))^t(E+\widehat{Y}-\mathbb{E}(Y))}{\sqrt{Var(Y)Var(\widehat{Y})}}=\frac{(\widehat{Y}-\mathbb{E}(Y))^t(\widehat{Y}-\mathbb{E}(Y))}{\sqrt{Var(Y)Var(\widehat{Y})}}=\\
&amp;=&amp;\frac{\sqrt{Var(\widehat{Y})}}{\sqrt{Var(Y)}}
\end{eqnarray}\]</span>
<p>y por tanto <span class="math inline">\(Cor(\widehat{Y},Y)=R^2\)</span> tal y como queríamos demostrar.<span class="math inline">\(\quad \square\)</span></p>
</div>
<div id="coeficientes-de-determinación-semi-parcial-y-parcial" class="section level3" number="4.2.2">
<h3>
<span class="header-section-number">4.2.2</span> Coeficientes de determinación semi-parcial y parcial<a class="anchor" aria-label="anchor" href="#coeficientes-de-determinaci%C3%B3n-semi-parcial-y-parcial"><i class="fas fa-link"></i></a>
</h3>
<p>Para conocer cuanto contribuye la variable <span class="math inline">\(X_k\)</span> de manera única al modelo de regresión, podemos pensar en cuanto se modifica el coeficiente de determinación al excluir esta variable en la regresión lineal. De esta manera si denotamos por <span class="math inline">\(R_{-k}^2\)</span> el coefficiente de determinación del modelo de regresión lineal omitiendo la variable <span class="math inline">\(X_k\)</span> la cantidad <span class="math display">\[R^2-R_{-k}^2\]</span> es una manera de cuantificar cuanta información única sobre <span class="math inline">\(Y\)</span> en <span class="math inline">\(X_k\)</span> no está explicada por el resto de variables indeependientes. Esta cantidad es conocida como <strong>coeficiente de determinación semi-parcial</strong>. Se define el <strong>coefficiente de determinacion parcial </strong> como <span class="math display">\[\frac{R^2-R^2_{-k}}{1-R^2_{-k}}\]</span></p>
<p><strong>Teorema:</strong> Sea <span class="math inline">\(U\)</span> el residuo de la regresión lineal de una variable independiente <span class="math inline">\(X_k\)</span> sobre el resto de las variables independientes <span class="math inline">\(X_i\)</span> con <span class="math inline">\(i\neq k\)</span>. Denotemos por <span class="math inline">\(\widehat{Y}_{-k}\)</span> y <span class="math inline">\(V\)</span> los valores ajustados de <span class="math inline">\(Y\)</span> y los residuos cuando hacemos la resgesion de <span class="math inline">\(Y\)</span> sobre todas las variables independientes excepto <span class="math inline">\(X_k\)</span> respectivamente. Entonces el coeficiente de determinación semi-parcial y el coeficiente de determinación parcial se pueden calcular como:</p>
<p><span class="math display">\[R^2-R^2_{-k}=Cor(Y,U)^2 \quad\quad\quad\quad\frac{R^2-R^2_{-k}}{1-R^2_{-k}}=Cor(U,V)^2\]</span>
<strong>Demostración:</strong> La demostración la haremos utilizando algebra lineal. Para ello utilizaremos algunos conceptos básicos. Denotamos el producto escalar de dos vectores <span class="math inline">\(S\)</span> y <span class="math inline">\(W\)</span> por <span class="math inline">\(\langle S, W \rangle=S^tW=\sum_{i=1}^n s_iw_i\)</span> y su norma por <span class="math inline">\(||S||=\sqrt{\langle S, S\rangle}\)</span>. Sabemos que si <span class="math inline">\(S\)</span> y <span class="math inline">\(W\)</span> son ortogonales entonves <span class="math inline">\(\langle S,W\rangle=0\)</span>. Sea la matriz <span class="math inline">\(P_{-k}\)</span> la proyección ortogonal en el espacio generado por todas las variables independientes excepto <span class="math inline">\(X_k\)</span>. Es conocido que la matriz <span class="math inline">\(P_{-k}\)</span> es simétrica e idempotente, es decir <span class="math inline">\(P_{-k}^2=P_{-k}\)</span>. Entonces se tiene que <span class="math inline">\(P_{-k}Y=\widehat{Y}_{-k}\)</span> y <span class="math inline">\(U=X_k-\widehat{X}_k=(I-P_{-k})X_k\)</span>. Además <span class="math inline">\(U\)</span> es ortogonal a todos los <span class="math inline">\(X_i\)</span> con <span class="math inline">\(i\neq k\)</span> ya que <span class="math inline">\(\langle U,X_i\rangle=U^tX_i=X_k^t(I-P_{-k})X_i=X_k^t(X_i-X_i)=0\)</span>. Por lo tanto, <span class="math inline">\(\widehat{Y}\)</span> que es la proyeccion ortogonal de <span class="math inline">\(Y\)</span> en el espacio generado por todas las variables predictoras se puede descomponer como la suma de la proyección ortogonal sobre el espacio generado por todas la variables excepto <span class="math inline">\(X_k\)</span> y uno ortogonal a este, e.g. el generado por <span class="math inline">\(U\)</span>. Pero la proyeccion de <span class="math inline">\(Y\)</span> en el espacio generado por <span class="math inline">\(U\)</span> es el valor estimado de la recta de regrsion de <span class="math inline">\(Y\)</span> sobre <span class="math inline">\(U\)</span>, es decir <span class="math inline">\(\frac{\langle Y,U\rangle}{||U||}U\)</span>. Por tanto tenemos que <span class="math display">\[\widehat{Y}=\widehat{Y}_{-k}+\frac{\langle Y,U\rangle}{||U||^2}U\]</span> y teniendo en cuenta que <span class="math inline">\(Y_{-k}\)</span> y <span class="math inline">\(U\)</span> son oryogonales calculando la norma al cuadrado en la expresion anterior <span class="math display">\[||Y||^2=||Y_{-k}||^2+\frac{\langle Y,U\rangle^2}{||U||^2}.\]</span> Utilizando estas expresiones y que <span class="math inline">\(\mathbb{E}(Y)=\mathbb{E}(\widehat{Y})\)</span> tenemos que</p>
<span class="math display">\[\begin{eqnarray}
R^2&amp;=&amp;1-\frac{Var(E)}{Var(Y)}=1-\frac{Var(Y)-Var(\widehat{Y})}{Var(Y)}=1-\frac{||Y||^2-||\widehat{Y}||^2}{||Y-\mathbb{E}(Y)||^2}\\
&amp;=&amp;1-\frac{||Y||^2-||\widehat{Y}_{-k}||^2-\frac{\langle Y,U\rangle^2}{||U||^2}}{||Y-\mathbb{E}(Y)||^2}\\
&amp;=&amp;1-\frac{||Y-\widehat{Y}_{-k}||^2}{||Y-\mathbb{E}(Y)||^2}+\frac{\frac{\langle Y,U\rangle^2}{||U||^2}}{||Y-\mathbb{E}(Y)||^2}=R^2_{-k}+\frac{\langle Y,U\rangle^2}{||Y-\mathbb{E}(Y)||^2||U||^2}\\
&amp;=&amp;R^2_{-k}+ Cor(Y,U)
\end{eqnarray}\]</span>
<p>Luego el coeficiente de determinación semi-parcial queda <span class="math inline">\(R^2-R^2_{-k}=Cor(Y,U)^2\)</span>.</p>
<p>Por otro lado como <span class="math inline">\(1-R^2_{-k}=1-(1-\frac{Var(V)}{Var(Y)})=\frac{||V||^2}{||Y-\mathbb{E}(Y)||^2}\)</span> y <span class="math inline">\(\widehat{Y}_{-k}\)</span> es ortogonal a <span class="math inline">\(U\)</span>, tenemos que el coeficiente de determinación parcial es</p>
<p><span class="math display">\[\frac{R^2-R^2_{-k}}{1-R^2_{-k}}=\frac{\frac{\langle Y,U\rangle^2}{||Y-\mathbb{E}(Y)||^2||U||^2}}{\frac{||V||^2}{||Y-\mathbb{E}(Y)||^2}}=\frac{\langle Y,U\rangle^2}{||U||^2||V||^2}=\\
=\frac{\langle Y-\widehat{Y}_{-k},U\rangle^2}{||U||^2||V||^2}=\frac{\langle V,U\rangle^2}{||U||^2||V||^2}=Cor(U,V)^2\]</span>
tal y como queríamos demostrar.<span class="math inline">\(\,\square\)</span></p>
<p>Como consecuencia el coeficiente de determinación semi-parcial tiene dos interpretaciones:</p>
<p>1.- La mejora en <span class="math inline">\(R^2\)</span> que resulta de introducir la variable <span class="math inline">\(X_k\)</span> en el modelo de regresión que ya incluía al resto de variables independientes.</p>
<p>2.- Es el coeficiente de determinación de la regresión lineal simple de <span class="math inline">\(Y\)</span> sobre <span class="math inline">\(U\)</span>.</p>
<p>Asimismo, el coeficiente de determinación parcial se puede interpretar como:</p>
<p>1.- La fracción de la máxima mejorta posible en <span class="math inline">\(R^2\)</span> al que contribuye la variable <span class="math inline">\(X_k\)</span>.</p>
<p>2.- Es el coeficiente de determinación de la regresión simple de <span class="math inline">\(V\)</span> sobre <span class="math inline">\(U\)</span>.</p>
</div>
</div>
<div id="ejemplo-de-regresión-lineal-múltiple-con-r" class="section level2" number="4.3">
<h2>
<span class="header-section-number">4.3</span> Ejemplo de regresión lineal múltiple con R<a class="anchor" aria-label="anchor" href="#ejemplo-de-regresi%C3%B3n-lineal-m%C3%BAltiple-con-r"><i class="fas fa-link"></i></a>
</h2>
<p>Carguemos la base de datos de empleados</p>
<div class="float">
<img src="Figures/Datos_empleados.png" style="width:100.0%" alt="Base de datos de empleados"><div class="figcaption">Base de datos de empleados</div>
</div>
<div class="sourceCode" id="cb56"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb56-1"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb56-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">library</span>(readxl)</span>
<span id="cb56-2"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb56-2" tabindex="-1"></a><span class="sc">&gt;</span> datos <span class="ot">&lt;-</span> <span class="fu">read_xlsx</span>(<span class="st">"Datos/Datos_de_empleados.xlsx"</span>)</span></code></pre></div>
<p>Expliquemos con un modelo de regresión lineal el salario de los trabajadores como función del salario inicial, el nivel educativo del trabajador y los meses de experiencia previa. Para ello en R tenemos que indicar las variables a relacionar de la siguiente manera:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb57-1" tabindex="-1"></a><span class="sc">&gt;</span> formula<span class="ot">&lt;-</span> datos<span class="sc">$</span>salario<span class="sc">~</span>datos<span class="sc">$</span>salini<span class="sc">+</span>datos<span class="sc">$</span>educ<span class="sc">+</span>datos<span class="sc">$</span>expprev</span></code></pre></div>
<p>El modelo de regresión se estima utilizando el comando <strong>lm</strong> y un resumen del modelo aparece con el comando <strong>summary</strong> tal y como mostramos a continuación:</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-1" tabindex="-1"></a><span class="sc">&gt;</span> modols<span class="ot">&lt;-</span><span class="fu">lm</span>(formula)</span>
<span id="cb58-2"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-2" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">summary</span>(modols)</span>
<span id="cb58-3"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-3" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb58-4"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-4" tabindex="-1"></a><span class="co">#&gt; Call:</span></span>
<span id="cb58-5"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-5" tabindex="-1"></a><span class="co">#&gt; lm(formula = formula)</span></span>
<span id="cb58-6"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-6" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb58-7"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-7" tabindex="-1"></a><span class="co">#&gt; Residuals:</span></span>
<span id="cb58-8"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-8" tabindex="-1"></a><span class="co">#&gt;    Min     1Q Median     3Q    Max </span></span>
<span id="cb58-9"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-9" tabindex="-1"></a><span class="co">#&gt; -28853  -4167  -1172   2724  48701 </span></span>
<span id="cb58-10"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-10" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb58-11"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-11" tabindex="-1"></a><span class="co">#&gt; Coefficients:</span></span>
<span id="cb58-12"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-12" tabindex="-1"></a><span class="co">#&gt;                 Estimate Std. Error t value Pr(&gt;|t|)    </span></span>
<span id="cb58-13"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-13" tabindex="-1"></a><span class="co">#&gt; (Intercept)   -3.662e+03  1.935e+03  -1.892   0.0591 .  </span></span>
<span id="cb58-14"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-14" tabindex="-1"></a><span class="co">#&gt; datos$salini   1.749e+00  5.989e-02  29.198  &lt; 2e-16 ***</span></span>
<span id="cb58-15"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-15" tabindex="-1"></a><span class="co">#&gt; datos$educ     7.360e+02  1.687e+02   4.363 1.58e-05 ***</span></span>
<span id="cb58-16"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-16" tabindex="-1"></a><span class="co">#&gt; datos$expprev -1.673e+01  3.605e+00  -4.641 4.51e-06 ***</span></span>
<span id="cb58-17"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-17" tabindex="-1"></a><span class="co">#&gt; ---</span></span>
<span id="cb58-18"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-18" tabindex="-1"></a><span class="co">#&gt; Signif. codes:  </span></span>
<span id="cb58-19"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-19" tabindex="-1"></a><span class="co">#&gt; 0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</span></span>
<span id="cb58-20"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-20" tabindex="-1"></a><span class="co">#&gt; </span></span>
<span id="cb58-21"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-21" tabindex="-1"></a><span class="co">#&gt; Residual standard error: 7632 on 470 degrees of freedom</span></span>
<span id="cb58-22"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-22" tabindex="-1"></a><span class="co">#&gt; Multiple R-squared:  0.8015, Adjusted R-squared:  0.8002 </span></span>
<span id="cb58-23"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb58-23" tabindex="-1"></a><span class="co">#&gt; F-statistic: 632.6 on 3 and 470 DF,  p-value: &lt; 2.2e-16</span></span></code></pre></div>
<p>Obsérvese, que el salario inicial y el nivel educativo tienen un impacto positivo y significativo en el salario de la persona, mientras que los años de experiencia, a pesar de ser significativo en el modelo, tiene un impacto negativo en el salario del empleado. EL coeficiente de determinación del modelo es de <span class="math inline">\(R^2=0.8015\)</span> y por tanto podemos decir que el modelo recoge aproximadamente el <span class="math inline">\(80\%\)</span> de la varianza de la variable dependiente lo que significa que es un ajuste bueno.</p>
<p>Podemos obtener los valores predichos por el modelo y representarlos graficamente.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb59-1" tabindex="-1"></a><span class="sc">&gt;</span> Yfit<span class="ot">=</span>modols<span class="sc">$</span>fitted.values</span>
<span id="cb59-2"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb59-2" tabindex="-1"></a><span class="sc">&gt;</span> xx<span class="ot">=</span><span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(Yfit))</span>
<span id="cb59-3"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb59-3" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">plot</span>(xx,modols<span class="sc">$</span>fitted.values,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">col=</span><span class="st">'red'</span>,<span class="at">ylim=</span><span class="fu">c</span>(<span class="dv">8500</span>,<span class="dv">136000</span>),<span class="at">xlab=</span><span class="st">""</span>,<span class="at">ylab=</span><span class="st">"Salario"</span>, <span class="at">xaxt=</span><span class="st">"n"</span>)</span>
<span id="cb59-4"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb59-4" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">axis</span>(<span class="dv">1</span>, <span class="at">at =</span> <span class="fu">seq</span>(<span class="dv">1</span>,<span class="fu">length</span>(Yfit),<span class="dv">20</span>))</span>
<span id="cb59-5"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb59-5" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">par</span>(<span class="at">new=</span><span class="cn">TRUE</span>)</span>
<span id="cb59-6"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb59-6" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">plot</span>(datos<span class="sc">$</span>salario,<span class="at">pch=</span><span class="dv">19</span>,<span class="at">col=</span><span class="st">'blue'</span>,<span class="at">xlab=</span><span class="st">""</span>,<span class="at">ylab=</span><span class="st">"Salario"</span>,<span class="at">axes=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<div class="inline-figure"><img src="03-Sesion3-Regresion-Lineal-multiple_files/figure-html/unnamed-chunk-4-1.png" width="672"></div>
<div class="sourceCode" id="cb60"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb60-1"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb60-1" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">par</span>(<span class="at">new=</span><span class="cn">FALSE</span>)</span></code></pre></div>
<p>Podemos ahora calcular las correlaciones parciales de todas las variables intervinientes en el modelo. Por ejemplo calculemos el coeficiente de correlación parcial de la variable <em>expprev</em> en el modelo *modols. Para ello calculamos <span class="math inline">\(R^2\)</span> y <span class="math inline">\(R^2_{-expprev}\)</span> tal y como sigue</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb61-1" tabindex="-1"></a><span class="sc">&gt;</span> restot<span class="ot">=</span><span class="fu">summary</span>(modols)</span>
<span id="cb61-2"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb61-2" tabindex="-1"></a><span class="sc">&gt;</span> R2<span class="ot">=</span>restot<span class="sc">$</span>r.squared</span>
<span id="cb61-3"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb61-3" tabindex="-1"></a><span class="sc">&gt;</span> modsinexpprev<span class="ot">=</span><span class="fu">lm</span>(datos<span class="sc">$</span>salario<span class="sc">~</span>datos<span class="sc">$</span>salini<span class="sc">+</span>datos<span class="sc">$</span>educ)<span class="co"># modelo sin la variable expprev</span></span>
<span id="cb61-4"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb61-4" tabindex="-1"></a><span class="sc">&gt;</span> resexpprev<span class="ot">=</span><span class="fu">summary</span>(modsinexpprev)</span>
<span id="cb61-5"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb61-5" tabindex="-1"></a><span class="sc">&gt;</span> R2expprev<span class="ot">=</span>resexpprev<span class="sc">$</span>r.squared</span>
<span id="cb61-6"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb61-6" tabindex="-1"></a><span class="sc">&gt;</span> PartialrR2expprev<span class="ot">=</span>(R2<span class="sc">-</span>R2expprev)<span class="sc">/</span>(<span class="dv">1</span><span class="sc">-</span>R2expprev)</span>
<span id="cb61-7"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb61-7" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">print</span>(PartialrR2expprev)</span>
<span id="cb61-8"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb61-8" tabindex="-1"></a><span class="co">#&gt; [1] 0.04381444</span></span></code></pre></div>
<p>Otra forma de calcularlo sería calculando <span class="math inline">\(1-SR_{full}/SR_{expprev}\)</span> donde <span class="math inline">\(SR_{full}\)</span> y <span class="math inline">\(SR_{expprev}\)</span> son la suma de los cuadrados de los residuos del modelo con todas las variables <span class="math inline">\(modols\)</span> y el modelo reducido sin la variable <em>expprev</em>, <span class="math inline">\(modsibexpprev\)</span> respectuivamente.</p>
<div class="sourceCode" id="cb62"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb62-1"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb62-1" tabindex="-1"></a><span class="sc">&gt;</span> PartialrR2expprev2<span class="ot">=</span><span class="dv">1</span><span class="sc">-</span><span class="fu">sum</span>(modols<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)<span class="sc">/</span><span class="fu">sum</span>(modsinexpprev<span class="sc">$</span>residuals<span class="sc">^</span><span class="dv">2</span>)</span>
<span id="cb62-2"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb62-2" tabindex="-1"></a><span class="sc">&gt;</span> <span class="fu">print</span>(PartialrR2expprev2)</span>
<span id="cb62-3"><a href="regresi%C3%B3n-lineal-m%C3%BAltiple.html#cb62-3" tabindex="-1"></a><span class="co">#&gt; [1] 0.04381444</span></span></code></pre></div>
<p>Por tanto tenemos que la mejora máxima en el coeficiente de determinación, <span class="math inline">\(R^2\)</span>, que produce la incorporación de la variable <em>expprev</em> es del <span class="math inline">\(4.38\%\)</span>.</p>
<p><strong>Ejercicio:</strong></p>
<p>1.- Calcula e interpreta los coeficientes de correlación parcial del resto de variables del modelo <em>modols</em>.</p>
<p>2.- Investiga el efecto que puede tener el género del empleado en el salario.</p>

</div>
</div>
  <div class="chapter-nav">
<div class="prev"><a href="estad%C3%ADstica-con-r.html"><span class="header-section-number">3</span> Estadística con R</a></div>
<div class="next"><a href="an%C3%A1lisis-de-componentes-principales.html"><span class="header-section-number">5</span> Análisis de Componentes Principales</a></div>
</div></main><div class="col-md-3 col-lg-2 d-none d-md-block sidebar sidebar-chapter">
    <nav id="toc" data-toggle="toc" aria-label="On this page"><h2>On this page</h2>
      <ul class="nav navbar-nav">
<li><a class="nav-link" href="#regresi%C3%B3n-lineal-m%C3%BAltiple"><span class="header-section-number">4</span> Regresión Lineal Múltiple</a></li>
<li><a class="nav-link" href="#estimaci%C3%B3n-de-los-par%C3%A1metros.-m%C3%A9todo-de-los-m%C3%ADnimos-cuadrados."><span class="header-section-number">4.1</span> Estimación de los parámetros. Método de los mínimos cuadrados.</a></li>
<li>
<a class="nav-link" href="#descomposici%C3%B3n-de-la-varianza-y-bondad-del-ajuste."><span class="header-section-number">4.2</span> Descomposición de la Varianza y bondad del ajuste.</a><ul class="nav navbar-nav">
<li><a class="nav-link" href="#coeficientes-de-determinaci%C3%B3n-y-correlaci%C3%B3n."><span class="header-section-number">4.2.1</span> Coeficientes de determinación y correlación.</a></li>
<li><a class="nav-link" href="#coeficientes-de-determinaci%C3%B3n-semi-parcial-y-parcial"><span class="header-section-number">4.2.2</span> Coeficientes de determinación semi-parcial y parcial</a></li>
</ul>
</li>
<li><a class="nav-link" href="#ejemplo-de-regresi%C3%B3n-lineal-m%C3%BAltiple-con-r"><span class="header-section-number">4.3</span> Ejemplo de regresión lineal múltiple con R</a></li>
</ul>

      <div class="book-extra">
        <ul class="list-unstyled">
<li><a id="book-source" href="https://github.com/rstudio/bookdown-demo/blob/master/03-Sesion3-Regresion-Lineal-multiple.Rmd">View source <i class="fab fa-github"></i></a></li>
          <li><a id="book-edit" href="https://github.com/rstudio/bookdown-demo/edit/master/03-Sesion3-Regresion-Lineal-multiple.Rmd">Edit this page <i class="fab fa-github"></i></a></li>
        </ul>
</div>
    </nav>
</div>

</div>
</div> <!-- .container -->

<footer class="bg-primary text-light mt-5"><div class="container"><div class="row">

  <div class="col-12 col-md-6 mt-3">
    <p>"<strong>Herramientas… MBA</strong>" was written by Fernando López <br> Manuel Reuz. It was last built on 2024-08-09.</p>
  </div>

  <div class="col-12 col-md-6 mt-3">
    <p>This book was built by the <a class="text-light" href="https://bookdown.org">bookdown</a> R package.</p>
  </div>

</div></div>
</footer><!-- dynamically load mathjax for compatibility with self-contained --><script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script><script type="text/x-mathjax-config">const popovers = document.querySelectorAll('a.footnote-ref[data-toggle="popover"]');
for (let popover of popovers) {
  const div = document.createElement('div');
  div.setAttribute('style', 'position: absolute; top: 0, left:0; width:0, height:0, overflow: hidden; visibility: hidden;');
  div.innerHTML = popover.getAttribute('data-content');

  var has_math = div.querySelector("span.math");
  if (has_math) {
    document.body.appendChild(div);
    MathJax.Hub.Queue(["Typeset", MathJax.Hub, div]);
    MathJax.Hub.Queue(function() {
      popover.setAttribute('data-content', div.innerHTML);
      document.body.removeChild(div);
    })
  }
}
</script>
</body>
</html>
