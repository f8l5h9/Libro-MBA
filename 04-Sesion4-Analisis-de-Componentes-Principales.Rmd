# Análisis de Componentes Principales

## Introducción

```{r, echo = FALSE}
# https://cienciadedatos.net/documentos/35_principal_component_analysis
# https://rstudio-pubs-static.s3.amazonaws.com/698492_92daf62ab6464ec7b59c5d6e44ba12be.html
```

El análisis de componentes principales (ACP) es una técnica fundamental en estadística y análisis multivariado que se utiliza para simplificar y entender la estructura subyacente en conjuntos de datos complejos. Esta metodología tiene aplicaciones en diversas disciplinas, como la estadística, la ingeniería, la biología, la economía y la ciencia de datos.

En esencia, el ACP busca transformar un conjunto de variables correlacionadas en un nuevo conjunto de variables no correlacionadas, conocidas como componentes principales. Estos componentes se ordenan en función de la cantidad de varianza que explican en los datos originales, lo que permite destacar las direcciones principales de variabilidad en el conjunto de datos.

La idea central detrás del ACP es reducir la dimensionalidad del conjunto de datos, manteniendo la mayor cantidad posible de información. Al proyectar los datos en un espacio de menor dimensión definido por los componentes principales, se facilita la visualización y la interpretación de patrones y tendencias en los datos, lo que puede ser crucial para la toma de decisiones informada.

A lo largo de este proceso, el ACP proporciona una herramienta valiosa para identificar patrones subyacentes, eliminar redundancias y resaltar las relaciones más importantes entre las variables, lo que contribuye significativamente a la simplificación y comprensión de conjuntos de datos complejos.

## Antes de empezar

Antes de aplicar el ACP, es importante considerar algunas condiciones y realizar ciertos pasos:

**Tipo de Variables:** El ACP se utiliza comúnmente con variables cuantitativas. Las variables deben ser de escala numérica, ya que el método implica operaciones algebraicas y estadísticas que requieren números. Si tienes variables categóricas, es posible que necesites realizar alguna transformación o utilizar técnicas diferentes.

**Escalas de Medición:** Las variables deben tener escalas de medición comparables. Si las unidades de medida son muy diferentes entre las variables, es recomendable estandarizar o normalizar las variables antes de aplicar el ACP para evitar que una variable con una escala más grande domine la variabilidad.

**Correlación entre Variables:** El ACP asume que existe cierta correlación entre las variables originales. Si las variables están completamente incorrelacionadas, el análisis no aportará información significativa.

**Linealidad:** El ACP asume linealidad entre las variables. Si la relación entre las variables es altamente no lineal, el ACP puede no capturar adecuadamente la estructura subyacente de los datos.

## El ACP

### Escalado de las variables

El proceso de PCA identifica aquellas direcciones en las que la varianza es mayor. Como la varianza de una variable se mide en su misma escala elevada al cuadrado, si antes de calcular las componentes no se estandarizan todas las variables para que tengan media 0 y desviación estándar 1, aquellas variables cuya escala sea mayor dominarán al resto. De ahí que sea recomendable estandarizar siempre los datos.

### Influencia de outliers

Al trabajar con varianzas, el método PCA es altamente sensible a outliers, por lo que es altamente recomendable estudiar si los hay. La detección de valores atípicos con respecto a una determinada dimensión es algo relativamente sencillo de hacer mediante comprobaciones gráficas. Sin embargo, cuando se trata con múltiples dimensiones el proceso se complica. Por ejemplo, considérese un hombre que mide 2 metros y pesa 50 kg. Ninguno de los dos valores es atípico de forma individual, pero en conjunto se trataría de un caso muy excepcional. La distancia de Mahalanobis es una medida de distancia entre un punto y la media que se ajusta en función de la correlación entre dimensiones y que permite encontrar potenciales outliers en distribuciones multivariante.

### Proporción de varianza explicada

Una de las preguntas más frecuentes que surge tras realizar un PCA es: ¿Cuánta información presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión? o lo que es lo mismo ¿Cuanta información es capaz de capturar cada una de las componentes principales obtenidas? Para contestar a estas preguntas se recurre a la proporción de varianza explicada por cada componente principal.

### Número óptimo de componentes principales

Por lo general, dada una matriz de datos de dimensiones n x p, el número de componentes principales que se pueden calcular es como máximo de n-1 o p (el menor de los dos valores es el limitante). Sin embargo, siendo el objetivo del PCA reducir la dimensionalidad, suelen ser de interés utilizar el número mínimo de componentes que resultan suficientes para explicar los datos. No existe una respuesta o método único que permita identificar cual es el número óptimo de componentes principales a utilizar. Una forma de proceder muy extendida consiste en evaluar la proporción de varianza explicada acumulada y seleccionar el número de componentes mínimo a partir del cual el incremento deja de ser sustancial.

## Test iniciales

### Pruebas esfericidad de Bartlett

Se utiliza para probar la Hipótesis Nula que afirma que las variables no están correlacionadas en la población. Es decir, comprueba si la matriz de correlaciones es una matriz de identidad. Se puede dar como válidos aquellos resultados que nos presenten un valor elevado del test y cuya fiabilidad sea menor a 0.05. En este caso se rechaza la Hipótesis Nula y se continúa con el Análisis.

Prueba de esfericidad de Bartlett:

:::{ .infobox_resume}

Si Sig. (p-valor) < 0.05 aceptamos $H_0$ (hipótesis nula) => se puede aplicar el análisis factorial.

Si Sig. (p-valor) > 0.05 rechazamos $H_0$ => no se puede aplicar el análisis factorial.

:::

### Prueba del KMO

El test KMO (Kaiser, Meyer y Olkin) relaciona los coeficientes de correlación, $r_{jh}$, observados entre las variables $X_j$ y $X_h$, y $a_{jh}$ son los coeficientes de correlación parcial entre las variables $X_j$ y $X_h$. Cuanto más cerca de 1 tenga el valor obtenido del test KMO, implica que la relación entres las variables es alta. Si KMO ≥ 0.9, el test es muy bueno; notable para KMO ≥ 0.8; mediano para KMO ≥ 0.7; bajo para KMO ≥ 0.6; y muy bajo para KMO < 0.5.

La prueba de evalúa la aplicabilidad del análisis factorial de las variables estudiadas. El modelo es significativo (aceptamos la hipótesis nula, H0) cuando se puede aplicar el análisis factorial


## Una aplicación con "Datos de Empleados"

El objetivo es aplicar el ACP a los datos "Datos de Empleados". Debemos seleccionar solo las variables cuantitativas a las que vamos a añadir la edad del empleado.

Lectura de datos y cálculo de matriz de correlaciones

```{r, collapse=TRUE, warning=FALSE}
library(openxlsx)
bbdd <- read.xlsx("Datos/Datos_de_empleados.xlsx")
cor(bbdd$salario,bbdd$salini)
cor(bbdd$salario,bbdd$expprev)
# Un plot con correlaciones
library(ggcorrplot)
ggcorrplot(cor(bbdd[,6:9]),lab_size = 3,hc.order = TRUE,method = "circle",lab = TRUE)
```


> **Ejercicio: Incluir nueva variable: Edad del empleado**

Buscar información para calcular la edad de cada empleado e introducirla como una nueva variable

```{r calculo edad, warning=FALSE,echo=FALSE}
bbdd$edad <- openxlsx::convertToDateTime(bbdd$fechnac, origin = "1900-01-01")
bbdd$edad <- 2023-as.numeric(substr(bbdd$edad,1,4))
bbdd$edad[is.na(bbdd$edad)] <- 50
```

Normalizamos datos

El cálculo de los componentes principales depende de las unidades de medida empleadas en las variables. Es importante, antes de aplicar el PCA, estandarizar las variables para que tengan media 0 y desviación estándar 1.


```{r}
de <- bbdd[,c(6:9,11)]
de.n <- scale(de)
corr_matrix <- cor(de.n)
ggcorrplot(corr_matrix,lab = TRUE,digits=3)
```


```{r, collapse=TRUE}
library(corrplot)
corrplot(cor(de.n))
```

Test de adecuación del ACP

```{r, warning=FALSE, message=FALSE, collapse=TRUE}
library(psych)
cortest.bartlett(cor(de.n),n=474)
```

Con este test podemos ver la relación de coeficientes de correlación entre variables si el valor es cercano a 1, mayor será la relación entre variables, dado los valores, se puede decir que es apto para la realización del análisis de componentes.

```{r, warning=FALSE}
KMO(de.n)
```

## Paquetes de R para el ACP

> **library(stats)**

* prcomp() -> Forma rápida de implementar PCA sobre una matriz de datos.

* princomp()

> **library(FactoMineR)**

* PCA() -> PCA con resultados más detallados. Los valores ausentes se reemplazan por la media de cada columna. Pueden incluirse variables categóricas suplementarias. Estandariza automáticamente los datos.

> **library(factoextra)**

* fviz_pca_ind() -> Representación de observaciones sobre componentes principales.

* fviz_pca_var() -> Representación de variables sobre componentes principales.

* fviz_screeplot() -> Representación (gráfico barras) de eigenvalores.

* fviz_contrib() -> Representa la contribución de filas/columnas de los resultados de un pca.

* get_pca() -> Extrae la información sobre las observaciones y variables de un análisis PCA.

* get_pca_var() -> Extrae la información sobre las variables.

* get_pca_ind() -> Extrae la información sobre las observaciones.

## Proporción de la varianza explicada

¿Cuánta información presente en el set de datos original se pierde al proyectar las observaciones en un espacio de menor dimensión?

¿Cuánta información es capaz de capturar cada una de las componentes principales obtenidas?

```{r}
# https://rpubs.com/laurarojasmar/ACP
pca <- prcomp(de, scale = TRUE)
print(pca)
```



```{r}
de.pca <- princomp(de.n)
summary(princomp(de.n))
```

## El gráfico de sedimentación

El gráfico de sedimentación se obtiene al representar en ordenadas las raíces características y en abscisas los números de las componentes principales correspondientes a cada raíz característica en orden decreciente.Uniendo todos los puntos se obtiene una Figura que, en general, se parece al perfil de una montaña con una fuerte pendiente hasta llegar a la base, formada por una meseta con una ligera inclinación.En este símil establecido de la montaña, en la meseta es donde se acumulan los guijarros caídos desde la cumbre, es decir, donde se sedimentan. Este es el motivo por lo que al gráfico se le conoce con el nombre de gráfico de sedimentación, su denominación en ingléses scree plot. De acuerdo con el criterio gráfico se retienen todas aquellas componentes previas a la zona de sedimentación.

```{r, warning=FALSE}
scree(de.n)
```

```{r, warning=FALSE,message=FALSE}
library(psych)
library(FactoMineR)
library(factoextra)
de.pca <- PCA(de.n, graph = FALSE)
fviz_eig(de.pca, addlabels=T)
```

## Las componentes

```{r}
biplot(x = pca, scale = 0, cex = 0.6, col = c("blue4", "brown3"))
```


```{r}
de.pca <- princomp(de.n)
a <- de.pca$scores
bbdd$cp1 <- a[,1]
bbdd$cp2 <- a[,2]
```


```{r}
plot(bbdd$cp1,bbdd$cp2,col=as.factor(bbdd$sexo))
plot(bbdd$cp1,bbdd$cp2,col=as.factor(bbdd$catlab))
```

## Ejercicio: ACP como indicador sintético

El ACP puede utilizarse para crear un indicador sintético al considerar los primeros componentes principales como resúmenes o representaciones condensadas del conjunto de datos original. Si bien cada componente principal es una combinación lineal de las variables originales, los primeros componentes suelen capturar las características más importantes o patrones de variabilidad presentes en los datos.

### Los datos

El siguiente conjunto de datos corresponde a calificaciones de 20 estudiantes en 5 materias Ciencias Natuales (CNa), Matemáticas (Mat), Francés (Fra), Latín (Lat) y Literatura (Lit)

```{r}
# https://bookdown.org/victor_morales/TecnicasML/an%C3%A1lisis-de-componentes-principales.html
CNa <- c(7,5,5,6,7,4,5,5,6,6,6,5,6,8,6,4,6,6,6,7)
Mat <- c(7,5,6,8,6,4,5,6,5,5,7,5,6,7,7,3,4,6,5,7)
Fra <- c(5,6,5,5,6,6,5,5,7,6,5,4,6,8,5,4,7,7,4,6)
Lat <- c(5,6,7,6,7,7,5,5,6,6,6,5,6,8,6,4,8,7,4,7)
Lit <- c(6,5,5,6,6,6,6,5,6,6,5,4,5,8,6,4,7,7,4,6)
Notas <- cbind(CNa,Mat,Fra,Lat,Lit)
Notas
```

* Obtener un sumario de la información
* Obtener la matriz de correlaciones
* Obtener test de adecuación
* Obtener CPs
* Interpretarlas

## Aplicación: Regresión y ACP

En esta sección realizaremos un ejercicio en el que se combina la Regresión Lineal Múltiple (RLM) con el ACP. El objetivo es reducir los problemas de multicolinealidad en la RLM al incluir variables altamente correlacionadas.

El ACP se utiliza principalmente para abordar problemas de multicolinealidad o reducción de dimensionalidad en conjuntos de datos, lo que puede tener implicaciones en modelos de regresión múltiple. Sin embargo, es importante tener en cuenta que al usar el ACP de esta manera, la interpretación de los coeficientes en términos de las variables originales puede volverse más complicada.

### Lectura y sumario de los datos

Utilizaremos la base de datos de venta de vehículos. El objetivo es predecir el precio en base a sus características. Un sumario de los datos se muestra a continuación.

```{r}
library(openxlsx)
CarSales <- read.xlsx("Datos/CarSAles.xlsx")
summary(CarSales)
```

### Eliminación de datos perdidos

Por simplicidad en el ejercicio se eliminan algunos datos en los que hay datos faltantes. En general estos valores podrian ser sustituidos por valores medios.

```{r, collapse=TRUE}
CarSales <- CarSales[!is.na(CarSales$peso_revestimiento),]
CarSales <- CarSales[!is.na(CarSales$precio),]
CarSales <- CarSales[!is.na(CarSales$kpl),]
```

### Un modelo de regresión básico

```{r, collapse=TRUE}
formula <- precio ~ motor_s + caballos + BaseNeumatico + anchura + longitud + peso_revestimiento + tapón_combustible + kpl
summary(lm(formula , data=CarSales))
```

* Se eliminan algunas variables no significativas. Pueden ser importantes para ontener un modelo explicativo pero no para un modelo predictivo.

```{r, collapse=TRUE}
# Eliminamos variables no informativas
formula <- precio ~ motor_s + caballos + anchura + longitud + peso_revestimiento + kpl
summary(lm(formula , data=CarSales))
```

* Finalmente se seleccionan algunas variables explicativas **Xs**

```{r}
Xs <- CarSales[,c("motor_s","caballos","anchura","longitud","peso_revestimiento")]
```

### Evaluamos matriz de correlaciones 

La multicolinealidad en un modelo de regresión múltiple implica la existencia de altas correlaciones entre las variables independientes, generando inestabilidad numérica y elevando la varianza de los estimadores de los coeficientes. Esto dificulta la interpretación precisa de los efectos individuales de las variables predictoras, así como la evaluación confiable de la importancia relativa de cada predictor en la predicción del resultado.

```{r, collapse=TRUE, message=FALSE}
library(ggcorrplot)
ggcorrplot(cor(Xs),lab_size = 5,lab = TRUE)
```

### Test de adecuación del ACP

Test de Bartlett contrasta la hipótesis de que el determinante es distinto de la unidad (matriz identidad)

```{r, warning=FALSE, message=FALSE, collapse=TRUE}
Xs.n <- scale(Xs)
library(psych)
cortest.bartlett(cor(Xs.n))
```

Indicador KMO sugiere buena adecuación del ACP

```{r, warning=FALSE}
KMO(Xs.n)
```

### Cálculo de las componentes

la librería 'stats' incluye la función 'princomp()' que realiza el ACP.

```{r}
Xs.pca <- princomp(Xs.n)
summary(Xs.pca)
```
Una o como máximo 2 CP serían necesarias para resumir la información. Con dos CPs se está recogiendo el 86% de la información.

### Interpretación de las CPs

```{r}
Xs.pca$loadings
```
La CP1 es un indicador global. Todas las variables puntúan de forma positiva. La CP2 discrimina coches potentes (xon muchos CVs y cc) frente a coches pequeños.

```{r}
biplot(Xs.pca)
```

```{r}
summary(lm(CarSales$precio ~ Xs.pca$scores[,1:2]))
```

